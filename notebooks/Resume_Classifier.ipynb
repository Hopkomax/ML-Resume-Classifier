{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fea70643-0527-4763-bfb4-10c03cbc4a38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gopko\\Documents\\GitHub\\ML-Resume-Classifier\\resume-ml-env\\Scripts\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "04c5b6fe-431c-4d38-bc50-2d033dce7ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.path.exists(\"../data/resume_data.csv\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "23db4751-2e35-48e7-ac8a-09052a5a8b50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.path.exists(r\"C:\\Users\\gopko\\Documents\\GitHub\\ML-Resume-Classifier\\data\\resume_data.csv\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "df4e5353-7b7d-4fd8-979f-c8dde84fea35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>category</th>\n",
       "      <th>linkedin</th>\n",
       "      <th>profile_picture</th>\n",
       "      <th>description</th>\n",
       "      <th>Experience</th>\n",
       "      <th>Name</th>\n",
       "      <th>position</th>\n",
       "      <th>location</th>\n",
       "      <th>skills</th>\n",
       "      <th>clean_skills</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>HR</td>\n",
       "      <td>https://in.linkedin.com/in/sameer-wadhawan-b55...</td>\n",
       "      <td>https://media-exp1.licdn.com/dms/image/C5603AQ...</td>\n",
       "      <td>An experienced HR professional,  HR mentor and...</td>\n",
       "      <td>Senior Vice President &amp; Head of HRCompany Name...</td>\n",
       "      <td>Sameer Wadhawan</td>\n",
       "      <td>Senior Vice President and Head of HR- Samsung ...</td>\n",
       "      <td>Gurgaon, Haryana, India</td>\n",
       "      <td>['\\nPerformance Management\\n', '\\nHuman Resour...</td>\n",
       "      <td>['Performance Management', 'Human Resources', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>HR</td>\n",
       "      <td>https://in.linkedin.com/in/adarsh-krishna-a4ab0a5</td>\n",
       "      <td>https://media-exp1.licdn.com/dms/image/C5103AQ...</td>\n",
       "      <td>Head Talent Acquisition, HR Leader and Strateg...</td>\n",
       "      <td>Head of Talent Acquisition - India &amp; APAC and ...</td>\n",
       "      <td>Adarsh Krishna</td>\n",
       "      <td>Head Talent Acquisition and HR Leader for Heal...</td>\n",
       "      <td>Pune, Maharashtra, India</td>\n",
       "      <td>['\\nTalent Acquisition\\n', '\\nEmployee Engagem...</td>\n",
       "      <td>['Talent Acquisition', 'Employee Engagement', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>HR</td>\n",
       "      <td>https://in.linkedin.com/in/shrivas-mohit</td>\n",
       "      <td>data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP//...</td>\n",
       "      <td>A Talent Acquisition and HR professional with ...</td>\n",
       "      <td>Company NameIBM INDIA Pvt LtdTotal Duration8 y...</td>\n",
       "      <td>Shrivas Mohit</td>\n",
       "      <td>HR@IBM</td>\n",
       "      <td>Bengaluru, Karnataka, India</td>\n",
       "      <td>['\\nHuman Resources\\n', '\\nRecruiting\\n', '\\nT...</td>\n",
       "      <td>['Human Resources', 'Recruiting', 'Team Manage...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>HR</td>\n",
       "      <td>https://in.linkedin.com/in/hr-hopes-086734b8</td>\n",
       "      <td>data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP//...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HR/Admin/Personnel/LegalCompany NameHR and HR ...</td>\n",
       "      <td>HR Hopes</td>\n",
       "      <td>HR</td>\n",
       "      <td>Pune Area, India</td>\n",
       "      <td>[]</td>\n",
       "      <td>['']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>HR</td>\n",
       "      <td>https://in.linkedin.com/in/rakeshkumar01</td>\n",
       "      <td>https://media-exp1.licdn.com/dms/image/C5103AQ...</td>\n",
       "      <td>Over 18 Years of experience in IT /ITES  / BPO...</td>\n",
       "      <td>Company NameEXLTotal Duration6 yrs 4 mosTitleV...</td>\n",
       "      <td>Rakesh Kumar</td>\n",
       "      <td>Vice President - Digital HR Transformation Lea...</td>\n",
       "      <td>Central Delhi, Delhi, India</td>\n",
       "      <td>['\\nTeam Management\\n', '\\nHuman Resources\\n',...</td>\n",
       "      <td>['Team Management', 'Human Resources', 'Employ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index category                                           linkedin  \\\n",
       "0      1       HR  https://in.linkedin.com/in/sameer-wadhawan-b55...   \n",
       "1      2       HR  https://in.linkedin.com/in/adarsh-krishna-a4ab0a5   \n",
       "2      3       HR           https://in.linkedin.com/in/shrivas-mohit   \n",
       "3      4       HR       https://in.linkedin.com/in/hr-hopes-086734b8   \n",
       "4      5       HR           https://in.linkedin.com/in/rakeshkumar01   \n",
       "\n",
       "                                     profile_picture  \\\n",
       "0  https://media-exp1.licdn.com/dms/image/C5603AQ...   \n",
       "1  https://media-exp1.licdn.com/dms/image/C5103AQ...   \n",
       "2  data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP//...   \n",
       "3  data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP//...   \n",
       "4  https://media-exp1.licdn.com/dms/image/C5103AQ...   \n",
       "\n",
       "                                         description  \\\n",
       "0  An experienced HR professional,  HR mentor and...   \n",
       "1  Head Talent Acquisition, HR Leader and Strateg...   \n",
       "2  A Talent Acquisition and HR professional with ...   \n",
       "3                                                NaN   \n",
       "4  Over 18 Years of experience in IT /ITES  / BPO...   \n",
       "\n",
       "                                          Experience             Name  \\\n",
       "0  Senior Vice President & Head of HRCompany Name...  Sameer Wadhawan   \n",
       "1  Head of Talent Acquisition - India & APAC and ...   Adarsh Krishna   \n",
       "2  Company NameIBM INDIA Pvt LtdTotal Duration8 y...    Shrivas Mohit   \n",
       "3  HR/Admin/Personnel/LegalCompany NameHR and HR ...         HR Hopes   \n",
       "4  Company NameEXLTotal Duration6 yrs 4 mosTitleV...     Rakesh Kumar   \n",
       "\n",
       "                                            position  \\\n",
       "0  Senior Vice President and Head of HR- Samsung ...   \n",
       "1  Head Talent Acquisition and HR Leader for Heal...   \n",
       "2                                             HR@IBM   \n",
       "3                                                 HR   \n",
       "4  Vice President - Digital HR Transformation Lea...   \n",
       "\n",
       "                      location  \\\n",
       "0      Gurgaon, Haryana, India   \n",
       "1     Pune, Maharashtra, India   \n",
       "2  Bengaluru, Karnataka, India   \n",
       "3             Pune Area, India   \n",
       "4  Central Delhi, Delhi, India   \n",
       "\n",
       "                                              skills  \\\n",
       "0  ['\\nPerformance Management\\n', '\\nHuman Resour...   \n",
       "1  ['\\nTalent Acquisition\\n', '\\nEmployee Engagem...   \n",
       "2  ['\\nHuman Resources\\n', '\\nRecruiting\\n', '\\nT...   \n",
       "3                                                 []   \n",
       "4  ['\\nTeam Management\\n', '\\nHuman Resources\\n',...   \n",
       "\n",
       "                                        clean_skills  \n",
       "0  ['Performance Management', 'Human Resources', ...  \n",
       "1  ['Talent Acquisition', 'Employee Engagement', ...  \n",
       "2  ['Human Resources', 'Recruiting', 'Team Manage...  \n",
       "3                                               ['']  \n",
       "4  ['Team Management', 'Human Resources', 'Employ...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "file_path = r\"C:\\Users\\gopko\\Documents\\GitHub\\ML-Resume-Classifier\\data\\resume_data.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "caf0ef2a-61ff-46e9-91db-cfab2b7f5e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1251 entries, 0 to 1250\n",
      "Data columns (total 11 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   index            1251 non-null   int64 \n",
      " 1   category         1251 non-null   object\n",
      " 2   linkedin         1251 non-null   object\n",
      " 3   profile_picture  1239 non-null   object\n",
      " 4   description      670 non-null    object\n",
      " 5   Experience       1226 non-null   object\n",
      " 6   Name             1239 non-null   object\n",
      " 7   position         1239 non-null   object\n",
      " 8   location         1239 non-null   object\n",
      " 9   skills           1251 non-null   object\n",
      " 10  clean_skills     1251 non-null   object\n",
      "dtypes: int64(1), object(10)\n",
      "memory usage: 107.6+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1f9f637e-846c-4b7a-8a5e-b0cc9003afe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index                0\n",
      "category             0\n",
      "linkedin             0\n",
      "profile_picture     12\n",
      "description        581\n",
      "Experience          25\n",
      "Name                12\n",
      "position            12\n",
      "location            12\n",
      "skills               0\n",
      "clean_skills         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cd77c606-53b8-4848-b314-b04657cf8b74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['index', 'category', 'linkedin', 'profile_picture', 'description',\n",
      "       'Experience', 'Name', 'position', 'location', 'skills', 'clean_skills'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "241a1337-ba97-4420-b887-01b9d2b9708f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  category                                        description  \\\n",
      "0       HR  An experienced HR professional,  HR mentor and...   \n",
      "1       HR  Head Talent Acquisition, HR Leader and Strateg...   \n",
      "2       HR  A Talent Acquisition and HR professional with ...   \n",
      "3       HR                                                NaN   \n",
      "4       HR  Over 18 Years of experience in IT /ITES  / BPO...   \n",
      "\n",
      "                                          Experience             Name  \\\n",
      "0  Senior Vice President & Head of HRCompany Name...  Sameer Wadhawan   \n",
      "1  Head of Talent Acquisition - India & APAC and ...   Adarsh Krishna   \n",
      "2  Company NameIBM INDIA Pvt LtdTotal Duration8 y...    Shrivas Mohit   \n",
      "3  HR/Admin/Personnel/LegalCompany NameHR and HR ...         HR Hopes   \n",
      "4  Company NameEXLTotal Duration6 yrs 4 mosTitleV...     Rakesh Kumar   \n",
      "\n",
      "                                            position  \\\n",
      "0  Senior Vice President and Head of HR- Samsung ...   \n",
      "1  Head Talent Acquisition and HR Leader for Heal...   \n",
      "2                                             HR@IBM   \n",
      "3                                                 HR   \n",
      "4  Vice President - Digital HR Transformation Lea...   \n",
      "\n",
      "                      location  \\\n",
      "0      Gurgaon, Haryana, India   \n",
      "1     Pune, Maharashtra, India   \n",
      "2  Bengaluru, Karnataka, India   \n",
      "3             Pune Area, India   \n",
      "4  Central Delhi, Delhi, India   \n",
      "\n",
      "                                              skills  \\\n",
      "0  ['\\nPerformance Management\\n', '\\nHuman Resour...   \n",
      "1  ['\\nTalent Acquisition\\n', '\\nEmployee Engagem...   \n",
      "2  ['\\nHuman Resources\\n', '\\nRecruiting\\n', '\\nT...   \n",
      "3                                                 []   \n",
      "4  ['\\nTeam Management\\n', '\\nHuman Resources\\n',...   \n",
      "\n",
      "                                        clean_skills  \n",
      "0  ['Performance Management', 'Human Resources', ...  \n",
      "1  ['Talent Acquisition', 'Employee Engagement', ...  \n",
      "2  ['Human Resources', 'Recruiting', 'Team Manage...  \n",
      "3                                               ['']  \n",
      "4  ['Team Management', 'Human Resources', 'Employ...  \n"
     ]
    }
   ],
   "source": [
    "df = df.drop(columns=['index', 'profile_picture', 'linkedin'])\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5a0e54d9-aafc-4091-85b6-6e8f0ba326de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category        0\n",
      "description     0\n",
      "Experience      0\n",
      "Name            0\n",
      "position        0\n",
      "location        0\n",
      "skills          0\n",
      "clean_skills    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = df.fillna(\"Unknown\")\n",
    "print(df.isnull().sum())  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5826eb9b-cf87-4b07-b775-8b70b477d39b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['HR' 'Designing' 'Managment' 'Information Technology' 'Education'\n",
      " 'Advocate' 'Business Development' 'Health & Fitness' 'Agricultural' 'BPO'\n",
      " 'Sales' 'Consultant' 'Digital Media' 'Building & Construction'\n",
      " 'Automobile' 'Banking' 'Engineering' 'Food & Beverages' 'Finance'\n",
      " 'Apparel' 'Accountant' 'Architects' 'Public Relations' 'Arts' 'Aviation']\n"
     ]
    }
   ],
   "source": [
    "print(df[\"category\"].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "670a4c88-ff08-4885-83c4-4c0d274db45a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Count the number of resumes per job category\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.countplot(y=df[\"category\"], order=df[\"category\"].value_counts().index)\n",
    "plt.xlabel(\"Number of Resumes\")\n",
    "plt.ylabel(\"Job Category\")\n",
    "plt.title(\"Distribution of Resume Categories\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "70a77090-f582-40b0-bac9-6fa66725af70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description_length</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>category</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accountant</th>\n",
       "      <td>87.480000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Advocate</th>\n",
       "      <td>130.886792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Agricultural</th>\n",
       "      <td>118.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Apparel</th>\n",
       "      <td>54.480000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Architects</th>\n",
       "      <td>73.440000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Arts</th>\n",
       "      <td>63.120000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Automobile</th>\n",
       "      <td>44.580000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Aviation</th>\n",
       "      <td>69.240000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BPO</th>\n",
       "      <td>51.980000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Banking</th>\n",
       "      <td>105.040816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Building &amp; Construction</th>\n",
       "      <td>71.240000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Business Development</th>\n",
       "      <td>182.280000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Consultant</th>\n",
       "      <td>170.380000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Designing</th>\n",
       "      <td>122.580000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Digital Media</th>\n",
       "      <td>157.540000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Education</th>\n",
       "      <td>119.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Engineering</th>\n",
       "      <td>134.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Finance</th>\n",
       "      <td>61.180000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Food &amp; Beverages</th>\n",
       "      <td>105.340000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HR</th>\n",
       "      <td>191.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Health &amp; Fitness</th>\n",
       "      <td>211.820000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Information Technology</th>\n",
       "      <td>155.820000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Managment</th>\n",
       "      <td>71.660000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Public Relations</th>\n",
       "      <td>175.780000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sales</th>\n",
       "      <td>172.860000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         description_length\n",
       "category                                   \n",
       "Accountant                        87.480000\n",
       "Advocate                         130.886792\n",
       "Agricultural                     118.714286\n",
       "Apparel                           54.480000\n",
       "Architects                        73.440000\n",
       "Arts                              63.120000\n",
       "Automobile                        44.580000\n",
       "Aviation                          69.240000\n",
       "BPO                               51.980000\n",
       "Banking                          105.040816\n",
       "Building & Construction           71.240000\n",
       "Business Development             182.280000\n",
       "Consultant                       170.380000\n",
       "Designing                        122.580000\n",
       "Digital Media                    157.540000\n",
       "Education                        119.800000\n",
       "Engineering                      134.400000\n",
       "Finance                           61.180000\n",
       "Food & Beverages                 105.340000\n",
       "HR                               191.100000\n",
       "Health & Fitness                 211.820000\n",
       "Information Technology           155.820000\n",
       "Managment                         71.660000\n",
       "Public Relations                 175.780000\n",
       "Sales                            172.860000"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"description_length\"] = df[\"description\"].apply(lambda x: len(x) if x != \"Unknown\" else 0)\n",
    "df[[\"category\", \"description_length\"]].groupby(\"category\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a0d72762-96bb-4421-921e-d05385cecb88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned dataset saved successfully!\n"
     ]
    }
   ],
   "source": [
    "df.to_csv(\"../data/resume_data_cleaned.csv\", index=False)\n",
    "\n",
    "print(\"Cleaned dataset saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f7d2cea-2e22-4e6e-826f-a05c8bb41dfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>clean_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>An experienced HR professional,  HR mentor and...</td>\n",
       "      <td>an experienced hr professional hr mentor and c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Head Talent Acquisition, HR Leader and Strateg...</td>\n",
       "      <td>head talent acquisition hr leader and strategi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A Talent Acquisition and HR professional with ...</td>\n",
       "      <td>a talent acquisition and hr professional with ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Unknown</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Over 18 Years of experience in IT /ITES  / BPO...</td>\n",
       "      <td>over years of experience in it ites bpo with l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         description  \\\n",
       "0  An experienced HR professional,  HR mentor and...   \n",
       "1  Head Talent Acquisition, HR Leader and Strateg...   \n",
       "2  A Talent Acquisition and HR professional with ...   \n",
       "3                                            Unknown   \n",
       "4  Over 18 Years of experience in IT /ITES  / BPO...   \n",
       "\n",
       "                                   clean_description  \n",
       "0  an experienced hr professional hr mentor and c...  \n",
       "1  head talent acquisition hr leader and strategi...  \n",
       "2  a talent acquisition and hr professional with ...  \n",
       "3                                            unknown  \n",
       "4  over years of experience in it ites bpo with l...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Function to clean text further\n",
    "def clean_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r\"\\d+\", \"\", text)  # Remove numbers\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)  # Remove punctuation\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()  # Remove extra spaces\n",
    "    return text\n",
    "\n",
    "# Apply text cleaning to the \"description\" column\n",
    "df[\"clean_description\"] = df[\"description\"].apply(clean_text)\n",
    "\n",
    "# Show cleaned text examples\n",
    "df[[\"description\", \"clean_description\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cdd18ea4-2b21-4f2f-87ca-2218bba00e10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>description</th>\n",
       "      <th>Experience</th>\n",
       "      <th>Name</th>\n",
       "      <th>position</th>\n",
       "      <th>location</th>\n",
       "      <th>skills</th>\n",
       "      <th>clean_skills</th>\n",
       "      <th>description_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HR</td>\n",
       "      <td>An experienced HR professional,  HR mentor and...</td>\n",
       "      <td>Senior Vice President &amp; Head of HRCompany Name...</td>\n",
       "      <td>Sameer Wadhawan</td>\n",
       "      <td>Senior Vice President and Head of HR- Samsung ...</td>\n",
       "      <td>Gurgaon, Haryana, India</td>\n",
       "      <td>['\\nPerformance Management\\n', '\\nHuman Resour...</td>\n",
       "      <td>['Performance Management', 'Human Resources', ...</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HR</td>\n",
       "      <td>Head Talent Acquisition, HR Leader and Strateg...</td>\n",
       "      <td>Head of Talent Acquisition - India &amp; APAC and ...</td>\n",
       "      <td>Adarsh Krishna</td>\n",
       "      <td>Head Talent Acquisition and HR Leader for Heal...</td>\n",
       "      <td>Pune, Maharashtra, India</td>\n",
       "      <td>['\\nTalent Acquisition\\n', '\\nEmployee Engagem...</td>\n",
       "      <td>['Talent Acquisition', 'Employee Engagement', ...</td>\n",
       "      <td>211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HR</td>\n",
       "      <td>A Talent Acquisition and HR professional with ...</td>\n",
       "      <td>Company NameIBM INDIA Pvt LtdTotal Duration8 y...</td>\n",
       "      <td>Shrivas Mohit</td>\n",
       "      <td>HR@IBM</td>\n",
       "      <td>Bengaluru, Karnataka, India</td>\n",
       "      <td>['\\nHuman Resources\\n', '\\nRecruiting\\n', '\\nT...</td>\n",
       "      <td>['Human Resources', 'Recruiting', 'Team Manage...</td>\n",
       "      <td>312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HR</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>HR/Admin/Personnel/LegalCompany NameHR and HR ...</td>\n",
       "      <td>HR Hopes</td>\n",
       "      <td>HR</td>\n",
       "      <td>Pune Area, India</td>\n",
       "      <td>[]</td>\n",
       "      <td>['']</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HR</td>\n",
       "      <td>Over 18 Years of experience in IT /ITES  / BPO...</td>\n",
       "      <td>Company NameEXLTotal Duration6 yrs 4 mosTitleV...</td>\n",
       "      <td>Rakesh Kumar</td>\n",
       "      <td>Vice President - Digital HR Transformation Lea...</td>\n",
       "      <td>Central Delhi, Delhi, India</td>\n",
       "      <td>['\\nTeam Management\\n', '\\nHuman Resources\\n',...</td>\n",
       "      <td>['Team Management', 'Human Resources', 'Employ...</td>\n",
       "      <td>208</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  category                                        description  \\\n",
       "0       HR  An experienced HR professional,  HR mentor and...   \n",
       "1       HR  Head Talent Acquisition, HR Leader and Strateg...   \n",
       "2       HR  A Talent Acquisition and HR professional with ...   \n",
       "3       HR                                            Unknown   \n",
       "4       HR  Over 18 Years of experience in IT /ITES  / BPO...   \n",
       "\n",
       "                                          Experience             Name  \\\n",
       "0  Senior Vice President & Head of HRCompany Name...  Sameer Wadhawan   \n",
       "1  Head of Talent Acquisition - India & APAC and ...   Adarsh Krishna   \n",
       "2  Company NameIBM INDIA Pvt LtdTotal Duration8 y...    Shrivas Mohit   \n",
       "3  HR/Admin/Personnel/LegalCompany NameHR and HR ...         HR Hopes   \n",
       "4  Company NameEXLTotal Duration6 yrs 4 mosTitleV...     Rakesh Kumar   \n",
       "\n",
       "                                            position  \\\n",
       "0  Senior Vice President and Head of HR- Samsung ...   \n",
       "1  Head Talent Acquisition and HR Leader for Heal...   \n",
       "2                                             HR@IBM   \n",
       "3                                                 HR   \n",
       "4  Vice President - Digital HR Transformation Lea...   \n",
       "\n",
       "                      location  \\\n",
       "0      Gurgaon, Haryana, India   \n",
       "1     Pune, Maharashtra, India   \n",
       "2  Bengaluru, Karnataka, India   \n",
       "3             Pune Area, India   \n",
       "4  Central Delhi, Delhi, India   \n",
       "\n",
       "                                              skills  \\\n",
       "0  ['\\nPerformance Management\\n', '\\nHuman Resour...   \n",
       "1  ['\\nTalent Acquisition\\n', '\\nEmployee Engagem...   \n",
       "2  ['\\nHuman Resources\\n', '\\nRecruiting\\n', '\\nT...   \n",
       "3                                                 []   \n",
       "4  ['\\nTeam Management\\n', '\\nHuman Resources\\n',...   \n",
       "\n",
       "                                        clean_skills  description_length  \n",
       "0  ['Performance Management', 'Human Resources', ...                 100  \n",
       "1  ['Talent Acquisition', 'Employee Engagement', ...                 211  \n",
       "2  ['Human Resources', 'Recruiting', 'Team Manage...                 312  \n",
       "3                                               ['']                   0  \n",
       "4  ['Team Management', 'Human Resources', 'Employ...                 208  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the file path\n",
    "file_path = r\"C:\\Users\\gopko\\Documents\\GitHub\\ML-Resume-Classifier\\data\\resume_data_cleaned.csv\"\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Show first few rows to confirm it's loaded\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ed79080-392f-4ede-b37c-25356ac29ed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF matrix shape: (1251, 4088)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\", max_features=5000)\n",
    "\n",
    "# Convert cleaned text into numerical vectors\n",
    "X = vectorizer.fit_transform(df[\"clean_description\"])\n",
    "\n",
    "# Show shape of transformed data\n",
    "print(\"TF-IDF matrix shape:\", X.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "537d5209-28e1-4e82-ba12-d85a9eef17fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (1000, 4088)\n",
      "Testing data shape: (251, 4088)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define features (X) and target labels (y)\n",
    "y = df[\"category\"]  # Job category is the target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Training data shape:\", X_train.shape)\n",
    "print(\"Testing data shape:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb383482-5f08-447c-8623-df6101ac477d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.25\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "             Accountant       1.00      0.40      0.57        10\n",
      "               Advocate       1.00      0.31      0.47        13\n",
      "           Agricultural       1.00      0.09      0.17        11\n",
      "                Apparel       0.00      0.00      0.00        10\n",
      "             Architects       0.33      0.10      0.15        10\n",
      "                   Arts       0.00      0.00      0.00        12\n",
      "             Automobile       0.00      0.00      0.00         5\n",
      "               Aviation       0.00      0.00      0.00         8\n",
      "                    BPO       0.04      0.62      0.08         8\n",
      "                Banking       0.50      0.08      0.14        12\n",
      "Building & Construction       1.00      0.13      0.24        15\n",
      "   Business Development       0.33      0.50      0.40        10\n",
      "             Consultant       0.19      0.40      0.26        10\n",
      "              Designing       0.00      0.00      0.00        12\n",
      "          Digital Media       0.33      0.25      0.29         8\n",
      "              Education       0.00      0.00      0.00        12\n",
      "            Engineering       0.27      0.50      0.35        12\n",
      "                Finance       0.50      0.08      0.13        13\n",
      "       Food & Beverages       0.25      0.33      0.29         3\n",
      "                     HR       0.40      0.29      0.33         7\n",
      "       Health & Fitness       0.60      0.67      0.63         9\n",
      " Information Technology       0.60      0.27      0.38        11\n",
      "              Managment       1.00      0.11      0.20         9\n",
      "       Public Relations       0.67      0.67      0.67        12\n",
      "                  Sales       0.38      0.56      0.45         9\n",
      "\n",
      "               accuracy                           0.25       251\n",
      "              macro avg       0.42      0.25      0.25       251\n",
      "           weighted avg       0.45      0.25      0.25       251\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gopko\\Documents\\GitHub\\ML-Resume-Classifier\\resume-ml-env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\gopko\\Documents\\GitHub\\ML-Resume-Classifier\\resume-ml-env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\gopko\\Documents\\GitHub\\ML-Resume-Classifier\\resume-ml-env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Initialize the Logistic Regression model\n",
    "log_reg = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = log_reg.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Show detailed classification metrics\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93787310-d5ad-4b8d-8037-994efd50af84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF matrix shape: (1251, 4088)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\", max_features=5000)\n",
    "\n",
    "# Convert cleaned descriptions into numerical vectors\n",
    "X = vectorizer.fit_transform(df[\"clean_description\"])\n",
    "\n",
    "# Show the shape of transformed data\n",
    "print(\"TF-IDF matrix shape:\", X.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "949f41b4-b6bf-4614-a574-673bf1ee7c77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_avp' 'aarti' 'aas' 'aat' 'abhijeet' 'abilities' 'ability' 'abilityto'\n",
      " 'able' 'aboutnational' 'abridging' 'abroad' 'absolute'\n",
      " 'absorptionspectroscopy' 'academia' 'academic' 'academics' 'academy'\n",
      " 'acb' 'accept']\n"
     ]
    }
   ],
   "source": [
    "# Get feature names (words)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Print the first 20 important words\n",
    "print(feature_names[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "312c1c5a-d7be-47a8-b4c3-a0cec2c40e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    experienced professional mentor and coach tale...\n",
      "1    head talent acquisition leader and strategic p...\n",
      "2    talent acquisition and professional with exper...\n",
      "3                                              unknown\n",
      "4    over years experience ites bpo with leading gl...\n",
      "Name: clean_description, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r\"\\d+\", \"\", text)  # Remove numbers\n",
    "    text = re.sub(r\"[^\\w\\s]\", \" \", text)  # Remove punctuation\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()  # Remove extra spaces\n",
    "    text = \" \".join([word for word in text.split() if len(word) > 2])  # Remove short words\n",
    "    return text\n",
    "\n",
    "# Apply cleaning function to text data\n",
    "df[\"clean_description\"] = df[\"description\"].apply(clean_text)\n",
    "\n",
    "# Check cleaned data\n",
    "print(df[\"clean_description\"].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e684f472-a841-404d-b533-b133f1eecd91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_avp' 'aarti' 'aas' 'aat' 'abhijeet' 'abilities' 'ability' 'abilityto'\n",
      " 'able' 'aboutnational' 'abridging' 'abroad' 'absolute'\n",
      " 'absorptionspectroscopy' 'academia' 'academic' 'academics' 'academy'\n",
      " 'acb' 'accept']\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words=\"english\", max_features=5000)\n",
    "X = vectorizer.fit_transform(df[\"clean_description\"])\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "print(feature_names[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c65bd54-48cb-47e8-aed3-23aa7e48d638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_avp' 'aarti' 'aas' 'aat' 'abhijeet' 'able' 'aboutnational' 'above'\n",
      " 'abridging' 'abroad' 'absolute' 'absorptionspectroscopy' 'academia'\n",
      " 'academics' 'academy' 'access' 'accessories' 'accompanied' 'accomplish'\n",
      " 'accomplished']\n"
     ]
    }
   ],
   "source": [
    "custom_stop_words = [\n",
    "    \"abilities\", \"ability\", \"about\", \"academic\", \"acb\", \"accept\", \"abilityto\"\n",
    "]  # Add more stopwords if needed\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=custom_stop_words, max_features=5000)\n",
    "X = vectorizer.fit_transform(df[\"clean_description\"])\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "print(feature_names[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f4b86794-17b4-4a8d-8f0e-d05f1f1c2b3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_avp' 'aarti' 'aas' 'aat' 'abhijeet' 'abilities' 'ability' 'abilityto'\n",
      " 'able' 'aboutnational' 'abridging' 'abroad' 'absolute'\n",
      " 'absorptionspectroscopy' 'academia' 'academic' 'academics' 'academy'\n",
      " 'acb' 'accept']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words=\"english\", max_features=5000)\n",
    "X = vectorizer.fit_transform(df[\"clean_description\"])\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "print(feature_names[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "31382634-177b-4025-be66-f2a6d6b112b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    experienced professional mentor and coach tale...\n",
      "1    head talent acquisition leader and strategic p...\n",
      "2    talent acquisition and professional with exper...\n",
      "3                                              unknown\n",
      "4    over years experience ites bpo with leading gl...\n",
      "Name: clean_description, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df[\"clean_description\"].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c806efb2-dc65-4286-b4e9-944fe017c185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    experienced professional mentor and coach tale...\n",
      "1    head talent acquisition leader and strategic p...\n",
      "2    talent acquisition and professional with exper...\n",
      "3                                              unknown\n",
      "4    over years experience ites bpo with leading gl...\n",
      "5    human resources business partner with demonstr...\n",
      "6    seasoned professional and experienced banker w...\n",
      "7    have been working practitioner for the past ye...\n",
      "8    passionate developing people and building comp...\n",
      "9    performance driven and accomplished profession...\n",
      "Name: clean_description, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df[\"clean_description\"].head(10))  # Show first 10 rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1e892c8a-cb84-4f72-ae58-f3b4c3f0fc37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:\n",
      " An experienced HR professional,  HR mentor and Coach , Talent advisory and HR strategist... see more\n",
      "\n",
      "Cleaned Text:\n",
      " experienced professional mentor coach talent advisory strategist more\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    text = str(text).lower()  # Convert text to lowercase\n",
    "    text = re.sub(r\"\\d+\", \"\", text)  # Remove numbers\n",
    "    text = re.sub(r\"[^\\w\\s]\", \" \", text)  # Remove punctuation\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()  # Remove extra spaces\n",
    "    words = [word for word in text.split() if len(word) > 3]  # Remove words shorter than 4 letters\n",
    "    return \" \".join(words)  # Join cleaned words back\n",
    "\n",
    "# Apply cleaning function to a small sample of text\n",
    "sample_text = df[\"description\"].iloc[0]  # Pick first resume text\n",
    "\n",
    "print(\"Original Text:\\n\", sample_text)\n",
    "print(\"\\nCleaned Text:\\n\", clean_text(sample_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e4f0ee79-4b4e-4344-bdf8-237931fa43e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\gopko\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:\n",
      " An experienced HR professional,  HR mentor and Coach , Talent advisory and HR strategist... see more\n",
      "\n",
      "Cleaned Text:\n",
      " experienced professional mentor coach talent advisory strategist\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download stopwords if not already present\n",
    "nltk.download(\"stopwords\")\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def clean_text(text):\n",
    "    text = str(text).lower()  # Convert to lowercase\n",
    "    text = re.sub(r\"\\d+\", \"\", text)  # Remove numbers\n",
    "    text = re.sub(r\"[^\\w\\s]\", \" \", text)  # Remove punctuation\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()  # Remove extra spaces\n",
    "    words = [word for word in text.split() if len(word) > 3 and word not in stop_words]  # Remove short words & stopwords\n",
    "    return \" \".join(words)  # Join cleaned words back\n",
    "\n",
    "# Apply improved cleaning function to the first resume text\n",
    "print(\"Original Text:\\n\", df[\"description\"].iloc[0])\n",
    "print(\"\\nCleaned Text:\\n\", clean_text(df[\"description\"].iloc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9cf5c7dd-f982-4eb5-918b-0d93483a356b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_avp' 'aarti' 'aas' 'aat' 'abhijeet' 'abilities' 'ability' 'abilityto'\n",
      " 'able' 'aboutnational' 'abridging' 'abroad' 'absolute'\n",
      " 'absorptionspectroscopy' 'academia' 'academic' 'academics' 'academy'\n",
      " 'acb' 'accept']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\", max_features=5000)\n",
    "\n",
    "# Convert cleaned descriptions into numerical vectors\n",
    "X = vectorizer.fit_transform(df[\"clean_description\"])\n",
    "\n",
    "# Print first 20 TF-IDF feature words\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "print(feature_names[:20])  # Check the first 20 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "73111ab2-486c-4629-b806-a9acbc8fbe5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 Words in TF-IDF Feature List:\n",
      " ['_avp' 'aarti' 'aas' 'aat' 'abhijeet' 'abilities' 'ability' 'abilityto'\n",
      " 'able' 'aboutnational' 'abridging' 'abroad' 'absolute'\n",
      " 'absorptionspectroscopy' 'academia' 'academic' 'academics' 'academy'\n",
      " 'acb' 'accept']\n"
     ]
    }
   ],
   "source": [
    "print(\"Top 20 Words in TF-IDF Feature List:\\n\", feature_names[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e8dc0dc8-c221-482f-829b-a1143af085e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     experienced professional mentor and coach tale...\n",
      "1     head talent acquisition leader and strategic p...\n",
      "2     talent acquisition and professional with exper...\n",
      "3                                               unknown\n",
      "4     over years experience ites bpo with leading gl...\n",
      "5     human resources business partner with demonstr...\n",
      "6     seasoned professional and experienced banker w...\n",
      "7     have been working practitioner for the past ye...\n",
      "8     passionate developing people and building comp...\n",
      "9     performance driven and accomplished profession...\n",
      "10                                              unknown\n",
      "11    administration coordinating handling operation...\n",
      "12                                              unknown\n",
      "13    experienced head rewards and operations with d...\n",
      "14    over years progressive experience domain acros...\n",
      "15    professional with years experience business pa...\n",
      "16                                              unknown\n",
      "17    recruitment professional years work experience...\n",
      "18                                              unknown\n",
      "19                                              unknown\n",
      "Name: clean_description, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df[\"clean_description\"].head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "86b2d061-f757-4c79-b499-9dd2b7feefc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 Words in TF-IDF Feature List:\n",
      " ['_avp' 'aarti' 'aas' 'aat' 'abhijeet' 'abilities' 'ability' 'abilityto'\n",
      " 'able' 'aboutnational' 'abridging' 'abroad' 'absolute'\n",
      " 'absorptionspectroscopy' 'academia' 'academic' 'academics' 'academy'\n",
      " 'acb' 'accept']\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words=\"english\", max_features=5000)\n",
    "X = vectorizer.fit_transform(df[\"clean_description\"])\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "print(\"Top 20 Words in TF-IDF Feature List:\\n\", feature_names[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "43c399b5-720d-42a4-b78a-7812ddc48e0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   index category                                           linkedin  \\\n",
      "0      1       HR  https://in.linkedin.com/in/sameer-wadhawan-b55...   \n",
      "1      2       HR  https://in.linkedin.com/in/adarsh-krishna-a4ab0a5   \n",
      "2      3       HR           https://in.linkedin.com/in/shrivas-mohit   \n",
      "3      4       HR       https://in.linkedin.com/in/hr-hopes-086734b8   \n",
      "4      5       HR           https://in.linkedin.com/in/rakeshkumar01   \n",
      "\n",
      "                                     profile_picture  \\\n",
      "0  https://media-exp1.licdn.com/dms/image/C5603AQ...   \n",
      "1  https://media-exp1.licdn.com/dms/image/C5103AQ...   \n",
      "2  data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP//...   \n",
      "3  data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP//...   \n",
      "4  https://media-exp1.licdn.com/dms/image/C5103AQ...   \n",
      "\n",
      "                                         description  \\\n",
      "0  An experienced HR professional,  HR mentor and...   \n",
      "1  Head Talent Acquisition, HR Leader and Strateg...   \n",
      "2  A Talent Acquisition and HR professional with ...   \n",
      "3                                                NaN   \n",
      "4  Over 18 Years of experience in IT /ITES  / BPO...   \n",
      "\n",
      "                                          Experience             Name  \\\n",
      "0  Senior Vice President & Head of HRCompany Name...  Sameer Wadhawan   \n",
      "1  Head of Talent Acquisition - India & APAC and ...   Adarsh Krishna   \n",
      "2  Company NameIBM INDIA Pvt LtdTotal Duration8 y...    Shrivas Mohit   \n",
      "3  HR/Admin/Personnel/LegalCompany NameHR and HR ...         HR Hopes   \n",
      "4  Company NameEXLTotal Duration6 yrs 4 mosTitleV...     Rakesh Kumar   \n",
      "\n",
      "                                            position  \\\n",
      "0  Senior Vice President and Head of HR- Samsung ...   \n",
      "1  Head Talent Acquisition and HR Leader for Heal...   \n",
      "2                                             HR@IBM   \n",
      "3                                                 HR   \n",
      "4  Vice President - Digital HR Transformation Lea...   \n",
      "\n",
      "                      location  \\\n",
      "0      Gurgaon, Haryana, India   \n",
      "1     Pune, Maharashtra, India   \n",
      "2  Bengaluru, Karnataka, India   \n",
      "3             Pune Area, India   \n",
      "4  Central Delhi, Delhi, India   \n",
      "\n",
      "                                              skills  \\\n",
      "0  ['\\nPerformance Management\\n', '\\nHuman Resour...   \n",
      "1  ['\\nTalent Acquisition\\n', '\\nEmployee Engagem...   \n",
      "2  ['\\nHuman Resources\\n', '\\nRecruiting\\n', '\\nT...   \n",
      "3                                                 []   \n",
      "4  ['\\nTeam Management\\n', '\\nHuman Resources\\n',...   \n",
      "\n",
      "                                        clean_skills  \n",
      "0  ['Performance Management', 'Human Resources', ...  \n",
      "1  ['Talent Acquisition', 'Employee Engagement', ...  \n",
      "2  ['Human Resources', 'Recruiting', 'Team Manage...  \n",
      "3                                               ['']  \n",
      "4  ['Team Management', 'Human Resources', 'Employ...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Reload original dataset\n",
    "file_path = r\"C:\\Users\\gopko\\Documents\\GitHub\\ML-Resume-Classifier\\data\\resume_data.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Show first 5 rows to confirm it's reset\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "067fe79a-f9e9-4326-8344-904265b96571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1251 entries, 0 to 1250\n",
      "Data columns (total 11 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   index            1251 non-null   int64 \n",
      " 1   category         1251 non-null   object\n",
      " 2   linkedin         1251 non-null   object\n",
      " 3   profile_picture  1239 non-null   object\n",
      " 4   description      670 non-null    object\n",
      " 5   Experience       1226 non-null   object\n",
      " 6   Name             1239 non-null   object\n",
      " 7   position         1239 non-null   object\n",
      " 8   location         1239 non-null   object\n",
      " 9   skills           1251 non-null   object\n",
      " 10  clean_skills     1251 non-null   object\n",
      "dtypes: int64(1), object(10)\n",
      "memory usage: 107.6+ KB\n",
      "None\n",
      "index                0\n",
      "category             0\n",
      "linkedin             0\n",
      "profile_picture     12\n",
      "description        581\n",
      "Experience          25\n",
      "Name                12\n",
      "position            12\n",
      "location            12\n",
      "skills               0\n",
      "clean_skills         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.info())  # Check columns & data types\n",
    "print(df.isnull().sum())  # Check missing values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f2825a8a-6c5f-49c0-8638-65333bdd9e94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    an experienced hr professional,  hr mentor and...\n",
      "1    head talent acquisition, hr leader and strateg...\n",
      "2    a talent acquisition and hr professional with ...\n",
      "3                                                  NaN\n",
      "4    over 18 years of experience in it /ites  / bpo...\n",
      "Name: clean_description, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df[\"clean_description\"] = df[\"description\"].str.lower()\n",
    "print(df[\"clean_description\"].head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "306a33c1-6686-4062-8d65-90669031f183",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object, got 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mremove_punctuations_numbers\u001b[39m(text):\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[^a-z\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms]\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, text)  \u001b[38;5;66;03m# Keep only letters & spaces\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclean_description\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mclean_description\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremove_punctuations_numbers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclean_description\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m5\u001b[39m))\n",
      "File \u001b[1;32m~\\Documents\\GitHub\\ML-Resume-Classifier\\resume-ml-env\\Lib\\site-packages\\pandas\\core\\series.py:4924\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4800\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4918\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4922\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m-> 4924\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\GitHub\\ML-Resume-Classifier\\resume-ml-env\\Lib\\site-packages\\pandas\\core\\apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[0;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[1;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\GitHub\\ML-Resume-Classifier\\resume-ml-env\\Lib\\site-packages\\pandas\\core\\apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[0;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[0;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[0;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[0;32m   1509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32m~\\Documents\\GitHub\\ML-Resume-Classifier\\resume-ml-env\\Lib\\site-packages\\pandas\\core\\base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[1;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\GitHub\\ML-Resume-Classifier\\resume-ml-env\\Lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[1;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[0;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[0;32m   1747\u001b[0m     )\n",
      "File \u001b[1;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[26], line 4\u001b[0m, in \u001b[0;36mremove_punctuations_numbers\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mremove_punctuations_numbers\u001b[39m(text):\n\u001b[1;32m----> 4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mre\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msub\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m[^a-z\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43ms]\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files\\Python311\\Lib\\re\\__init__.py:185\u001b[0m, in \u001b[0;36msub\u001b[1;34m(pattern, repl, string, count, flags)\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msub\u001b[39m(pattern, repl, string, count\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, flags\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m    179\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the string obtained by replacing the leftmost\u001b[39;00m\n\u001b[0;32m    180\u001b[0m \u001b[38;5;124;03m    non-overlapping occurrences of the pattern in string by the\u001b[39;00m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;124;03m    replacement repl.  repl can be either a string or a callable;\u001b[39;00m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;124;03m    if a string, backslash escapes in it are processed.  If it is\u001b[39;00m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;124;03m    a callable, it's passed the Match object and must return\u001b[39;00m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;124;03m    a replacement string to be used.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msub\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstring\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcount\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected string or bytes-like object, got 'float'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def remove_punctuations_numbers(text):\n",
    "    return re.sub(r\"[^a-z\\s]\", \"\", text)  # Keep only letters & spaces\n",
    "\n",
    "df[\"clean_description\"] = df[\"clean_description\"].apply(remove_punctuations_numbers)\n",
    "print(df[\"clean_description\"].head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a2b8064b-1ecf-497b-bccf-ecd0f521255f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    an experienced hr professional  hr mentor and ...\n",
      "1    head talent acquisition hr leader and strategi...\n",
      "2    a talent acquisition and hr professional with ...\n",
      "3                                                     \n",
      "4    over  years of experience in it ites   bpo wit...\n",
      "Name: clean_description, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def remove_punctuations_numbers(text):\n",
    "    if not isinstance(text, str):  # If text is NaN or not a string, convert it\n",
    "        return \"\"\n",
    "    return re.sub(r\"[^a-z\\s]\", \"\", text)  # Keep only letters & spaces\n",
    "\n",
    "df[\"clean_description\"] = df[\"clean_description\"].apply(remove_punctuations_numbers)\n",
    "print(df[\"clean_description\"].head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a028196c-dbec-4bcd-946b-2bddd2784ce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    an experienced hr professional  hr mentor and ...\n",
      "1    head talent acquisition hr leader and strategi...\n",
      "2    a talent acquisition and hr professional with ...\n",
      "3                                                     \n",
      "4    over  years of experience in it ites   bpo wit...\n",
      "5    human resources business partner with a demons...\n",
      "6    a seasoned hr professional and an experienced ...\n",
      "7    i have been working as an hr practitioner for ...\n",
      "8    passionate in developing people and building c...\n",
      "9    performance driven and accomplished profession...\n",
      "Name: clean_description, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df[\"clean_description\"].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "def17347-b955-45a1-a455-02fbe1380bf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(df[\"clean_description\"].isnull().sum())  # Check for NaN (missing) values\n",
    "print((df[\"clean_description\"] == \"\").sum())  # Count fully empty rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c94cb5ea-5053-4ed2-83b6-3ce7d007257e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df[\"clean_description\"] != \"\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "22b32684-5cdc-4407-a287-1842048ded7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     experienced professional mentor coach talent a...\n",
      "1     head talent acquisition leader strategic partn...\n",
      "2     talent acquisition professional experience soc...\n",
      "4     years experience ites leading global organizat...\n",
      "5     human resources business partner demonstrated ...\n",
      "6     seasoned professional experienced banker years...\n",
      "7     working practitioner past years attract engage...\n",
      "8     passionate developing people building competen...\n",
      "9     performance driven accomplished professional c...\n",
      "11    administration coordinating handling operation...\n",
      "Name: clean_description, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\gopko\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"Unknown\"  # Handle missing values\n",
    "    \n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r\"\\d+\", \"\", text)  # Remove numbers\n",
    "    text = re.sub(r\"[^\\w\\s]\", \" \", text)  # Remove punctuation\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()  # Remove extra spaces\n",
    "    words = [word for word in text.split() if word not in stop_words and len(word) > 3]  # Remove stopwords & short words\n",
    "    return \" \".join(words)\n",
    "\n",
    "# Apply improved cleaning function\n",
    "df[\"clean_description\"] = df[\"description\"].apply(clean_text)\n",
    "\n",
    "# Check results\n",
    "print(df[\"clean_description\"].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "786f6110-b357-4cc8-9dbb-3a945c1ae5d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(df[\"clean_description\"].isnull().sum())  # Check for missing values\n",
    "print((df[\"clean_description\"] == \"\").sum())  # Check for fully empty rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0d761afd-d65c-4ede-b637-aa3dd20fd214",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df[\"clean_description\"] != \"\"]  # Remove fully empty rows\n",
    "df = df.dropna(subset=[\"clean_description\"])  # Remove NaN values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ca0c2c65-56d4-4dd9-be80-75b9743612dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned dataset saved successfully!\n"
     ]
    }
   ],
   "source": [
    "df.to_csv(\"../data/resume_data_cleaned.csv\", index=False)\n",
    "print(\"Cleaned dataset saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1c40ee39-ac12-4947-a170-a4d87db9920a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\gopko\\documents\\github\\ml-resume-classifier\\resume-ml-env\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\gopko\\documents\\github\\ml-resume-classifier\\resume-ml-env\\lib\\site-packages (from scikit-learn) (2.0.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\gopko\\documents\\github\\ml-resume-classifier\\resume-ml-env\\lib\\site-packages (from scikit-learn) (1.15.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\gopko\\documents\\github\\ml-resume-classifier\\resume-ml-env\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\gopko\\documents\\github\\ml-resume-classifier\\resume-ml-env\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1b2bb26a-59fc-4e26-870b-86e4767327df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9cb16d1e-49d1-4802-a3fa-317545fc9469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF matrix shape: (668, 3585)\n"
     ]
    }
   ],
   "source": [
    "# Initialize TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\", max_features=5000)\n",
    "\n",
    "# Convert cleaned descriptions into numerical vectors\n",
    "X = vectorizer.fit_transform(df[\"clean_description\"])\n",
    "\n",
    "# Print the shape of transformed data\n",
    "print(\"TF-IDF matrix shape:\", X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dc0ea752-2c2c-4e68-a3fc-bfe3e8ef9cf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 Words in TF-IDF Feature List:\n",
      " ['_avp' 'aarti' 'abhijeet' 'abilities' 'ability' 'abilityto' 'able'\n",
      " 'aboutnational' 'abridging' 'abroad' 'absolute' 'absorptionspectroscopy'\n",
      " 'academia' 'academic' 'academics' 'academy' 'accept' 'access'\n",
      " 'accessories' 'accompanied']\n"
     ]
    }
   ],
   "source": [
    "# Get feature names (words)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Print the first 20 important words\n",
    "print(\"Top 20 Words in TF-IDF Feature List:\\n\", feature_names[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4cc8d309-6e61-489e-945a-8a54c043c375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_avp         0.0\n",
      "aarti        0.0\n",
      "abhijeet     0.0\n",
      "abilities    0.0\n",
      "ability      0.0\n",
      "            ... \n",
      "youtube      0.0\n",
      "yuva         0.0\n",
      "zealand      0.0\n",
      "zettler      0.0\n",
      "zone         0.0\n",
      "Name: 0, Length: 3585, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert TF-IDF matrix to a DataFrame\n",
    "tfidf_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# Print first resume vector\n",
    "print(tfidf_df.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e6b0f699-acdf-4eec-adde-b79a1fc35ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     experienced professional mentor coach talent a...\n",
      "1     head talent acquisition leader strategic partn...\n",
      "2     talent acquisition professional experience soc...\n",
      "4     years experience ites leading global organizat...\n",
      "5     human resources business partner demonstrated ...\n",
      "6     seasoned professional experienced banker years...\n",
      "7     working practitioner past years attract engage...\n",
      "8     passionate developing people building competen...\n",
      "9     performance driven accomplished professional c...\n",
      "11    administration coordinating handling operation...\n",
      "13    experienced head rewards operations demonstrat...\n",
      "14    years progressive experience domain across div...\n",
      "15    professional years experience business partner...\n",
      "17       recruitment professional years work experience\n",
      "20    proficient hrms implementation administration ...\n",
      "21    dynamic executive years comprehensive experien...\n",
      "22    analytics modeling expert business partner dat...\n",
      "25    experienced human resource capital management ...\n",
      "28    payroll attendance kronos management recruitme...\n",
      "29    firm believer human potential collectively cha...\n",
      "Name: clean_description, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df[\"clean_description\"].head(20))  # Show first 20 cleaned resumes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a0220234-204e-45a5-89e0-d2d43a342f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 Words in TF-IDF Feature List:\n",
      " ['ability' 'able' 'academy' 'accomplished' 'account' 'accountant'\n",
      " 'accounting' 'accounts' 'achieve' 'achieved' 'acquisition' 'active'\n",
      " 'activities' 'administration' 'advertising' 'advisor' 'advisory'\n",
      " 'advocate' 'affairs' 'agencies']\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "    stop_words=\"english\", \n",
    "    max_features=3000,  # Reduce number of words\n",
    "    min_df=5,  # Ignore words that appear in fewer than 5 resumes\n",
    "    max_df=0.9  # Ignore words that appear in more than 90% of resumes (too common)\n",
    ")\n",
    "\n",
    "# Reapply TF-IDF transformation\n",
    "X = vectorizer.fit_transform(df[\"clean_description\"])\n",
    "\n",
    "# Print top words\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "print(\"Top 20 Words in TF-IDF Feature List:\\n\", feature_names[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "861036f2-c866-4380-b96a-0571091f7925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 Words in TF-IDF Feature List:\n",
      " ['ability' 'able' 'academy' 'accomplished' 'account' 'accountant'\n",
      " 'accounting' 'accounts' 'achieve' 'achieved' 'acquisition' 'across'\n",
      " 'active' 'activities' 'administration' 'advertising' 'advisor' 'advisory'\n",
      " 'advocate' 'affairs']\n"
     ]
    }
   ],
   "source": [
    "custom_stop_words = [\"_avp\", \"aarti\", \"abhijeet\", \"zettler\", \"yuva\", \"zone\"]\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    stop_words=custom_stop_words,  \n",
    "    max_features=3000,\n",
    "    min_df=5,  \n",
    "    max_df=0.9\n",
    ")\n",
    "\n",
    "X = vectorizer.fit_transform(df[\"clean_description\"])\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "print(\"Top 20 Words in TF-IDF Feature List:\\n\", feature_names[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "46ef89b2-34a8-472f-9e16-9f77d45ae9ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 Words in TF-IDF Feature List:\n",
      " ['ability' 'able' 'academy' 'accomplished' 'account' 'accountant'\n",
      " 'accounting' 'accounts' 'achieve' 'achieved' 'acquisition' 'across'\n",
      " 'active' 'activities' 'administration' 'advertising' 'advisor' 'advisory'\n",
      " 'advocate' 'affairs']\n"
     ]
    }
   ],
   "source": [
    "print(\"Top 20 Words in TF-IDF Feature List:\\n\", feature_names[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "87386946-3728-45c7-8d29-b4e34caa5e89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 Words in TF-IDF Feature List:\n",
      " ['academy' 'accomplished' 'account' 'accountant' 'accounting' 'accounts'\n",
      " 'acquisition' 'active' 'administration' 'advertising' 'advisor'\n",
      " 'advisory' 'advocate' 'affairs' 'agencies' 'agency' 'agricultural'\n",
      " 'agriculture' 'airlines' 'also']\n"
     ]
    }
   ],
   "source": [
    "custom_stop_words = [\n",
    "    \"ability\", \"able\", \"across\", \"activities\", \"achieve\", \"achieved\"\n",
    "]\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    stop_words=custom_stop_words,  \n",
    "    max_features=3000,\n",
    "    min_df=5,  \n",
    "    max_df=0.9\n",
    ")\n",
    "\n",
    "# Apply TF-IDF again\n",
    "X = vectorizer.fit_transform(df[\"clean_description\"])\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Check top words\n",
    "print(\"Top 20 Words in TF-IDF Feature List:\\n\", feature_names[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "150909b8-2c85-4c22-ae0b-5371600008ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 Words in TF-IDF Feature List:\n",
      " ['ability' 'able' 'account' 'accountant' 'accounting' 'accounts' 'achieve'\n",
      " 'achieved' 'acquisition' 'across' 'activities' 'administration'\n",
      " 'advertising' 'advisor' 'advisory' 'advocate' 'always' 'analysis'\n",
      " 'analytical' 'analytics']\n"
     ]
    }
   ],
   "source": [
    "custom_stop_words = [\n",
    "    \"academy\", \"accomplished\", \"active\", \"affairs\", \"agencies\",\n",
    "    \"agency\", \"agricultural\", \"agriculture\", \"airlines\", \"also\"\n",
    "]\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    stop_words=custom_stop_words,  \n",
    "    max_features=3000,\n",
    "    min_df=5,  \n",
    "    max_df=0.9\n",
    ")\n",
    "\n",
    "# Apply TF-IDF again\n",
    "X = vectorizer.fit_transform(df[\"clean_description\"])\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Check top words\n",
    "print(\"Top 20 Words in TF-IDF Feature List:\\n\", feature_names[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "36b89289-17c2-419c-84a9-9f78ef454849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words before TF-IDF:\n",
      " ['experienced', 'professional', 'mentor', 'coach', 'talent', 'advisory', 'strategist']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\gopko\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "stop_words = set(stopwords.words(\"english\")).union(custom_stop_words)\n",
    "\n",
    "# Print first 30 words before TF-IDF to confirm stopwords are applied\n",
    "print(\"Words before TF-IDF:\\n\", df[\"clean_description\"].iloc[0].split()[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5aaa0f2f-137b-438f-92c7-1df298a93a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words in first resume after stopword removal: ['talent']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\gopko\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "stop_words = set(stopwords.words(\"english\")).union(set([\n",
    "    \"experienced\", \"professional\", \"mentor\", \"coach\", \"strategist\", \"advisory\",\n",
    "    \"expertise\", \"specialist\", \"career\", \"success\", \"background\", \"certified\",\n",
    "    \"ability\", \"proven\", \"passionate\", \"dedicated\", \"hardworking\", \"teamwork\"\n",
    "]))\n",
    "\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"Unknown\"  # Handle missing values\n",
    "    \n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r\"\\d+\", \"\", text)  # Remove numbers\n",
    "    text = re.sub(r\"[^\\w\\s]\", \" \", text)  # Remove punctuation\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()  # Remove extra spaces\n",
    "    words = [word for word in text.split() if word not in stop_words and len(word) > 3]  # Remove stopwords & short words\n",
    "    return \" \".join(words)\n",
    "\n",
    "# Apply improved cleaning function\n",
    "df[\"clean_description\"] = df[\"description\"].apply(clean_text)\n",
    "\n",
    "# Check if generic words are removed before TF-IDF\n",
    "print(\"Words in first resume after stopword removal:\", df[\"clean_description\"].iloc[0].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "93f7283c-690e-403d-bf2c-bcdb6a6e88f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 Words in TF-IDF Feature List:\n",
      " ['able' 'academy' 'accomplished' 'account' 'accountant' 'accounting'\n",
      " 'accounts' 'achieve' 'achieved' 'acquisition' 'active' 'activities'\n",
      " 'administration' 'advertising' 'advisor' 'advocate' 'affairs' 'agencies'\n",
      " 'agency' 'agricultural']\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words=\"english\", max_features=3000, min_df=5, max_df=0.9)\n",
    "X = vectorizer.fit_transform(df[\"clean_description\"])\n",
    "\n",
    "# Check top words\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "print(\"Top 20 Words in TF-IDF Feature List:\\n\", feature_names[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "58e173a2-3d6c-4baf-8067-940df1ac51c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 Words in TF-IDF Feature List:\n",
      " ['accountant' 'accounting' 'accounts' 'acquisition' 'across' 'agriculture'\n",
      " 'airlines' 'also' 'always' 'analysis' 'analytical' 'analytics' 'apparel'\n",
      " 'applications' 'approach' 'architect' 'architects' 'architecture' 'area'\n",
      " 'areas']\n"
     ]
    }
   ],
   "source": [
    "custom_stop_words = [\n",
    "    \"able\", \"academy\", \"accomplished\", \"active\", \"activities\",\n",
    "    \"administration\", \"affairs\", \"agencies\", \"agency\", \"agricultural\",\n",
    "    \"achieve\", \"achieved\", \"account\", \"advertising\", \"advisor\",\n",
    "    \"advocate\"\n",
    "]\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    stop_words=custom_stop_words,\n",
    "    max_features=3000,\n",
    "    min_df=5,\n",
    "    max_df=0.9\n",
    ")\n",
    "\n",
    "# Reapply TF-IDF transformation\n",
    "X = vectorizer.fit_transform(df[\"clean_description\"])\n",
    "\n",
    "# Print top words\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "print(\"Top 20 Words in TF-IDF Feature List:\\n\", feature_names[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "823def2d-ec29-4286-a7a0-c522a9741e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words in first resume after stopword removal: ['talent']\n"
     ]
    }
   ],
   "source": [
    "print(\"Words in first resume after stopword removal:\", df[\"clean_description\"].iloc[0].split()[:30])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d38c37af-937f-4c60-8da5-0131986bd8f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 Words in TF-IDF Feature List:\n",
      " ['accountant' 'accounting' 'accounts' 'acquisition' 'across' 'agriculture'\n",
      " 'airlines' 'also' 'always' 'analysis' 'analytical' 'analytics' 'apparel'\n",
      " 'applications' 'approach' 'architect' 'architects' 'architecture' 'area'\n",
      " 'areas']\n"
     ]
    }
   ],
   "source": [
    "X = vectorizer.fit_transform(df[\"clean_description\"])\n",
    "\n",
    "# Check top words\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "print(\"Top 20 Words in TF-IDF Feature List:\\n\", feature_names[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5c45d5a4-ea1f-441f-ae37-a232b460b67a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   index category                                           linkedin  \\\n",
      "0      1       HR  https://in.linkedin.com/in/sameer-wadhawan-b55...   \n",
      "1      2       HR  https://in.linkedin.com/in/adarsh-krishna-a4ab0a5   \n",
      "2      3       HR           https://in.linkedin.com/in/shrivas-mohit   \n",
      "3      4       HR       https://in.linkedin.com/in/hr-hopes-086734b8   \n",
      "4      5       HR           https://in.linkedin.com/in/rakeshkumar01   \n",
      "\n",
      "                                     profile_picture  \\\n",
      "0  https://media-exp1.licdn.com/dms/image/C5603AQ...   \n",
      "1  https://media-exp1.licdn.com/dms/image/C5103AQ...   \n",
      "2  data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP//...   \n",
      "3  data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP//...   \n",
      "4  https://media-exp1.licdn.com/dms/image/C5103AQ...   \n",
      "\n",
      "                                         description  \\\n",
      "0  An experienced HR professional,  HR mentor and...   \n",
      "1  Head Talent Acquisition, HR Leader and Strateg...   \n",
      "2  A Talent Acquisition and HR professional with ...   \n",
      "3                                                NaN   \n",
      "4  Over 18 Years of experience in IT /ITES  / BPO...   \n",
      "\n",
      "                                          Experience             Name  \\\n",
      "0  Senior Vice President & Head of HRCompany Name...  Sameer Wadhawan   \n",
      "1  Head of Talent Acquisition - India & APAC and ...   Adarsh Krishna   \n",
      "2  Company NameIBM INDIA Pvt LtdTotal Duration8 y...    Shrivas Mohit   \n",
      "3  HR/Admin/Personnel/LegalCompany NameHR and HR ...         HR Hopes   \n",
      "4  Company NameEXLTotal Duration6 yrs 4 mosTitleV...     Rakesh Kumar   \n",
      "\n",
      "                                            position  \\\n",
      "0  Senior Vice President and Head of HR- Samsung ...   \n",
      "1  Head Talent Acquisition and HR Leader for Heal...   \n",
      "2                                             HR@IBM   \n",
      "3                                                 HR   \n",
      "4  Vice President - Digital HR Transformation Lea...   \n",
      "\n",
      "                      location  \\\n",
      "0      Gurgaon, Haryana, India   \n",
      "1     Pune, Maharashtra, India   \n",
      "2  Bengaluru, Karnataka, India   \n",
      "3             Pune Area, India   \n",
      "4  Central Delhi, Delhi, India   \n",
      "\n",
      "                                              skills  \\\n",
      "0  ['\\nPerformance Management\\n', '\\nHuman Resour...   \n",
      "1  ['\\nTalent Acquisition\\n', '\\nEmployee Engagem...   \n",
      "2  ['\\nHuman Resources\\n', '\\nRecruiting\\n', '\\nT...   \n",
      "3                                                 []   \n",
      "4  ['\\nTeam Management\\n', '\\nHuman Resources\\n',...   \n",
      "\n",
      "                                        clean_skills  \n",
      "0  ['Performance Management', 'Human Resources', ...  \n",
      "1  ['Talent Acquisition', 'Employee Engagement', ...  \n",
      "2  ['Human Resources', 'Recruiting', 'Team Manage...  \n",
      "3                                               ['']  \n",
      "4  ['Team Management', 'Human Resources', 'Employ...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Reload the original dataset\n",
    "file_path = r\"C:\\Users\\gopko\\Documents\\GitHub\\ML-Resume-Classifier\\data\\resume_data.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Show first few rows to confirm it's reset\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5996d1ca-0b41-409f-96f6-6a5cc29347d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words in first resume after cleaning: ['talent']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\gopko\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "stop_words = set(stopwords.words(\"english\")).union(set([\n",
    "    \"experienced\", \"professional\", \"mentor\", \"coach\", \"strategist\", \"advisory\",\n",
    "    \"expertise\", \"specialist\", \"career\", \"success\", \"background\", \"certified\",\n",
    "    \"ability\", \"proven\", \"passionate\", \"dedicated\", \"hardworking\", \"teamwork\"\n",
    "]))\n",
    "\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"Unknown\"  # Handle missing values\n",
    "    \n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r\"\\d+\", \"\", text)  # Remove numbers\n",
    "    text = re.sub(r\"[^\\w\\s]\", \" \", text)  # Remove punctuation\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()  # Remove extra spaces\n",
    "    words = [word for word in text.split() if word not in stop_words and len(word) > 3]  # Remove stopwords & short words\n",
    "    return \" \".join(words)\n",
    "\n",
    "# Apply cleaning function\n",
    "df[\"clean_description\"] = df[\"description\"].apply(clean_text)\n",
    "\n",
    "# Print cleaned descriptions before TF-IDF\n",
    "print(\"Words in first resume after cleaning:\", df[\"clean_description\"].iloc[0].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5e4e206f-339d-432a-bdcc-a581c40d846c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 Words in TF-IDF Feature List:\n",
      " ['account' 'accounting' 'acquisition' 'administration' 'agency' 'analysis'\n",
      " 'analytics' 'approach' 'architecture' 'areas' 'arts' 'automobile'\n",
      " 'aviation' 'bachelor' 'banking' 'based' 'believe' 'best' 'beverage'\n",
      " 'brand']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    stop_words=\"english\",  \n",
    "    max_features=3000,\n",
    "    min_df=10,  # Ignore words that appear in fewer than 10 resumes\n",
    "    max_df=0.9  # Ignore words that appear in more than 90% of resumes\n",
    ")\n",
    "\n",
    "# Apply TF-IDF again\n",
    "X = vectorizer.fit_transform(df[\"clean_description\"])\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Print top words\n",
    "print(\"Top 20 Words in TF-IDF Feature List:\\n\", feature_names[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a9a631ad-27ce-4102-9bdb-88922415f451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 Words in TF-IDF Feature List:\n",
      " ['account' 'accounting' 'acquisition' 'administration' 'agency' 'analysis'\n",
      " 'analytics' 'approach' 'architecture' 'areas' 'arts' 'automobile'\n",
      " 'aviation' 'bachelor' 'banking' 'based' 'believe' 'best' 'beverage'\n",
      " 'brand']\n"
     ]
    }
   ],
   "source": [
    "print(\"Top 20 Words in TF-IDF Feature List:\\n\", feature_names[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e20937d5-2961-452d-a4a3-5e9a93a8940f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Category                                             Resume\n",
      "0  Data Science  Skills * Programming Languages: Python (pandas...\n",
      "1  Data Science  Education Details \\r\\nMay 2013 to May 2017 B.E...\n",
      "2  Data Science  Areas of Interest Deep Learning, Control Syste...\n",
      "3  Data Science  Skills â¢ R â¢ Python â¢ SAP HANA â¢ Table...\n",
      "4  Data Science  Education Details \\r\\n MCA   YMCAUST,  Faridab...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the new dataset\n",
    "file_path = r\"C:\\Users\\gopko\\Documents\\GitHub\\ML-Resume-Classifier\\data\\Resume.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Show the first few rows\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e12dd1ab-f196-4eff-848f-537ecfe916a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Category', 'Resume'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d03adc60-0bcd-4f74-b322-7a1b8a19587f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Category                                             Resume\n",
      "0  Data Science  Skills * Programming Languages: Python (pandas...\n",
      "1  Data Science  Education Details \\r\\nMay 2013 to May 2017 B.E...\n",
      "2  Data Science  Areas of Interest Deep Learning, Control Syste...\n",
      "3  Data Science  Skills â¢ R â¢ Python â¢ SAP HANA â¢ Table...\n",
      "4  Data Science  Education Details \\r\\n MCA   YMCAUST,  Faridab...\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 962 entries, 0 to 961\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   Category  962 non-null    object\n",
      " 1   Resume    962 non-null    object\n",
      "dtypes: object(2)\n",
      "memory usage: 15.2+ KB\n",
      "None\n",
      "Columns in dataset: Index(['Category', 'Resume'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the new dataset\n",
    "file_path = r\"C:\\Users\\gopko\\Documents\\GitHub\\ML-Resume-Classifier\\data\\Resume.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Show first few rows\n",
    "print(df.head())\n",
    "\n",
    "# Show dataset structure (columns, data types, missing values)\n",
    "print(df.info())\n",
    "\n",
    "# Show column names\n",
    "print(\"Columns in dataset:\", df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ccce58e1-8c17-4fb6-8b01-9f4ab6fa513e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Category', 'Resume'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0d81e879-5d2c-4877-8083-fcbcf2921f83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       category                                  clean_description\n",
      "0  Data Science  Skills * Programming Languages: Python (pandas...\n",
      "1  Data Science  Education Details \\r\\nMay 2013 to May 2017 B.E...\n",
      "2  Data Science  Areas of Interest Deep Learning, Control Syste...\n",
      "3  Data Science  Skills â¢ R â¢ Python â¢ SAP HANA â¢ Table...\n",
      "4  Data Science  Education Details \\r\\n MCA   YMCAUST,  Faridab...\n"
     ]
    }
   ],
   "source": [
    "df.rename(columns={\"Resume\": \"clean_description\", \"Category\": \"category\"}, inplace=True)\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d747f824-9ee9-4d61-9693-bbe38f066a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\gopko\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Resume'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\Documents\\GitHub\\ML-Resume-Classifier\\resume-ml-env\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Resume'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[63], line 24\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(words)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Apply cleaning function\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclean_description\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mResume\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mapply(clean_text)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Show first cleaned resumes\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclean_description\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m10\u001b[39m))\n",
      "File \u001b[1;32m~\\Documents\\GitHub\\ML-Resume-Classifier\\resume-ml-env\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\Documents\\GitHub\\ML-Resume-Classifier\\resume-ml-env\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Resume'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "stop_words = set(stopwords.words(\"english\")).union(set([\n",
    "    \"experienced\", \"professional\", \"mentor\", \"coach\", \"strategist\", \"advisory\",\n",
    "    \"expertise\", \"specialist\", \"career\", \"success\", \"background\", \"certified\",\n",
    "    \"ability\", \"proven\", \"passionate\", \"dedicated\", \"hardworking\", \"teamwork\"\n",
    "]))\n",
    "\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"Unknown\"  # Handle missing values\n",
    "    \n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r\"\\d+\", \"\", text)  # Remove numbers\n",
    "    text = re.sub(r\"[^\\w\\s]\", \" \", text)  # Remove punctuation\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()  # Remove extra spaces\n",
    "    words = [word for word in text.split() if word not in stop_words and len(word) > 3]  # Remove stopwords & short words\n",
    "    return \" \".join(words)\n",
    "\n",
    "# Apply cleaning function\n",
    "df[\"clean_description\"] = df[\"Resume\"].apply(clean_text)\n",
    "\n",
    "# Show first cleaned resumes\n",
    "print(df[\"clean_description\"].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a6613e4f-f5f5-49a9-b7c0-b5d062b41687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Columns: Index(['category', 'clean_description'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataset Columns:\", df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f10b8c13-a68e-4f8f-a7c6-2949918c6a92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Columns: Index(['category', 'clean_description'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "df.rename(columns={\"actual_column_name\": \"Resume\"}, inplace=True)\n",
    "print(\"Updated Columns:\", df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1731218d-6883-46da-b0b0-db55cda46ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       category                                  clean_description\n",
      "0  Data Science  Skills * Programming Languages: Python (pandas...\n",
      "1  Data Science  Education Details \\r\\nMay 2013 to May 2017 B.E...\n",
      "2  Data Science  Areas of Interest Deep Learning, Control Syste...\n",
      "3  Data Science  Skills â¢ R â¢ Python â¢ SAP HANA â¢ Table...\n",
      "4  Data Science  Education Details \\r\\n MCA   YMCAUST,  Faridab...\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 962 entries, 0 to 961\n",
      "Data columns (total 2 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   category           962 non-null    object\n",
      " 1   clean_description  962 non-null    object\n",
      "dtypes: object(2)\n",
      "memory usage: 15.2+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df.head())  # Show first few rows\n",
    "print(df.info())  # Show data types and missing values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f5cba2e3-1798-4392-89f1-69a9d2772f0c",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Resume'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\Documents\\GitHub\\ML-Resume-Classifier\\resume-ml-env\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Resume'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[67], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclean_description\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mResume\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\u001b[38;5;241m.\u001b[39mapply(clean_text)\n",
      "File \u001b[1;32m~\\Documents\\GitHub\\ML-Resume-Classifier\\resume-ml-env\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\Documents\\GitHub\\ML-Resume-Classifier\\resume-ml-env\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Resume'"
     ]
    }
   ],
   "source": [
    "df[\"clean_description\"] = df[\"Resume\"].astype(str).apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0160f78c-b672-4ff1-8a2e-07058d217b1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Columns: Index(['category', 'clean_description'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataset Columns:\", df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "00fc45f3-22d8-449e-9501-63d69aa64d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"clean_description\"] = df[\"clean_description\"].astype(str).apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3f333474-0f1e-4c99-b96c-e0063126472b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    skills programming languages python pandas num...\n",
      "1    education details rgpv data scientist data sci...\n",
      "2    areas interest deep learning control system de...\n",
      "3    skills python hana tableau hana hana lumira li...\n",
      "4    education details ymcaust faridabad haryana da...\n",
      "5    skills basics python matlab data science machi...\n",
      "6    skills python tableau data visualization studi...\n",
      "7    education details tech rayat bahra institute e...\n",
      "8    personal skills quickly grasp technical aspect...\n",
      "9    data quantitative analysis decision analytics ...\n",
      "Name: clean_description, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df[\"clean_description\"].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "275cfb84-ff93-48b1-bacd-082d920613d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned dataset saved successfully!\n"
     ]
    }
   ],
   "source": [
    "df.to_csv(r\"C:\\Users\\gopko\\Documents\\GitHub\\ML-Resume-Classifier\\data\\resume_data_cleaned.csv\", index=False)\n",
    "print(\"Cleaned dataset saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c4e1d3f2-5415-438d-94e9-0f40c5eb4bc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       category                                  clean_description\n",
      "0  Data Science  skills programming languages python pandas num...\n",
      "1  Data Science  education details rgpv data scientist data sci...\n",
      "2  Data Science  areas interest deep learning control system de...\n",
      "3  Data Science  skills python hana tableau hana hana lumira li...\n",
      "4  Data Science  education details ymcaust faridabad haryana da...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the cleaned dataset\n",
    "file_path = r\"C:\\Users\\gopko\\Documents\\GitHub\\ML-Resume-Classifier\\data\\resume_data_cleaned.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Show first few rows\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "afaf49d8-f0eb-43c3-a893-0505928cc65a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF matrix shape: (962, 3000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer(\n",
    "    stop_words=\"english\",  # Removes common words (e.g., \"the\", \"is\", \"and\")\n",
    "    max_features=3000,  # Keep only the top 3000 most important words\n",
    "    min_df=10,  # Ignore words that appear in fewer than 10 resumes\n",
    "    max_df=0.9  # Ignore words that appear in more than 90% of resumes (too common)\n",
    ")\n",
    "\n",
    "# Convert cleaned resumes into numerical vectors\n",
    "X = vectorizer.fit_transform(df[\"clean_description\"])\n",
    "\n",
    "# Print the shape of transformed data\n",
    "print(\"TF-IDF matrix shape:\", X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b0934578-9328-462f-af50-3822a429c782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 Words in TF-IDF Feature List:\n",
      " ['abacus' 'abilities' 'able' 'abridged' 'absence' 'academic' 'academy'\n",
      " 'accenture' 'accept' 'acceptance' 'accepted' 'accepting' 'access'\n",
      " 'accesses' 'accessories' 'accordance' 'according' 'accordingly' 'account'\n",
      " 'accountability']\n"
     ]
    }
   ],
   "source": [
    "# Get feature names (words)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Print the first 20 important words\n",
    "print(\"Top 20 Words in TF-IDF Feature List:\\n\", feature_names[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "988d882c-c8f3-4121-b2ca-8a0ecf27e737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words in first cleaned resume: ['skills', 'programming', 'languages', 'python', 'pandas', 'numpy', 'scipy', 'scikit', 'learn', 'matplotlib', 'java', 'javascript', 'jquery', 'machine', 'learning', 'regression', 'bayes', 'random', 'forest', 'decision', 'trees', 'boosting', 'techniques', 'cluster', 'analysis', 'word', 'embedding', 'sentiment', 'analysis', 'natural', 'language', 'processing', 'dimensionality', 'reduction', 'topic', 'modelling', 'neural', 'nets', 'database', 'visualizations', 'mysql', 'sqlserver', 'cassandra', 'hbase', 'elasticsearch', 'plotly', 'kibana', 'matplotlib', 'ggplot', 'tableau', 'others', 'regular', 'expression', 'html', 'angular', 'logstash', 'kafka', 'python', 'flask', 'docker', 'computer', 'vision', 'open', 'understanding', 'deep', 'learning', 'education', 'details', 'data', 'science', 'assurance', 'associate', 'data', 'science', 'assurance', 'associate', 'ernst', 'young', 'skill', 'details', 'javascript', 'exprience', 'months', 'jquery', 'exprience', 'months', 'python', 'exprience', 'monthscompany', 'details', 'company', 'ernst', 'young', 'description', 'fraud', 'investigations', 'dispute', 'services', 'assurance', 'technology', 'assisted', 'review', 'technology', 'assisted', 'review', 'assists', 'accelerating', 'review', 'process', 'analytics', 'generate', 'reports', 'core', 'member', 'team', 'helped', 'developing', 'automated', 'review', 'platform', 'tool', 'scratch', 'assisting', 'discovery', 'domain', 'tool', 'implements', 'predictive', 'coding', 'topic', 'modelling', 'automating', 'reviews', 'resulting', 'reduced', 'labor', 'costs', 'time', 'spent', 'lawyers', 'review', 'understand', 'flow', 'solution', 'research', 'development', 'classification', 'models', 'predictive', 'analysis', 'mining', 'information', 'present', 'text', 'data', 'worked', 'analyzing', 'outputs', 'precision', 'monitoring', 'entire', 'tool', 'assists', 'predictive', 'coding', 'topic', 'modelling', 'evidence', 'following', 'standards', 'developed', 'classifier', 'models', 'order', 'identify', 'flags', 'fraud', 'related', 'issues', 'tools', 'technologies', 'python', 'scikit', 'learn', 'tfidf', 'wordvec', 'docvec', 'cosine', 'similarity', 'bayes', 'topic', 'modelling', 'vader', 'text', 'blob', 'sentiment', 'analysis', 'matplot', 'tableau', 'dashboard', 'reporting', 'multiple', 'data', 'science', 'analytic', 'projects', 'clients', 'text', 'analytics', 'motor', 'vehicle', 'customer', 'review', 'data', 'received', 'customer', 'feedback', 'survey', 'data', 'past', 'year', 'performed', 'sentiment', 'positive', 'negative', 'neutral', 'time', 'series', 'analysis', 'customer', 'comments', 'across', 'categories', 'created', 'heat', 'terms', 'survey', 'category', 'based', 'frequency', 'words', 'extracted', 'positive', 'negative', 'words', 'across', 'survey', 'categories', 'plotted', 'word', 'cloud', 'created', 'customized', 'tableau', 'dashboards', 'effective', 'reporting', 'visualizations', 'chatbot', 'developed', 'user', 'friendly', 'chatbot', 'products', 'handle', 'simple', 'questions', 'hours', 'operation', 'reservation', 'options', 'chat', 'serves', 'entire', 'product', 'related', 'questions', 'giving', 'overview', 'tool', 'platform', 'also', 'give', 'recommendation', 'responses', 'user', 'question', 'build', 'chain', 'relevant', 'answer', 'intelligence', 'build', 'pipeline', 'questions', 'user', 'requirement', 'asks', 'relevant', 'recommended', 'questions', 'tools', 'technologies', 'python', 'natural', 'language', 'processing', 'nltk', 'spacy', 'topic', 'modelling', 'sentiment', 'analysis', 'word', 'embedding', 'scikit', 'learn', 'javascript', 'jquery', 'sqlserver', 'information', 'governance', 'organizations', 'make', 'informed', 'decisions', 'information', 'store', 'integrated', 'information', 'governance', 'portfolio', 'synthesizes', 'intelligence', 'across', 'unstructured', 'data', 'sources', 'facilitates', 'action', 'ensure', 'organizations', 'best', 'positioned', 'counter', 'information', 'risk', 'scan', 'data', 'multiple', 'sources', 'formats', 'parse', 'different', 'file', 'formats', 'extract', 'meta', 'data', 'information', 'push', 'results', 'indexing', 'elastic', 'search', 'created', 'customized', 'interactive', 'dashboards', 'using', 'kibana', 'preforming', 'analysis', 'data', 'give', 'information', 'data', 'helps', 'identify', 'content', 'either', 'redundant', 'outdated', 'trivial', 'preforming', 'full', 'text', 'search', 'analysis', 'elastic', 'search', 'predefined', 'methods', 'personally', 'identifiable', 'information', 'social', 'security', 'numbers', 'addresses', 'names', 'frequently', 'targeted', 'cyber', 'attacks', 'tools', 'technologies', 'python', 'flask', 'elastic', 'search', 'kibana', 'fraud', 'analytic', 'platform', 'fraud', 'analytics', 'investigative', 'platform', 'review', 'flag', 'cases', 'fraud', 'analytics', 'investigative', 'platform', 'inbuilt', 'case', 'manager', 'suite', 'analytics', 'various', 'systems', 'used', 'clients', 'interrogate', 'accounting', 'systems', 'identifying', 'anomalies', 'indicators', 'fraud', 'running', 'advanced', 'analytics', 'tools', 'technologies', 'html', 'javascript', 'sqlserver', 'jquery', 'bootstrap', 'node']\n"
     ]
    }
   ],
   "source": [
    "print(\"Words in first cleaned resume:\", df[\"clean_description\"].iloc[0].split())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "6a998b33-f7ba-4fbf-8928-73feb225168c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\gopko\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words in first cleaned resume: ['skills', 'programming', 'languages', 'python', 'pandas', 'numpy', 'scipy', 'scikit', 'learn', 'matplotlib', 'java', 'javascript', 'jquery', 'machine', 'learning', 'regression', 'bayes', 'random', 'forest', 'decision', 'trees', 'boosting', 'techniques', 'cluster', 'analysis', 'word', 'embedding', 'sentiment', 'analysis', 'natural', 'language', 'processing', 'dimensionality', 'reduction', 'topic', 'modelling', 'neural', 'nets', 'database', 'visualizations', 'mysql', 'sqlserver', 'cassandra', 'hbase', 'elasticsearch', 'plotly', 'kibana', 'matplotlib', 'ggplot', 'tableau', 'others', 'regular', 'expression', 'html', 'angular', 'logstash', 'kafka', 'python', 'flask', 'docker', 'computer', 'vision', 'open', 'understanding', 'deep', 'learning', 'education', 'details', 'data', 'science', 'assurance', 'associate', 'data', 'science', 'assurance', 'associate', 'ernst', 'young', 'skill', 'details', 'javascript', 'exprience', 'months', 'jquery', 'exprience', 'months', 'python', 'exprience', 'monthscompany', 'details', 'company', 'ernst', 'young', 'description', 'fraud', 'investigations', 'dispute', 'services', 'assurance', 'technology', 'assisted', 'review', 'technology', 'assisted', 'review', 'assists', 'accelerating', 'review', 'process', 'analytics', 'generate', 'reports', 'core', 'member', 'team', 'helped', 'developing', 'automated', 'review', 'platform', 'tool', 'scratch', 'assisting', 'discovery', 'domain', 'tool', 'implements', 'predictive', 'coding', 'topic', 'modelling', 'automating', 'reviews', 'resulting', 'reduced', 'labor', 'costs', 'time', 'spent', 'lawyers', 'review', 'understand', 'flow', 'solution', 'research', 'development', 'classification', 'models', 'predictive', 'analysis', 'mining', 'information', 'present', 'text', 'data', 'worked', 'analyzing', 'outputs', 'precision', 'monitoring', 'entire', 'tool', 'assists', 'predictive', 'coding', 'topic', 'modelling', 'evidence', 'following', 'standards', 'developed', 'classifier', 'models', 'order', 'identify', 'flags', 'fraud', 'related', 'issues', 'tools', 'technologies', 'python', 'scikit', 'learn', 'tfidf', 'wordvec', 'docvec', 'cosine', 'similarity', 'bayes', 'topic', 'modelling', 'vader', 'text', 'blob', 'sentiment', 'analysis', 'matplot', 'tableau', 'dashboard', 'reporting', 'multiple', 'data', 'science', 'analytic', 'projects', 'clients', 'text', 'analytics', 'motor', 'vehicle', 'customer', 'review', 'data', 'received', 'customer', 'feedback', 'survey', 'data', 'past', 'year', 'performed', 'sentiment', 'positive', 'negative', 'neutral', 'time', 'series', 'analysis', 'customer', 'comments', 'across', 'categories', 'created', 'heat', 'terms', 'survey', 'category', 'based', 'frequency', 'words', 'extracted', 'positive', 'negative', 'words', 'across', 'survey', 'categories', 'plotted', 'word', 'cloud', 'created', 'customized', 'tableau', 'dashboards', 'effective', 'reporting', 'visualizations', 'chatbot', 'developed', 'user', 'friendly', 'chatbot', 'products', 'handle', 'simple', 'questions', 'hours', 'operation', 'reservation', 'options', 'chat', 'serves', 'entire', 'product', 'related', 'questions', 'giving', 'overview', 'tool', 'platform', 'also', 'give', 'recommendation', 'responses', 'user', 'question', 'build', 'chain', 'relevant', 'answer', 'intelligence', 'build', 'pipeline', 'questions', 'user', 'requirement', 'asks', 'relevant', 'recommended', 'questions', 'tools', 'technologies', 'python', 'natural', 'language', 'processing', 'nltk', 'spacy', 'topic', 'modelling', 'sentiment', 'analysis', 'word', 'embedding', 'scikit', 'learn', 'javascript', 'jquery', 'sqlserver', 'information', 'governance', 'organizations', 'make', 'informed', 'decisions', 'information', 'store', 'integrated', 'information', 'governance', 'portfolio', 'synthesizes', 'intelligence', 'across', 'unstructured', 'data', 'sources', 'facilitates', 'action', 'ensure', 'organizations', 'best', 'positioned', 'counter', 'information', 'risk', 'scan', 'data', 'multiple', 'sources', 'formats', 'parse', 'different', 'file', 'formats', 'extract', 'meta', 'data', 'information', 'push', 'results', 'indexing', 'elastic', 'search', 'created', 'customized', 'interactive', 'dashboards', 'using', 'kibana', 'preforming', 'analysis', 'data', 'give', 'information', 'data', 'helps', 'identify', 'content', 'either', 'redundant', 'outdated', 'trivial', 'preforming', 'full', 'text', 'search', 'analysis', 'elastic', 'search', 'predefined', 'methods', 'personally', 'identifiable', 'information', 'social', 'security', 'numbers', 'addresses', 'names', 'frequently', 'targeted', 'cyber', 'attacks', 'tools', 'technologies', 'python', 'flask', 'elastic', 'search', 'kibana', 'fraud', 'analytic', 'platform', 'fraud', 'analytics', 'investigative', 'platform', 'review', 'flag', 'cases', 'fraud', 'analytics', 'investigative', 'platform', 'inbuilt', 'case', 'manager', 'suite', 'analytics', 'various', 'systems', 'used', 'clients', 'interrogate', 'accounting', 'systems', 'identifying', 'anomalies', 'indicators', 'fraud', 'running', 'advanced', 'analytics', 'tools', 'technologies', 'html', 'javascript', 'sqlserver', 'jquery', 'bootstrap', 'node']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "stop_words = set(stopwords.words(\"english\")).union(set([\n",
    "    \"abilities\", \"able\", \"academic\", \"academy\", \"accenture\", \"access\",\n",
    "    \"account\", \"accept\", \"acceptance\", \"accepted\", \"abridged\",\n",
    "    \"absence\", \"accessories\", \"accordance\", \"according\", \"accordingly\"\n",
    "]))\n",
    "\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"Unknown\"  # Handle missing values\n",
    "    \n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r\"\\d+\", \"\", text)  # Remove numbers\n",
    "    text = re.sub(r\"[^\\w\\s]\", \" \", text)  # Remove punctuation\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()  # Remove extra spaces\n",
    "    words = [word for word in text.split() if word not in stop_words and len(word) > 3]  # Remove stopwords & short words\n",
    "    return \" \".join(words)\n",
    "\n",
    "# Reapply cleaning function\n",
    "df[\"clean_description\"] = df[\"clean_description\"].apply(clean_text)\n",
    "\n",
    "# Verify cleaning worked before TF-IDF\n",
    "print(\"Words in first cleaned resume:\", df[\"clean_description\"].iloc[0].split())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "2103da0a-719a-434b-8619-13d31276f644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 Words in TF-IDF Feature List:\n",
      " ['accounting' 'accounts' 'accuracy' 'accurate' 'accurately' 'achieve'\n",
      " 'achieved' 'achievements' 'achieving' 'acquired' 'acquiring' 'action'\n",
      " 'actions' 'active' 'actively' 'activities' 'activity' 'actual' 'actuator'\n",
      " 'actuators']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    stop_words=\"english\",\n",
    "    max_features=3000,\n",
    "    min_df=15,  # Ignore words appearing in fewer than 15 resumes\n",
    "    max_df=0.85  # Ignore words appearing in more than 85% of resumes\n",
    ")\n",
    "\n",
    "# Apply TF-IDF again\n",
    "X = vectorizer.fit_transform(df[\"clean_description\"])\n",
    "\n",
    "# Print top words\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "print(\"Top 20 Words in TF-IDF Feature List:\\n\", feature_names[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "bd9c38a3-31aa-47e5-8784-c446c9e4514c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 Words in TF-IDF Feature List:\n",
      " ['accounting' 'accounts' 'accuracy' 'accurate' 'accurately' 'achieve'\n",
      " 'achieved' 'achievements' 'achieving' 'acquired' 'acquiring' 'action'\n",
      " 'actions' 'active' 'actively' 'activities' 'activity' 'actual' 'actuator'\n",
      " 'actuators']\n"
     ]
    }
   ],
   "source": [
    "print(\"Top 20 Words in TF-IDF Feature List:\\n\", feature_names[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "54d14927-1067-40eb-87b9-e403f39a9fc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words in first cleaned resume: ['skills', 'programming', 'languages', 'python', 'pandas', 'numpy', 'scipy', 'scikit', 'learn', 'matplotlib', 'java', 'javascript', 'jquery', 'machine', 'learning', 'regression', 'bayes', 'random', 'forest', 'decision', 'trees', 'boosting', 'techniques', 'cluster', 'analysis', 'word', 'embedding', 'sentiment', 'analysis', 'natural']\n"
     ]
    }
   ],
   "source": [
    "print(\"Words in first cleaned resume:\", df[\"clean_description\"].iloc[0].split()[:30])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "95794853-95f2-4235-a6b9-37810629c655",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\gopko\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words in first cleaned resume after stopword removal: ['skills', 'programming', 'languages', 'python', 'pandas', 'numpy', 'scipy', 'scikit', 'learn', 'matplotlib', 'java', 'javascript', 'jquery', 'machine', 'learning', 'regression', 'bayes', 'random', 'forest', 'decision', 'trees', 'boosting', 'techniques', 'cluster', 'analysis', 'word', 'embedding', 'sentiment', 'analysis', 'natural']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "custom_stop_words = set(stopwords.words(\"english\")).union(set([\n",
    "    \"accounting\", \"accounts\", \"accuracy\", \"accurate\", \"achieve\", \"achieved\",\n",
    "    \"achievements\", \"acquired\", \"action\", \"actions\", \"active\", \"actively\",\n",
    "    \"activities\", \"activity\", \"actual\", \"administration\", \"advisor\"\n",
    "]))\n",
    "\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"Unknown\"  # Handle missing values\n",
    "    \n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r\"\\d+\", \"\", text)  # Remove numbers\n",
    "    text = re.sub(r\"[^\\w\\s]\", \" \", text)  # Remove punctuation\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()  # Remove extra spaces\n",
    "    words = [word for word in text.split() if word not in custom_stop_words and len(word) > 3]  # Remove stopwords & short words\n",
    "    return \" \".join(words)\n",
    "\n",
    "# Apply cleaning function\n",
    "df[\"clean_description\"] = df[\"clean_description\"].apply(clean_text)\n",
    "\n",
    "# Verify stopwords were removed\n",
    "print(\"Words in first cleaned resume after stopword removal:\", df[\"clean_description\"].iloc[0].split()[:30])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "76de5055-19a8-4ff4-af4f-ae2fd8957d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 Words in TF-IDF Feature List:\n",
      " ['accurately' 'achieving' 'actuators' 'added' 'adding' 'additional'\n",
      " 'address' 'addressed' 'adherence' 'admin' 'administrative'\n",
      " 'administrator' 'advance' 'advanced' 'advertising' 'advocate' 'agents'\n",
      " 'agile' 'ajax' 'alarm']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    stop_words=\"english\",\n",
    "    max_features=3000,\n",
    "    min_df=20,  # Ignore words appearing in fewer than 20 resumes\n",
    "    max_df=0.85  # Ignore words appearing in more than 85% of resumes\n",
    ")\n",
    "\n",
    "# Apply TF-IDF again\n",
    "X = vectorizer.fit_transform(df[\"clean_description\"])\n",
    "\n",
    "# Print top words\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "print(\"Top 20 Words in TF-IDF Feature List:\\n\", feature_names[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "393950cd-2215-4e18-af0f-3b193a7004bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 Words in TF-IDF Feature List:\n",
      " ['accurately' 'achieving' 'actuators' 'added' 'adding' 'additional'\n",
      " 'address' 'addressed' 'adherence' 'admin' 'administrative'\n",
      " 'administrator' 'advance' 'advanced' 'advertising' 'advocate' 'agents'\n",
      " 'agile' 'ajax' 'alarm']\n"
     ]
    }
   ],
   "source": [
    "print(\"Top 20 Words in TF-IDF Feature List:\\n\", feature_names[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "cd61d560-65ba-4f70-9e72-0de86d5695fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 Most Important Words in TF-IDF:\n",
      " [('exprience', np.float64(0.08799129665033294)), ('months', np.float64(0.07863752546699765)), ('project', np.float64(0.061287807535098195)), ('java', np.float64(0.04948886311595684)), ('year', np.float64(0.048336809545806186)), ('january', np.float64(0.04662000594086879)), ('data', np.float64(0.04611264817153065)), ('developer', np.float64(0.04366994003815385)), ('maharashtra', np.float64(0.04259710222451858)), ('pune', np.float64(0.038483407138450075)), ('management', np.float64(0.03666987618456666)), ('testing', np.float64(0.03597328177028684)), ('engineering', np.float64(0.03435391695576994)), ('team', np.float64(0.0335343025111765)), ('python', np.float64(0.033432491127299464)), ('database', np.float64(0.031625882389743634)), ('test', np.float64(0.02951204577068826)), ('college', np.float64(0.0289010123193091)), ('sales', np.float64(0.028427090239116708)), ('mumbai', np.float64(0.028339779497194274))]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Get TF-IDF values for all words\n",
    "word_tfidf_values = np.asarray(X.mean(axis=0)).flatten()\n",
    "\n",
    "# Pair words with their TF-IDF importance\n",
    "word_tfidf_pairs = list(zip(vectorizer.get_feature_names_out(), word_tfidf_values))\n",
    "\n",
    "# Sort words by importance (highest first)\n",
    "word_tfidf_pairs = sorted(word_tfidf_pairs, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print top 20 most important words\n",
    "print(\"Top 20 Most Important Words in TF-IDF:\\n\", word_tfidf_pairs[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "17cae461-4984-440d-9ff8-41e49b8f7357",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidParameterError",
     "evalue": "The 'stop_words' parameter of TfidfVectorizer must be a str among {'english'}, an instance of 'list' or None. Got {'accurately', 'administrator', 'alarm', 'admin', 'agents', 'added', 'address', 'analysis', 'additional', 'analytic', 'assist', 'apply', 'advance', 'administrative', 'advanced', 'advertising', 'advocate', 'achieving'} instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidParameterError\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[85], line 15\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Re-run TF-IDF with updated stopwords\u001b[39;00m\n\u001b[0;32m      8\u001b[0m vectorizer \u001b[38;5;241m=\u001b[39m TfidfVectorizer(\n\u001b[0;32m      9\u001b[0m     stop_words\u001b[38;5;241m=\u001b[39mcustom_stop_words,\n\u001b[0;32m     10\u001b[0m     max_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3000\u001b[39m,\n\u001b[0;32m     11\u001b[0m     min_df\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m25\u001b[39m,  \u001b[38;5;66;03m# Increase to remove even more rare words\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     max_df\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.85\u001b[39m\n\u001b[0;32m     13\u001b[0m )\n\u001b[1;32m---> 15\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mvectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mclean_description\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Print new top words\u001b[39;00m\n\u001b[0;32m     18\u001b[0m feature_names \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mget_feature_names_out()\n",
      "File \u001b[1;32m~\\Documents\\GitHub\\ML-Resume-Classifier\\resume-ml-env\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:2104\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   2097\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params()\n\u001b[0;32m   2098\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf \u001b[38;5;241m=\u001b[39m TfidfTransformer(\n\u001b[0;32m   2099\u001b[0m     norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm,\n\u001b[0;32m   2100\u001b[0m     use_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_idf,\n\u001b[0;32m   2101\u001b[0m     smooth_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msmooth_idf,\n\u001b[0;32m   2102\u001b[0m     sublinear_tf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msublinear_tf,\n\u001b[0;32m   2103\u001b[0m )\n\u001b[1;32m-> 2104\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2105\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[0;32m   2106\u001b[0m \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[0;32m   2107\u001b[0m \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n",
      "File \u001b[1;32m~\\Documents\\GitHub\\ML-Resume-Classifier\\resume-ml-env\\Lib\\site-packages\\sklearn\\base.py:1382\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1377\u001b[0m partial_fit_and_fitted \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1378\u001b[0m     fit_method\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpartial_fit\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m _is_fitted(estimator)\n\u001b[0;32m   1379\u001b[0m )\n\u001b[0;32m   1381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m global_skip_validation \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m partial_fit_and_fitted:\n\u001b[1;32m-> 1382\u001b[0m     \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[0;32m   1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\Documents\\GitHub\\ML-Resume-Classifier\\resume-ml-env\\Lib\\site-packages\\sklearn\\base.py:436\u001b[0m, in \u001b[0;36mBaseEstimator._validate_params\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    428\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_validate_params\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    429\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Validate types and values of constructor parameters\u001b[39;00m\n\u001b[0;32m    430\u001b[0m \n\u001b[0;32m    431\u001b[0m \u001b[38;5;124;03m    The expected type and values must be defined in the `_parameter_constraints`\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    434\u001b[0m \u001b[38;5;124;03m    accepted constraints.\u001b[39;00m\n\u001b[0;32m    435\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 436\u001b[0m     \u001b[43mvalidate_parameter_constraints\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    437\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parameter_constraints\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    438\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    439\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcaller_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    440\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\GitHub\\ML-Resume-Classifier\\resume-ml-env\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:98\u001b[0m, in \u001b[0;36mvalidate_parameter_constraints\u001b[1;34m(parameter_constraints, params, caller_name)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     93\u001b[0m     constraints_str \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     94\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;28mstr\u001b[39m(c)\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mconstraints[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m or\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     95\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     96\u001b[0m     )\n\u001b[1;32m---> 98\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m InvalidParameterError(\n\u001b[0;32m     99\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_name\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m parameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcaller_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_val\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    101\u001b[0m )\n",
      "\u001b[1;31mInvalidParameterError\u001b[0m: The 'stop_words' parameter of TfidfVectorizer must be a str among {'english'}, an instance of 'list' or None. Got {'accurately', 'administrator', 'alarm', 'admin', 'agents', 'added', 'address', 'analysis', 'additional', 'analytic', 'assist', 'apply', 'advance', 'administrative', 'advanced', 'advertising', 'advocate', 'achieving'} instead."
     ]
    }
   ],
   "source": [
    "custom_stop_words = set([\n",
    "    \"accurately\", \"achieving\", \"added\", \"additional\", \"address\", \"admin\",\n",
    "    \"administrative\", \"administrator\", \"advance\", \"advanced\", \"advertising\",\n",
    "    \"advocate\", \"agents\", \"alarm\", \"analysis\", \"analytic\", \"apply\", \"assist\"\n",
    "])\n",
    "\n",
    "# Re-run TF-IDF with updated stopwords\n",
    "vectorizer = TfidfVectorizer(\n",
    "    stop_words=custom_stop_words,\n",
    "    max_features=3000,\n",
    "    min_df=25,  # Increase to remove even more rare words\n",
    "    max_df=0.85\n",
    ")\n",
    "\n",
    "X = vectorizer.fit_transform(df[\"clean_description\"])\n",
    "\n",
    "# Print new top words\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "print(\"Top 20 Words in TF-IDF Feature List:\\n\", feature_names[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "12f4ee5c-80a1-4ad8-9c68-597af37ea452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 Words in TF-IDF Feature List:\n",
      " ['accurately' 'achieving' 'actuators' 'added' 'adding' 'additional'\n",
      " 'address' 'addressed' 'adherence' 'admin' 'administrative'\n",
      " 'administrator' 'advance' 'advanced' 'advertising' 'advocate' 'agents'\n",
      " 'agile' 'ajax' 'alarm']\n"
     ]
    }
   ],
   "source": [
    "print(\"Top 20 Words in TF-IDF Feature List:\\n\", feature_names[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ca0161ab-03dc-4712-b45a-c35730ddbbf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 Words in TF-IDF Feature List:\n",
      " ['across' 'adding' 'agile' 'ajax' 'alert' 'algorithm' 'algorithms' 'along'\n",
      " 'already' 'also' 'american' 'amravati' 'analyst' 'analysts' 'analytical'\n",
      " 'analytics' 'analyze' 'analyzed' 'analyzing' 'android']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Convert stopword set to a list\n",
    "custom_stop_words = list([\n",
    "    \"accurately\", \"achieving\", \"added\", \"additional\", \"address\", \"admin\",\n",
    "    \"administrative\", \"administrator\", \"advance\", \"advanced\", \"advertising\",\n",
    "    \"advocate\", \"agents\", \"alarm\", \"analysis\", \"analytic\", \"apply\", \"assist\"\n",
    "])\n",
    "\n",
    "# Re-run TF-IDF with updated stopwords\n",
    "vectorizer = TfidfVectorizer(\n",
    "    stop_words=custom_stop_words,  # Now it's a list, not a set\n",
    "    max_features=3000,\n",
    "    min_df=25,  # Increase to remove even more rare words\n",
    "    max_df=0.85\n",
    ")\n",
    "\n",
    "X = vectorizer.fit_transform(df[\"clean_description\"])\n",
    "\n",
    "# Print new top words\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "print(\"Top 20 Words in TF-IDF Feature List:\\n\", feature_names[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "84bdf542-0b37-4ea8-9164-d4407cc8b718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words in first cleaned resume: ['skills', 'programming', 'languages', 'python', 'pandas', 'numpy', 'scipy', 'scikit', 'learn', 'matplotlib', 'java', 'javascript', 'jquery', 'machine', 'learning', 'regression', 'bayes', 'random', 'forest', 'decision', 'trees', 'boosting', 'techniques', 'cluster', 'analysis', 'word', 'embedding', 'sentiment', 'analysis', 'natural']\n"
     ]
    }
   ],
   "source": [
    "print(\"Words in first cleaned resume:\", df[\"clean_description\"].iloc[0].split()[:30])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "41c9ec41-33b0-4a33-befb-c4b5e88ea075",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\gopko\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words in first cleaned resume after stopword removal: ['skills', 'programming', 'languages', 'python', 'pandas', 'numpy', 'scipy', 'scikit', 'learn', 'matplotlib', 'java', 'javascript', 'jquery', 'machine', 'learning', 'regression', 'bayes', 'random', 'forest', 'decision', 'trees', 'boosting', 'techniques', 'cluster', 'analysis', 'word', 'embedding', 'sentiment', 'analysis', 'natural']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "custom_stop_words = set(stopwords.words(\"english\")).union(set([\n",
    "    \"across\", \"adding\", \"agile\", \"ajax\", \"alert\", \"already\", \"also\",\n",
    "    \"amravati\", \"analyst\", \"analysts\", \"analytics\", \"american\", \"along\"\n",
    "]))\n",
    "\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"Unknown\"  # Handle missing values\n",
    "    \n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r\"\\d+\", \"\", text)  # Remove numbers\n",
    "    text = re.sub(r\"[^\\w\\s]\", \" \", text)  # Remove punctuation\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()  # Remove extra spaces\n",
    "    words = [word for word in text.split() if word not in custom_stop_words and len(word) > 3]  # Remove stopwords & short words\n",
    "    return \" \".join(words)\n",
    "\n",
    "# Apply cleaning function again\n",
    "df[\"clean_description\"] = df[\"clean_description\"].apply(clean_text)\n",
    "\n",
    "# Verify cleaned resumes before TF-IDF\n",
    "print(\"Words in first cleaned resume after stopword removal:\", df[\"clean_description\"].iloc[0].split()[:30])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "efd98a5a-0468-4044-ad3c-d9d21c06aa9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 Words in TF-IDF Feature List:\n",
      " ['accurately' 'achieving' 'admin' 'administrative' 'administrator'\n",
      " 'advance' 'advanced' 'algorithms' 'analysis' 'analytical'\n",
      " 'analytical skills' 'analyze' 'analyzed' 'analyzing' 'android' 'angular'\n",
      " 'angular exprience' 'annual' 'apache' 'apache tomcat']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    stop_words=\"english\",\n",
    "    max_features=2000,  # Reduce vocabulary size further\n",
    "    min_df=30,  # Ignore words appearing in fewer than 30 resumes\n",
    "    max_df=0.8,  # Ignore words appearing in more than 80% of resumes\n",
    "    ngram_range=(1,2)  # Use word pairs (bigrams) to capture context\n",
    ")\n",
    "\n",
    "# Apply TF-IDF again\n",
    "X = vectorizer.fit_transform(df[\"clean_description\"])\n",
    "\n",
    "# Print new top words\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "print(\"Top 20 Words in TF-IDF Feature List:\\n\", feature_names[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "5b58a52c-5eac-4949-8090-26f365043c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 Words in TF-IDF Feature List:\n",
      " ['accurately' 'achieving' 'admin' 'administrative' 'administrator'\n",
      " 'advance' 'advanced' 'algorithms' 'analysis' 'analytical'\n",
      " 'analytical skills' 'analyze' 'analyzed' 'analyzing' 'android' 'angular'\n",
      " 'angular exprience' 'annual' 'apache' 'apache tomcat']\n"
     ]
    }
   ],
   "source": [
    "print(\"Top 20 Words in TF-IDF Feature List:\\n\", feature_names[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "bd942aa3-99e0-4eb3-938e-0f5b8cf2d876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words in first cleaned resume before TF-IDF: ['skills', 'programming', 'languages', 'python', 'pandas', 'numpy', 'scipy', 'scikit', 'learn', 'matplotlib', 'java', 'javascript', 'jquery', 'machine', 'learning', 'regression', 'bayes', 'random', 'forest', 'decision', 'trees', 'boosting', 'techniques', 'cluster', 'analysis', 'word', 'embedding', 'sentiment', 'analysis', 'natural']\n"
     ]
    }
   ],
   "source": [
    "print(\"Words in first cleaned resume before TF-IDF:\", df[\"clean_description\"].iloc[0].split()[:30])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "db0f27c8-0356-4e60-b6e7-4a84cf1aafa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\gopko\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words in first cleaned resume after stopword removal: ['programming', 'languages', 'python', 'pandas', 'numpy', 'scipy', 'scikit', 'learn', 'matplotlib', 'java', 'javascript', 'jquery', 'machine', 'learning', 'regression', 'bayes', 'random', 'forest', 'decision', 'trees', 'boosting', 'techniques', 'cluster', 'analysis', 'word', 'embedding', 'sentiment', 'analysis', 'natural', 'language']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "custom_stop_words = set(stopwords.words(\"english\")).union(set([\n",
    "    \"accurately\", \"achieving\", \"admin\", \"administrative\", \"administrator\",\n",
    "    \"advance\", \"advanced\", \"annual\", \"apply\", \"assist\", \"experience\",\n",
    "    \"knowledge\", \"skills\", \"ability\", \"responsible\", \"manage\", \"working\",\n",
    "    \"maintain\", \"ensure\", \"providing\", \"understanding\"\n",
    "]))\n",
    "\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"Unknown\"  # Handle missing values\n",
    "    \n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r\"\\d+\", \"\", text)  # Remove numbers\n",
    "    text = re.sub(r\"[^\\w\\s]\", \" \", text)  # Remove punctuation\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()  # Remove extra spaces\n",
    "    words = [word for word in text.split() if word not in custom_stop_words and len(word) > 3]  # Remove stopwords & short words\n",
    "    return \" \".join(words)\n",
    "\n",
    "# Apply cleaning function again\n",
    "df[\"clean_description\"] = df[\"clean_description\"].apply(clean_text)\n",
    "\n",
    "# Verify stopwords were removed before TF-IDF\n",
    "print(\"Words in first cleaned resume after stopword removal:\", df[\"clean_description\"].iloc[0].split()[:30])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c1dc0a55-4d9b-489b-ba2b-dc940c156c2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 Words in TF-IDF Feature List:\n",
      " ['algorithms' 'analysis' 'analytical' 'analyze' 'analyzing' 'android'\n",
      " 'angular' 'apache' 'application' 'applications' 'applying' 'appropriate'\n",
      " 'april' 'architect' 'architecture' 'area' 'areas' 'arts' 'assistant'\n",
      " 'assisting']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    stop_words=\"english\",\n",
    "    max_features=1500,  # Reduce vocabulary size further\n",
    "    min_df=40,  # Ignore words appearing in fewer than 40 resumes\n",
    "    max_df=0.75,  # Ignore words appearing in more than 75% of resumes\n",
    "    ngram_range=(1,2)  # Use word pairs (bigrams) to capture better context\n",
    ")\n",
    "\n",
    "# Apply TF-IDF again\n",
    "X = vectorizer.fit_transform(df[\"clean_description\"])\n",
    "\n",
    "# Print new top words\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "print(\"Top 20 Words in TF-IDF Feature List:\\n\", feature_names[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "e569eba1-dbdd-417f-91e3-2175e258d704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 Words in TF-IDF Feature List:\n",
      " ['algorithms' 'analysis' 'analytical' 'analyze' 'analyzing' 'android'\n",
      " 'angular' 'apache' 'application' 'applications' 'applying' 'appropriate'\n",
      " 'april' 'architect' 'architecture' 'area' 'areas' 'arts' 'assistant'\n",
      " 'assisting']\n"
     ]
    }
   ],
   "source": [
    "print(\"Top 20 Words in TF-IDF Feature List:\\n\", feature_names[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b74324a1-5539-4c2f-8f3d-be8d43cf6ada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Resume (Cleaned Text):\n",
      " programming languages python pandas numpy scipy scikit learn matplotlib java javascript jquery machine learning regression bayes random forest decision trees boosting techniques cluster analysis word embedding sentiment analysis natural language processing dimensionality reduction topic modelling neural nets database visualizations mysql sqlserver cassandra hbase elasticsearch plotly kibana matplotlib ggplot tableau others regular expression html angular logstash kafka python flask docker computer vision open deep learning education details data science assurance associate data science assurance associate ernst young skill details javascript exprience months jquery exprience months python exprience monthscompany details company ernst young description fraud investigations dispute services assurance technology assisted review technology assisted review assists accelerating review process generate reports core member team helped developing automated review platform tool scratch assisting discovery domain tool implements predictive coding topic modelling automating reviews resulting reduced labor costs time spent lawyers review understand flow solution research development classification models predictive analysis mining information present text data worked analyzing outputs precision monitoring entire tool assists predictive coding topic modelling evidence following standards developed classifier models order identify flags fraud related issues tools technologies python scikit learn tfidf wordvec docvec cosine similarity bayes topic modelling vader text blob sentiment analysis matplot tableau dashboard reporting multiple data science analytic projects clients text motor vehicle customer review data received customer feedback survey data past year performed sentiment positive negative neutral time series analysis customer comments categories created heat terms survey category based frequency words extracted positive negative words survey categories plotted word cloud created customized tableau dashboards effective reporting visualizations chatbot developed user friendly chatbot products handle simple questions hours operation reservation options chat serves entire product related questions giving overview tool platform give recommendation responses user question build chain relevant answer intelligence build pipeline questions user requirement asks relevant recommended questions tools technologies python natural language processing nltk spacy topic modelling sentiment analysis word embedding scikit learn javascript jquery sqlserver information governance organizations make informed decisions information store integrated information governance portfolio synthesizes intelligence unstructured data sources facilitates organizations best positioned counter information risk scan data multiple sources formats parse different file formats extract meta data information push results indexing elastic search created customized interactive dashboards using kibana preforming analysis data give information data helps identify content either redundant outdated trivial preforming full text search analysis elastic search predefined methods personally identifiable information social security numbers addresses names frequently targeted cyber attacks tools technologies python flask elastic search kibana fraud analytic platform fraud investigative platform review flag cases fraud investigative platform inbuilt case manager suite various systems used clients interrogate systems identifying anomalies indicators fraud running tools technologies html javascript sqlserver jquery bootstrap node\n",
      "\n",
      "Another Sample Resume:\n",
      " education details rgpv data scientist data scientist matelabs skill details python exprience less year months statsmodels exprience months exprience less year months machine learning exprience less year months sklearn exprience less year months scipy exprience less year months keras exprience less year monthscompany details company matelabs description platform business professionals dummies enthusiasts koramangala block tasks behind sukh sagar bengaluru india developed deployed auto preprocessing steps machine learning mainly missing value treatment outlier detection encoding scaling feature selection dimensionality reduction deployed automated classification regression model linkedin aditya rathore reasearch deployed time series forecasting model arima sarimax holt winter prophet worked meta feature extracting problem github rathorology implemented state research paper outlier detection mixed attributes company matelabs description\n",
      "\n",
      "Another Sample Resume:\n",
      " areas interest deep learning control system design programming python electric machinery development technical hindustan aeronautics limited bangalore weeks guidance satish senior engineer hangar mirage fighter aircraft technical programming matlab python java labview python webframework django flask ltspice intermediate languages mipower intermediate github gitbash jupyter notebook xampp mysql basics python software packages interpreters anaconda python python pycharm java eclipse operating systems windows ubuntu debian kali linux education details january tech electrical electronics engineering manipal institute technology january deeksha center january little flower public school august manipal higher data science data science electrical enthusiast skill details data analysis exprience less year months excel exprience less year months machine learning exprience less year months mathematics exprience less year months python exprience less year months matlab exprience less year months electrical engineering exprience less year months exprience less year monthscompany details company themathcompany description currently casino based operator name disclosed macau need segment customers visit property based value patrons bring company basically prove segmentation done much better current system proper numbers back henceforth implement target marketing strategy attract customers value business\n"
     ]
    }
   ],
   "source": [
    "print(\"Sample Resume (Cleaned Text):\\n\", df[\"clean_description\"].iloc[0])\n",
    "print(\"\\nAnother Sample Resume:\\n\", df[\"clean_description\"].iloc[1])\n",
    "print(\"\\nAnother Sample Resume:\\n\", df[\"clean_description\"].iloc[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "3d92f037-bfba-4448-818c-f8fc0b1621e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\gopko\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words in first cleaned resume: ['programming', 'languages', 'python', 'pandas', 'numpy', 'scipy', 'scikit', 'learn', 'matplotlib', 'java', 'javascript', 'jquery', 'machine', 'learning', 'regression', 'bayes', 'random', 'forest', 'decision', 'trees', 'boosting', 'techniques', 'cluster', 'analysis', 'word', 'embedding', 'sentiment', 'analysis', 'natural', 'language']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "custom_stop_words = set(stopwords.words(\"english\")).union(set([\n",
    "    \"application\", \"appropriate\", \"april\", \"assistant\", \"areas\", \"arts\",\n",
    "    \"analyze\", \"analyzing\", \"architect\", \"architecture\", \"apply\",\n",
    "    \"responsibilities\", \"responsible\", \"working\", \"experience\", \"knowledge\"\n",
    "]))\n",
    "\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"Unknown\"  # Handle missing values\n",
    "    \n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r\"\\d+\", \"\", text)  # Remove numbers\n",
    "    text = re.sub(r\"[^\\w\\s]\", \" \", text)  # Remove punctuation\n",
    "    text = re.sub(r\"\\b[a-zA-Z]{1,2}\\b\", \"\", text)  # Remove very short words (1-2 letters)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()  # Remove extra spaces\n",
    "    words = [word for word in text.split() if word not in custom_stop_words and len(word) > 3]  # Remove stopwords & short words\n",
    "    return \" \".join(words)\n",
    "\n",
    "# Apply the improved cleaning function\n",
    "df[\"clean_description\"] = df[\"clean_description\"].apply(clean_text)\n",
    "\n",
    "# Check if words are properly removed before TF-IDF\n",
    "print(\"Words in first cleaned resume:\", df[\"clean_description\"].iloc[0].split()[:30])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "adb5226b-57d9-4d64-a7b9-f25d6c0571ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 Words in TF-IDF Feature List:\n",
      " ['analysis' 'analytical' 'angular' 'apache' 'applications' 'applying'\n",
      " 'area' 'assisting' 'attending' 'attitude' 'audit' 'audits' 'august'\n",
      " 'auto' 'autocad' 'automated' 'automation' 'available' 'bachelor'\n",
      " 'bachelor engineering']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    stop_words=\"english\",\n",
    "    max_features=1000,  # Reduce vocabulary size further\n",
    "    min_df=50,  # Ignore words appearing in fewer than 50 resumes\n",
    "    max_df=0.7,  # Ignore words appearing in more than 70% of resumes\n",
    "    ngram_range=(1,2)  # Use bigrams to improve word context\n",
    ")\n",
    "\n",
    "# Apply TF-IDF again\n",
    "X = vectorizer.fit_transform(df[\"clean_description\"])\n",
    "\n",
    "# Print new top words\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "print(\"Top 20 Words in TF-IDF Feature List:\\n\", feature_names[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "6dc00c09-45ce-4f19-9fdd-b8f173af285e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 Words in TF-IDF Feature List:\n",
      " ['analysis' 'analytical' 'angular' 'apache' 'applications' 'applying'\n",
      " 'area' 'assisting' 'attending' 'attitude' 'audit' 'audits' 'august'\n",
      " 'auto' 'autocad' 'automated' 'automation' 'available' 'bachelor'\n",
      " 'bachelor engineering']\n"
     ]
    }
   ],
   "source": [
    "print(\"Top 20 Words in TF-IDF Feature List:\\n\", feature_names[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e182b33-79bb-423f-ad15-ac31956f1b11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "02be4c08-37e0-4897-9061-da973b57b2ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0             Job Title  \\\n",
      "0           0     Flutter Developer   \n",
      "1           1      Django Developer   \n",
      "2           2      Machine Learning   \n",
      "3           3         iOS Developer   \n",
      "4           4  Full Stack Developer   \n",
      "\n",
      "                                     Job Description  \n",
      "0  We are looking for hire experts flutter develo...  \n",
      "1  PYTHON/DJANGO (Developer/Lead) - Job Code(PDJ ...  \n",
      "2  Data Scientist (Contractor)\\n\\nBangalore, IN\\n...  \n",
      "3  JOB DESCRIPTION:\\n\\nStrong framework outside o...  \n",
      "4  job responsibility full stack engineer – react...  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2277 entries, 0 to 2276\n",
      "Data columns (total 3 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   Unnamed: 0       2277 non-null   int64 \n",
      " 1   Job Title        2277 non-null   object\n",
      " 2   Job Description  2277 non-null   object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 53.5+ KB\n",
      "None\n",
      "Columns in dataset: Index(['Unnamed: 0', 'Job Title', 'Job Description'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the new dataset\n",
    "file_path = r\"C:\\Users\\gopko\\Documents\\GitHub\\ML-Resume-Classifier\\data\\resumeV3.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Show first few rows\n",
    "print(df.head())\n",
    "\n",
    "# Show dataset structure\n",
    "print(df.info())\n",
    "\n",
    "# Show column names\n",
    "print(\"Columns in dataset:\", df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "831330ae-a06a-42e0-b298-de5913bf94bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'Job Title', 'Job Description'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "e2f0dfa3-0e48-4877-a9a7-94293f72703c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Columns: Index(['Unnamed: 0', 'Job Title', 'Job Description'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "df.rename(columns={\"actual_resume_column\": \"clean_description\", \"job_category_column\": \"category\"}, inplace=True)\n",
    "print(\"Updated Columns:\", df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "630654fb-fb09-4f23-99c5-4e87d3403f6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\gopko\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'clean_description'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\Documents\\GitHub\\ML-Resume-Classifier\\resume-ml-env\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'clean_description'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[103], line 24\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(words)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Apply the improved cleaning function\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclean_description\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mclean_description\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mapply(clean_text)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Show cleaned text\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWords in first cleaned resume:\u001b[39m\u001b[38;5;124m\"\u001b[39m, df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclean_description\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39msplit()[:\u001b[38;5;241m30\u001b[39m])\n",
      "File \u001b[1;32m~\\Documents\\GitHub\\ML-Resume-Classifier\\resume-ml-env\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\Documents\\GitHub\\ML-Resume-Classifier\\resume-ml-env\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'clean_description'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "custom_stop_words = set(stopwords.words(\"english\")).union(set([\n",
    "    \"experience\", \"skills\", \"knowledge\", \"worked\", \"expertise\", \"responsible\",\n",
    "    \"ability\", \"working\", \"proficient\", \"development\", \"team\", \"projects\"\n",
    "]))\n",
    "\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"Unknown\"  # Handle missing values\n",
    "    \n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r\"\\d+\", \"\", text)  # Remove numbers\n",
    "    text = re.sub(r\"[^\\w\\s]\", \" \", text)  # Remove punctuation\n",
    "    text = re.sub(r\"\\b[a-zA-Z]{1,2}\\b\", \"\", text)  # Remove very short words (1-2 letters)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()  # Remove extra spaces\n",
    "    words = [word for word in text.split() if word not in custom_stop_words and len(word) > 3]  # Remove stopwords & short words\n",
    "    return \" \".join(words)\n",
    "\n",
    "# Apply the improved cleaning function\n",
    "df[\"clean_description\"] = df[\"clean_description\"].apply(clean_text)\n",
    "\n",
    "# Show cleaned text\n",
    "print(\"Words in first cleaned resume:\", df[\"clean_description\"].iloc[0].split()[:30])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "2dbe5bf1-6a25-4cb5-a8ff-7e8c1d6cd14d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Columns: Index(['Unnamed: 0', 'Job Title', 'Job Description'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataset Columns:\", df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "465cfa12-6281-4a09-86ef-46d3026b35da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Columns: Index(['Unnamed: 0', 'Job Title', 'Job Description'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "df.rename(columns={\"actual_resume_column\": \"clean_description\"}, inplace=True)\n",
    "print(\"Updated Columns:\", df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "19b3cb01-d1d4-4c38-bce7-27d1d7a81197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0             Job Title  \\\n",
      "0           0     Flutter Developer   \n",
      "1           1      Django Developer   \n",
      "2           2      Machine Learning   \n",
      "3           3         iOS Developer   \n",
      "4           4  Full Stack Developer   \n",
      "\n",
      "                                     Job Description  \n",
      "0  We are looking for hire experts flutter develo...  \n",
      "1  PYTHON/DJANGO (Developer/Lead) - Job Code(PDJ ...  \n",
      "2  Data Scientist (Contractor)\\n\\nBangalore, IN\\n...  \n",
      "3  JOB DESCRIPTION:\\n\\nStrong framework outside o...  \n",
      "4  job responsibility full stack engineer – react...  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2277 entries, 0 to 2276\n",
      "Data columns (total 3 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   Unnamed: 0       2277 non-null   int64 \n",
      " 1   Job Title        2277 non-null   object\n",
      " 2   Job Description  2277 non-null   object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 53.5+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df.head())  # Show first few rows\n",
    "print(df.info())  # Show data types and missing values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "05566a82-fce4-4c81-8c66-c04e158ad7b2",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'clean_description'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\Documents\\GitHub\\ML-Resume-Classifier\\resume-ml-env\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'clean_description'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[107], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclean_description\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mclean_description\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\u001b[38;5;241m.\u001b[39mapply(clean_text)\n",
      "File \u001b[1;32m~\\Documents\\GitHub\\ML-Resume-Classifier\\resume-ml-env\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\Documents\\GitHub\\ML-Resume-Classifier\\resume-ml-env\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'clean_description'"
     ]
    }
   ],
   "source": [
    "df[\"clean_description\"] = df[\"clean_description\"].astype(str).apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "f1bad94a-fb3a-41b3-bd9a-bbae9ad03746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Columns: ['Unnamed: 0', 'Job Title', 'Job Description']\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataset Columns:\", df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "4b962493-d412-4f1c-949b-b8a7a6bd033a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Columns: ['Unnamed: 0', 'Job Title', 'Job Description']\n"
     ]
    }
   ],
   "source": [
    "df.rename(columns={\"actual_resume_column\": \"clean_description\"}, inplace=True)\n",
    "print(\"Updated Columns:\", df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "a0eb94da-b8e3-46d6-9061-6861cb6605bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0             Job Title  \\\n",
      "0           0     Flutter Developer   \n",
      "1           1      Django Developer   \n",
      "2           2      Machine Learning   \n",
      "3           3         iOS Developer   \n",
      "4           4  Full Stack Developer   \n",
      "\n",
      "                                     Job Description  \n",
      "0  We are looking for hire experts flutter develo...  \n",
      "1  PYTHON/DJANGO (Developer/Lead) - Job Code(PDJ ...  \n",
      "2  Data Scientist (Contractor)\\n\\nBangalore, IN\\n...  \n",
      "3  JOB DESCRIPTION:\\n\\nStrong framework outside o...  \n",
      "4  job responsibility full stack engineer – react...  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2277 entries, 0 to 2276\n",
      "Data columns (total 3 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   Unnamed: 0       2277 non-null   int64 \n",
      " 1   Job Title        2277 non-null   object\n",
      " 2   Job Description  2277 non-null   object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 53.5+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df.head())  # Show first few rows\n",
    "print(df.info())  # Show data types and missing values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "d9d31608-4f16-484e-9f83-4ac399b04c02",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'clean_description'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\Documents\\GitHub\\ML-Resume-Classifier\\resume-ml-env\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'clean_description'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[111], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclean_description\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mclean_description\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\u001b[38;5;241m.\u001b[39mapply(clean_text)\n",
      "File \u001b[1;32m~\\Documents\\GitHub\\ML-Resume-Classifier\\resume-ml-env\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\Documents\\GitHub\\ML-Resume-Classifier\\resume-ml-env\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'clean_description'"
     ]
    }
   ],
   "source": [
    "df[\"clean_description\"] = df[\"clean_description\"].astype(str).apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "0f5af101-658c-4f96-bece-16143d2a3406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Columns: ['Unnamed: 0', 'Job Title', 'Job Description']\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataset Columns:\", df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "08eb1296-0e84-4256-8b9a-1688f71aa79d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Columns: ['Unnamed: 0', 'Job Title', 'Job Description']\n"
     ]
    }
   ],
   "source": [
    "df.rename(columns={\"Resume\": \"clean_description\"}, inplace=True)\n",
    "print(\"Updated Columns:\", df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "8ae1059c-4f3b-4fbf-9405-6feebe9c32b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0             Job Title  \\\n",
      "0           0     Flutter Developer   \n",
      "1           1      Django Developer   \n",
      "2           2      Machine Learning   \n",
      "3           3         iOS Developer   \n",
      "4           4  Full Stack Developer   \n",
      "\n",
      "                                     Job Description  \n",
      "0  We are looking for hire experts flutter develo...  \n",
      "1  PYTHON/DJANGO (Developer/Lead) - Job Code(PDJ ...  \n",
      "2  Data Scientist (Contractor)\\n\\nBangalore, IN\\n...  \n",
      "3  JOB DESCRIPTION:\\n\\nStrong framework outside o...  \n",
      "4  job responsibility full stack engineer – react...  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2277 entries, 0 to 2276\n",
      "Data columns (total 3 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   Unnamed: 0       2277 non-null   int64 \n",
      " 1   Job Title        2277 non-null   object\n",
      " 2   Job Description  2277 non-null   object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 53.5+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df.head())  # Show first few rows\n",
    "print(df.info())  # Show data types and missing values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "cd798012-040c-4473-8261-55986b987071",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'clean_description'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\Documents\\GitHub\\ML-Resume-Classifier\\resume-ml-env\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'clean_description'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[115], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclean_description\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mclean_description\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\u001b[38;5;241m.\u001b[39mapply(clean_text)\n",
      "File \u001b[1;32m~\\Documents\\GitHub\\ML-Resume-Classifier\\resume-ml-env\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\Documents\\GitHub\\ML-Resume-Classifier\\resume-ml-env\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'clean_description'"
     ]
    }
   ],
   "source": [
    "df[\"clean_description\"] = df[\"clean_description\"].astype(str).apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "6d33aecb-5e0b-4b28-bc11-573ca17eb865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Columns: ['category', 'clean_description']\n"
     ]
    }
   ],
   "source": [
    "df.rename(columns={\"Job Description\": \"clean_description\", \"Job Title\": \"category\"}, inplace=True)\n",
    "\n",
    "# Drop unnecessary index column\n",
    "df.drop(columns=[\"Unnamed: 0\"], inplace=True, errors=\"ignore\")\n",
    "\n",
    "print(\"Updated Columns:\", df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "8050d659-8fc7-45f8-9891-4f9f744923d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               category                                  clean_description\n",
      "0     Flutter Developer  We are looking for hire experts flutter develo...\n",
      "1      Django Developer  PYTHON/DJANGO (Developer/Lead) - Job Code(PDJ ...\n",
      "2      Machine Learning  Data Scientist (Contractor)\\n\\nBangalore, IN\\n...\n",
      "3         iOS Developer  JOB DESCRIPTION:\\n\\nStrong framework outside o...\n",
      "4  Full Stack Developer  job responsibility full stack engineer – react...\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2277 entries, 0 to 2276\n",
      "Data columns (total 2 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   category           2277 non-null   object\n",
      " 1   clean_description  2277 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 35.7+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df.head())  # Show first few rows\n",
    "print(df.info())  # Show data types and missing values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "5a1495eb-8349-4bb7-9280-e7b5fca4ab96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\gopko\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words in first cleaned job description: ['hire', 'experts', 'flutter', 'developer', 'eligible', 'post', 'apply', 'resume', 'types', 'full', 'time', 'part', 'time', 'salary', 'month', 'benefits', 'flexible', 'schedule', 'food', 'allowance', 'schedule', 'shift', 'supplemental', 'joining', 'bonus', 'overtime', 'total', 'work', 'year', 'preferred']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "custom_stop_words = set(stopwords.words(\"english\")).union(set([\n",
    "    \"experience\", \"skills\", \"knowledge\", \"worked\", \"expertise\", \"responsible\",\n",
    "    \"ability\", \"working\", \"proficient\", \"development\", \"team\", \"projects\",\n",
    "    \"hiring\", \"looking\", \"job\", \"description\"\n",
    "]))\n",
    "\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"Unknown\"  # Handle missing values\n",
    "    \n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r\"\\d+\", \"\", text)  # Remove numbers\n",
    "    text = re.sub(r\"[^\\w\\s]\", \" \", text)  # Remove punctuation\n",
    "    text = re.sub(r\"\\b[a-zA-Z]{1,2}\\b\", \"\", text)  # Remove very short words (1-2 letters)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()  # Remove extra spaces\n",
    "    words = [word for word in text.split() if word not in custom_stop_words and len(word) > 3]  # Remove stopwords & short words\n",
    "    return \" \".join(words)\n",
    "\n",
    "# Apply the improved cleaning function\n",
    "df[\"clean_description\"] = df[\"clean_description\"].apply(clean_text)\n",
    "\n",
    "# Show cleaned text\n",
    "print(\"Words in first cleaned job description:\", df[\"clean_description\"].iloc[0].split()[:30])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "2033243f-25c6-47da-80ee-07374c699164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned dataset saved successfully!\n"
     ]
    }
   ],
   "source": [
    "df.to_csv(r\"C:\\Users\\gopko\\Documents\\GitHub\\ML-Resume-Classifier\\data\\resumeV3_cleaned.csv\", index=False)\n",
    "print(\"Cleaned dataset saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "b5698a7e-b56b-4434-8dd4-f7dc754b6cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF matrix shape: (2277, 1448)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    stop_words=\"english\",\n",
    "    max_features=2000,  # Reduce vocabulary size\n",
    "    min_df=50,  # Ignore words appearing in fewer than 50 resumes\n",
    "    max_df=0.7,  # Ignore words appearing in more than 70% of resumes\n",
    "    ngram_range=(1,2)  # Use bigrams for better context\n",
    ")\n",
    "\n",
    "# Convert cleaned job descriptions into numerical vectors\n",
    "X = vectorizer.fit_transform(df[\"clean_description\"])\n",
    "\n",
    "# Print the shape of transformed data\n",
    "print(\"TF-IDF matrix shape:\", X.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "d4592134-8ab8-4323-bf67-2a39f6045a2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 Words in TF-IDF Feature List:\n",
      " ['abilities' 'able' 'able work' 'access' 'accessibility'\n",
      " 'accessibility security' 'accommodation' 'according' 'account' 'accurate'\n",
      " 'achieve' 'action' 'active' 'actively' 'activities' 'activity' 'adapt'\n",
      " 'added' 'added advantage' 'addition']\n"
     ]
    }
   ],
   "source": [
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Print the first 20 important words\n",
    "print(\"Top 20 Words in TF-IDF Feature List:\\n\", feature_names[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "056bdfc3-2f89-4b75-9218-0f1ed98927a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Job Description (Cleaned Text):\n",
      " hire experts flutter developer eligible post apply resume types full time part time salary month benefits flexible schedule food allowance schedule shift supplemental joining bonus overtime total work year preferred housing rent subsidy industry software work remotely temporarily covid\n",
      "\n",
      "Another Sample:\n",
      " python django developer lead code strong python rest frameworks django flask evaluating improving efficiency programs linux environment effectively handle multiple tasks high level accuracy attention detail good verbal written communication json preferred good automated unit testing using pyunit\n",
      "\n",
      "Another Sample:\n",
      " data scientist contractor bangalore responsibilities capable data scientist join analytics reporting locally india bangalore person responsibilities include research design machine learning deep learning algorithms tackle variety fraud oriented challenges data scientist work closely software engineers program managers deliver products including data collection scale analysis exploring different algorithmic approaches model assessment validation production qualifications least years hands complex machine learning models using modern frameworks tools ideally python based solid understanding statistics applied mathematics creative thinker proven tackle open problems apply trivial solutions software using python java similar language graduate computer science mathematics equivalent preferably machine learning write clean concise code quick learner independent methodical detail oriented player positive attitude collaborative good communication dedicated makes things happen flexible capable making decisions ambiguous changing environment advantages prior software developer data engineer advantage data advantage spark advantage deep learning frameworks pytorch tensorflow keras advantage telecommunication domain fraud prevention advantage\n"
     ]
    }
   ],
   "source": [
    "print(\"Sample Job Description (Cleaned Text):\\n\", df[\"clean_description\"].iloc[0])\n",
    "print(\"\\nAnother Sample:\\n\", df[\"clean_description\"].iloc[1])\n",
    "print(\"\\nAnother Sample:\\n\", df[\"clean_description\"].iloc[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "1f2659fe-510b-4e66-9b1e-a91792236479",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\gopko\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words in first cleaned job description: ['hire', 'experts', 'flutter', 'developer', 'eligible', 'post', 'resume', 'types', 'full', 'time', 'part', 'time', 'salary', 'month', 'benefits', 'flexible', 'schedule', 'food', 'allowance', 'schedule', 'shift', 'supplemental', 'joining', 'bonus', 'overtime', 'total', 'work', 'year', 'preferred', 'housing']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "custom_stop_words = set(stopwords.words(\"english\")).union(set([\n",
    "    \"abilities\", \"able\", \"access\", \"accessibility\", \"account\", \"accurate\",\n",
    "    \"achieve\", \"action\", \"active\", \"activities\", \"activity\", \"adapt\",\n",
    "    \"added\", \"advantage\", \"addition\", \"applicant\", \"apply\", \"appropriate\",\n",
    "    \"assigned\", \"assistant\", \"collaborate\", \"communication\", \"company\",\n",
    "    \"competitive\", \"compliance\", \"concepts\", \"consider\", \"consulting\",\n",
    "    \"contribute\", \"coordinate\", \"corporate\", \"criteria\", \"customer\",\n",
    "    \"decision\", \"department\", \"demonstrated\", \"desirable\", \"detail\",\n",
    "    \"developed\", \"development\", \"duties\", \"education\", \"effective\",\n",
    "    \"efficiency\", \"employees\", \"ensure\", \"enhance\", \"environment\",\n",
    "    \"established\", \"evaluate\", \"execute\", \"expected\", \"experience\",\n",
    "    \"familiar\", \"focus\", \"function\", \"future\", \"gained\", \"general\",\n",
    "    \"goal\", \"group\", \"handling\", \"helping\", \"identified\", \"impact\",\n",
    "    \"important\", \"improve\", \"including\", \"independently\", \"individual\",\n",
    "    \"information\", \"initiatives\", \"interacting\", \"job\", \"join\", \"knowledge\",\n",
    "    \"leadership\", \"maintain\", \"making\", \"management\", \"manager\",\n",
    "    \"methodologies\", \"methods\", \"multiple\", \"needs\", \"offer\", \"office\",\n",
    "    \"opportunity\", \"organization\", \"participate\", \"perform\", \"performance\",\n",
    "    \"planning\", \"practices\", \"prepare\", \"process\", \"proficiency\",\n",
    "    \"program\", \"project\", \"providing\", \"related\", \"reporting\",\n",
    "    \"requirements\", \"resources\", \"responsibility\", \"role\", \"self\",\n",
    "    \"services\", \"skills\", \"solutions\", \"specialized\", \"strategic\",\n",
    "    \"strong\", \"successful\", \"support\", \"system\", \"team\", \"technical\",\n",
    "    \"technology\", \"track\", \"training\", \"understanding\", \"various\",\n",
    "    \"working\", \"workplace\", \"writing\"\n",
    "]))\n",
    "\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"Unknown\"  # Handle missing values\n",
    "    \n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r\"\\d+\", \"\", text)  # Remove numbers\n",
    "    text = re.sub(r\"[^\\w\\s]\", \" \", text)  # Remove punctuation\n",
    "    text = re.sub(r\"\\b[a-zA-Z]{1,2}\\b\", \"\", text)  # Remove very short words (1-2 letters)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()  # Remove extra spaces\n",
    "    words = [word for word in text.split() if word not in custom_stop_words and len(word) > 3]  # Remove stopwords & short words\n",
    "    return \" \".join(words)\n",
    "\n",
    "# Apply the improved cleaning function\n",
    "df[\"clean_description\"] = df[\"clean_description\"].apply(clean_text)\n",
    "\n",
    "# Check cleaned text again\n",
    "print(\"Words in first cleaned job description:\", df[\"clean_description\"].iloc[0].split()[:30])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "deccb9cb-df23-4882-bed3-be3b61b448bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 Words in TF-IDF Feature List:\n",
      " ['accommodation' 'actively' 'additional' 'address' 'administration'\n",
      " 'administrator' 'advanced' 'agile' 'agile scrum' 'ajax' 'algorithms'\n",
      " 'amazing' 'amazon' 'analysis' 'analytical' 'analytical problem'\n",
      " 'analytics' 'analyze' 'analyzing' 'android']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    stop_words=\"english\",\n",
    "    max_features=1000,  # Reduce vocabulary size further\n",
    "    min_df=60,  # Ignore words appearing in fewer than 60 job descriptions\n",
    "    max_df=0.65,  # Ignore words appearing in more than 65% of job descriptions\n",
    "    ngram_range=(1,2)  # Use bigrams to improve word context\n",
    ")\n",
    "\n",
    "# Apply TF-IDF again\n",
    "X = vectorizer.fit_transform(df[\"clean_description\"])\n",
    "\n",
    "# Print new top words\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "print(\"Top 20 Words in TF-IDF Feature List:\\n\", feature_names[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "efa596f1-a875-411e-bd4e-a21188e61116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 Words in TF-IDF Feature List:\n",
      " ['accommodation' 'actively' 'additional' 'address' 'administration'\n",
      " 'administrator' 'advanced' 'agile' 'agile scrum' 'ajax' 'algorithms'\n",
      " 'amazing' 'amazon' 'analysis' 'analytical' 'analytical problem'\n",
      " 'analytics' 'analyze' 'analyzing' 'android']\n"
     ]
    }
   ],
   "source": [
    "print(\"Top 20 Words in TF-IDF Feature List:\\n\", feature_names[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "9766e2a2-9f2f-42d7-85b7-8fc9f22f56f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Job Description (Cleaned Text):\n",
      " hire experts flutter developer eligible post resume types full time part time salary month benefits flexible schedule food allowance schedule shift supplemental joining bonus overtime total work year preferred housing rent subsidy industry software work remotely temporarily covid\n",
      "\n",
      "Another Sample:\n",
      " python django developer lead code python rest frameworks django flask evaluating improving programs linux effectively handle tasks high level accuracy attention good verbal written json preferred good automated unit testing using pyunit\n",
      "\n",
      "Another Sample:\n",
      " data scientist contractor bangalore responsibilities capable data scientist analytics locally india bangalore person responsibilities include research design machine learning deep learning algorithms tackle variety fraud oriented challenges data scientist work closely software engineers managers deliver products data collection scale analysis exploring different algorithmic approaches model assessment validation production qualifications least years hands complex machine learning models using modern frameworks tools ideally python based solid statistics applied mathematics creative thinker proven tackle open problems trivial software using python java similar language graduate computer science mathematics equivalent preferably machine learning write clean concise code quick learner independent methodical oriented player positive attitude collaborative good dedicated makes things happen flexible capable decisions ambiguous changing advantages prior software developer data engineer data spark deep learning frameworks pytorch tensorflow keras telecommunication domain fraud prevention\n"
     ]
    }
   ],
   "source": [
    "print(\"Sample Job Description (Cleaned Text):\\n\", df[\"clean_description\"].iloc[0])\n",
    "print(\"\\nAnother Sample:\\n\", df[\"clean_description\"].iloc[1])\n",
    "print(\"\\nAnother Sample:\\n\", df[\"clean_description\"].iloc[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "23bc3907-e0ce-4728-9211-627134a4bce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\gopko\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words in first cleaned job description: ['hire', 'experts', 'flutter', 'developer', 'eligible', 'post', 'resume', 'types', 'full', 'time', 'part', 'time', 'salary', 'month', 'benefits', 'flexible', 'schedule', 'food', 'allowance', 'schedule', 'shift', 'supplemental', 'joining', 'bonus', 'overtime', 'total', 'work', 'year', 'preferred', 'housing']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "custom_stop_words = set(stopwords.words(\"english\")).union(set([\n",
    "    \"accommodation\", \"actively\", \"additional\", \"address\", \"administration\",\n",
    "    \"administrator\", \"advanced\", \"amazing\", \"amazon\", \"analysis\", \"analytics\",\n",
    "    \"application\", \"apply\", \"assistant\", \"collaborate\", \"communication\",\n",
    "    \"company\", \"compliance\", \"customer\", \"department\", \"development\",\n",
    "    \"duties\", \"education\", \"effective\", \"employees\", \"ensure\", \"experience\",\n",
    "    \"general\", \"goal\", \"handling\", \"impact\", \"important\", \"including\",\n",
    "    \"independently\", \"individual\", \"information\", \"initiatives\", \"job\",\n",
    "    \"join\", \"knowledge\", \"leadership\", \"maintain\", \"making\", \"management\",\n",
    "    \"manager\", \"needs\", \"office\", \"opportunity\", \"organization\",\n",
    "    \"participate\", \"perform\", \"performance\", \"planning\", \"prepare\",\n",
    "    \"process\", \"project\", \"providing\", \"reporting\", \"requirements\",\n",
    "    \"responsibility\", \"role\", \"services\", \"skills\", \"solutions\",\n",
    "    \"successful\", \"support\", \"team\", \"technology\", \"training\", \"various\",\n",
    "    \"working\", \"workplace\", \"writing\"\n",
    "]))\n",
    "\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"Unknown\"  # Handle missing values\n",
    "    \n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r\"\\d+\", \"\", text)  # Remove numbers\n",
    "    text = re.sub(r\"[^\\w\\s]\", \" \", text)  # Remove punctuation\n",
    "    text = re.sub(r\"\\b[a-zA-Z]{1,2}\\b\", \"\", text)  # Remove very short words (1-2 letters)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()  # Remove extra spaces\n",
    "    words = [word for word in text.split() if word not in custom_stop_words and len(word) > 3]  # Remove stopwords & short words\n",
    "    return \" \".join(words)\n",
    "\n",
    "# Apply the improved cleaning function\n",
    "df[\"clean_description\"] = df[\"clean_description\"].apply(clean_text)\n",
    "\n",
    "# Check cleaned text again\n",
    "print(\"Words in first cleaned job description:\", df[\"clean_description\"].iloc[0].split()[:30])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "dbbd2498-5d7c-4def-a804-aa9f9470c199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 Words in TF-IDF Feature List:\n",
      " ['agile' 'ajax' 'algorithms' 'analytical' 'analytical problem' 'analyze'\n",
      " 'android' 'angular' 'angularjs' 'ansible' 'apache' 'apis' 'applicable'\n",
      " 'applicants' 'applications' 'applications using' 'applying' 'approach'\n",
      " 'apps' 'architect']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    stop_words=\"english\",\n",
    "    max_features=800,  # Reduce vocabulary size even further\n",
    "    min_df=75,  # Ignore words appearing in fewer than 75 job descriptions\n",
    "    max_df=0.6,  # Ignore words appearing in more than 60% of job descriptions\n",
    "    ngram_range=(1,2)  # Use bigrams to improve word context\n",
    ")\n",
    "\n",
    "# Apply TF-IDF again\n",
    "X = vectorizer.fit_transform(df[\"clean_description\"])\n",
    "\n",
    "# Print new top words\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "print(\"Top 20 Words in TF-IDF Feature List:\\n\", feature_names[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "23748232-11c3-460a-b831-63c68a376640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 Words in TF-IDF Feature List:\n",
      " ['agile' 'ajax' 'algorithms' 'analytical' 'analytical problem' 'analyze'\n",
      " 'android' 'angular' 'angularjs' 'ansible' 'apache' 'apis' 'applicable'\n",
      " 'applicants' 'applications' 'applications using' 'applying' 'approach'\n",
      " 'apps' 'architect']\n"
     ]
    }
   ],
   "source": [
    "print(\"Top 20 Words in TF-IDF Feature List:\\n\", feature_names[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "94d1ae9a-ae27-47e0-83c5-bd72b04de9c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Cleaned Job Description:\n",
      " hire experts flutter developer eligible post resume types full time part time salary month benefits flexible schedule food allowance schedule shift supplemental joining bonus overtime total work year preferred housing rent subsidy industry software work remotely temporarily covid\n",
      "\n",
      "Another Sample:\n",
      " python django developer lead code python rest frameworks django flask evaluating improving programs linux effectively handle tasks high level accuracy attention good verbal written json preferred good automated unit testing using pyunit\n",
      "\n",
      "Another Sample:\n",
      " data scientist contractor bangalore responsibilities capable data scientist locally india bangalore person responsibilities include research design machine learning deep learning algorithms tackle variety fraud oriented challenges data scientist work closely software engineers managers deliver products data collection scale exploring different algorithmic approaches model assessment validation production qualifications least years hands complex machine learning models using modern frameworks tools ideally python based solid statistics applied mathematics creative thinker proven tackle open problems trivial software using python java similar language graduate computer science mathematics equivalent preferably machine learning write clean concise code quick learner independent methodical oriented player positive attitude collaborative good dedicated makes things happen flexible capable decisions ambiguous changing advantages prior software developer data engineer data spark deep learning frameworks pytorch tensorflow keras telecommunication domain fraud prevention\n"
     ]
    }
   ],
   "source": [
    "print(\"Sample Cleaned Job Description:\\n\", df[\"clean_description\"].iloc[0])\n",
    "print(\"\\nAnother Sample:\\n\", df[\"clean_description\"].iloc[1])\n",
    "print(\"\\nAnother Sample:\\n\", df[\"clean_description\"].iloc[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "78e5ef71-4eaa-4c8e-ad7e-08409677fbbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\gopko\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words in first cleaned job description: ['hire', 'experts', 'flutter', 'developer', 'eligible', 'post', 'resume', 'types', 'full', 'time', 'part', 'time', 'salary', 'month', 'benefits', 'flexible', 'schedule', 'food', 'allowance', 'schedule', 'shift', 'supplemental', 'joining', 'bonus', 'overtime', 'total', 'work', 'year', 'preferred', 'housing']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "custom_stop_words = set(stopwords.words(\"english\")).union(set([\n",
    "    \"ability\", \"able\", \"about\", \"across\", \"action\", \"activities\", \"added\",\n",
    "    \"additional\", \"address\", \"administration\", \"administrator\", \"advanced\",\n",
    "    \"agile\", \"ajax\", \"applicable\", \"applicants\", \"applications\", \"apply\",\n",
    "    \"approach\", \"assistant\", \"collaborate\", \"communication\", \"company\",\n",
    "    \"compliance\", \"concepts\", \"consider\", \"consulting\", \"contribute\",\n",
    "    \"coordinate\", \"corporate\", \"customer\", \"decision\", \"department\",\n",
    "    \"demonstrated\", \"detail\", \"developed\", \"development\", \"duties\",\n",
    "    \"education\", \"effective\", \"efficiency\", \"employees\", \"ensure\",\n",
    "    \"enhance\", \"environment\", \"established\", \"evaluate\", \"execute\",\n",
    "    \"expected\", \"experience\", \"familiar\", \"focus\", \"function\", \"future\",\n",
    "    \"gained\", \"general\", \"goal\", \"group\", \"handling\", \"helping\",\n",
    "    \"identified\", \"impact\", \"important\", \"improve\", \"including\",\n",
    "    \"independently\", \"individual\", \"information\", \"initiatives\", \"job\",\n",
    "    \"join\", \"knowledge\", \"leadership\", \"maintain\", \"making\", \"management\",\n",
    "    \"manager\", \"methodologies\", \"methods\", \"multiple\", \"needs\", \"offer\",\n",
    "    \"office\", \"opportunity\", \"organization\", \"participate\", \"perform\",\n",
    "    \"performance\", \"planning\", \"practices\", \"prepare\", \"process\",\n",
    "    \"proficiency\", \"program\", \"project\", \"providing\", \"related\",\n",
    "    \"reporting\", \"requirements\", \"resources\", \"responsibility\", \"role\",\n",
    "    \"self\", \"services\", \"skills\", \"solutions\", \"specialized\", \"strategic\",\n",
    "    \"strong\", \"successful\", \"support\", \"system\", \"team\", \"technical\",\n",
    "    \"technology\", \"track\", \"training\", \"understanding\", \"various\",\n",
    "    \"working\", \"workplace\", \"writing\"\n",
    "]))\n",
    "\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"Unknown\"  # Handle missing values\n",
    "    \n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r\"\\d+\", \"\", text)  # Remove numbers\n",
    "    text = re.sub(r\"[^\\w\\s]\", \" \", text)  # Remove punctuation\n",
    "    text = re.sub(r\"\\b[a-zA-Z]{1,2}\\b\", \"\", text)  # Remove very short words (1-2 letters)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()  # Remove extra spaces\n",
    "    words = [word for word in text.split() if word not in custom_stop_words and len(word) > 3]  # Remove stopwords & short words\n",
    "    return \" \".join(words)\n",
    "\n",
    "# Apply the improved cleaning function\n",
    "df[\"clean_description\"] = df[\"clean_description\"].apply(clean_text)\n",
    "\n",
    "# Check cleaned text again\n",
    "print(\"Words in first cleaned job description:\", df[\"clean_description\"].iloc[0].split()[:30])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "a346fb60-1856-4279-a199-a304460f8180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 Words in TF-IDF Feature List:\n",
      " ['algorithms' 'analytical' 'analyze' 'android' 'angular' 'angularjs'\n",
      " 'ansible' 'apis' 'apps' 'architecture' 'architectures' 'area' 'areas'\n",
      " 'assist' 'attention' 'attitude' 'automated' 'automation' 'availability'\n",
      " 'available']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    stop_words=\"english\",\n",
    "    max_features=600,  # Reduce vocabulary size even further\n",
    "    min_df=80,  # Ignore words appearing in fewer than 80 job descriptions\n",
    "    max_df=0.55,  # Ignore words appearing in more than 55% of job descriptions\n",
    "    ngram_range=(1,2)  # Use bigrams to improve word context\n",
    ")\n",
    "\n",
    "# Apply TF-IDF again\n",
    "X = vectorizer.fit_transform(df[\"clean_description\"])\n",
    "\n",
    "# Print new top words\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "print(\"Top 20 Words in TF-IDF Feature List:\\n\", feature_names[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "0cf7b04e-203a-4dad-8bae-4515489d3b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 Words in TF-IDF Feature List:\n",
      " ['algorithms' 'analytical' 'analyze' 'android' 'angular' 'angularjs'\n",
      " 'ansible' 'apis' 'apps' 'architecture' 'architectures' 'area' 'areas'\n",
      " 'assist' 'attention' 'attitude' 'automated' 'automation' 'availability'\n",
      " 'available']\n"
     ]
    }
   ],
   "source": [
    "print(\"Top 20 Words in TF-IDF Feature List:\\n\", feature_names[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "1207d5a3-0298-40a7-a8a5-6227d1ba7b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Job Description (Cleaned):\n",
      " hire experts flutter developer eligible post resume types full time part time salary month benefits flexible schedule food allowance schedule shift supplemental joining bonus overtime total work year preferred housing rent subsidy industry software work remotely temporarily covid\n",
      "\n",
      "Second Job Description (Cleaned):\n",
      " python django developer lead code python rest frameworks django flask evaluating improving programs linux effectively handle tasks high level accuracy attention good verbal written json preferred good automated unit testing using pyunit\n",
      "\n",
      "Third Job Description (Cleaned):\n",
      " data scientist contractor bangalore responsibilities capable data scientist locally india bangalore person responsibilities include research design machine learning deep learning algorithms tackle variety fraud oriented challenges data scientist work closely software engineers managers deliver products data collection scale exploring different algorithmic approaches model assessment validation production qualifications least years hands complex machine learning models using modern frameworks tools ideally python based solid statistics applied mathematics creative thinker proven tackle open problems trivial software using python java similar language graduate computer science mathematics equivalent preferably machine learning write clean concise code quick learner independent methodical oriented player positive attitude collaborative good dedicated makes things happen flexible capable decisions ambiguous changing advantages prior software developer data engineer data spark deep learning frameworks pytorch tensorflow keras telecommunication domain fraud prevention\n"
     ]
    }
   ],
   "source": [
    "print(\"First Job Description (Cleaned):\\n\", df[\"clean_description\"].iloc[0])\n",
    "print(\"\\nSecond Job Description (Cleaned):\\n\", df[\"clean_description\"].iloc[1])\n",
    "print(\"\\nThird Job Description (Cleaned):\\n\", df[\"clean_description\"].iloc[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "92df2dcc-1cf8-4f95-87bd-5cc8802945c3",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[136], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Train an LDA model to detect main topics\u001b[39;00m\n\u001b[0;32m      9\u001b[0m lda \u001b[38;5;241m=\u001b[39m LatentDirichletAllocation(n_components\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m---> 10\u001b[0m \u001b[43mlda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Print most common words for each topic\u001b[39;00m\n\u001b[0;32m     13\u001b[0m words \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mget_feature_names_out()\n",
      "File \u001b[1;32m~\\Documents\\GitHub\\ML-Resume-Classifier\\resume-ml-env\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\GitHub\\ML-Resume-Classifier\\resume-ml-env\\Lib\\site-packages\\sklearn\\decomposition\\_lda.py:674\u001b[0m, in \u001b[0;36mLatentDirichletAllocation.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    666\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_em_step(\n\u001b[0;32m    667\u001b[0m             X[idx_slice, :],\n\u001b[0;32m    668\u001b[0m             total_samples\u001b[38;5;241m=\u001b[39mn_samples,\n\u001b[0;32m    669\u001b[0m             batch_update\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    670\u001b[0m             parallel\u001b[38;5;241m=\u001b[39mparallel,\n\u001b[0;32m    671\u001b[0m         )\n\u001b[0;32m    672\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    673\u001b[0m     \u001b[38;5;66;03m# batch update\u001b[39;00m\n\u001b[1;32m--> 674\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_em_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_update\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparallel\u001b[49m\n\u001b[0;32m    676\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    678\u001b[0m \u001b[38;5;66;03m# check perplexity\u001b[39;00m\n\u001b[0;32m    679\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m evaluate_every \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m (i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m evaluate_every \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\Documents\\GitHub\\ML-Resume-Classifier\\resume-ml-env\\Lib\\site-packages\\sklearn\\decomposition\\_lda.py:523\u001b[0m, in \u001b[0;36mLatentDirichletAllocation._em_step\u001b[1;34m(self, X, total_samples, batch_update, parallel)\u001b[0m\n\u001b[0;32m    496\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"EM update for 1 iteration.\u001b[39;00m\n\u001b[0;32m    497\u001b[0m \n\u001b[0;32m    498\u001b[0m \u001b[38;5;124;03mupdate `_component` by batch VB or online VB.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    519\u001b[0m \u001b[38;5;124;03m    Unnormalized document topic distribution.\u001b[39;00m\n\u001b[0;32m    520\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    522\u001b[0m \u001b[38;5;66;03m# E-step\u001b[39;00m\n\u001b[1;32m--> 523\u001b[0m _, suff_stats \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_e_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcal_sstats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparallel\u001b[49m\n\u001b[0;32m    525\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[38;5;66;03m# M-step\u001b[39;00m\n\u001b[0;32m    528\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_update:\n",
      "File \u001b[1;32m~\\Documents\\GitHub\\ML-Resume-Classifier\\resume-ml-env\\Lib\\site-packages\\sklearn\\decomposition\\_lda.py:466\u001b[0m, in \u001b[0;36mLatentDirichletAllocation._e_step\u001b[1;34m(self, X, cal_sstats, random_init, parallel)\u001b[0m\n\u001b[0;32m    464\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m parallel \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    465\u001b[0m     parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mn_jobs, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m--> 466\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    467\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_update_doc_distribution\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    468\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx_slice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    469\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexp_dirichlet_component_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    470\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdoc_topic_prior_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    471\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_doc_update_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    472\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean_change_tol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    473\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcal_sstats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    474\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    475\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    476\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx_slice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgen_even_slices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    477\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    479\u001b[0m \u001b[38;5;66;03m# merge result\u001b[39;00m\n\u001b[0;32m    480\u001b[0m doc_topics, sstats_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mresults)\n",
      "File \u001b[1;32m~\\Documents\\GitHub\\ML-Resume-Classifier\\resume-ml-env\\Lib\\site-packages\\sklearn\\utils\\parallel.py:77\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     72\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     73\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     74\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     76\u001b[0m )\n\u001b[1;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\GitHub\\ML-Resume-Classifier\\resume-ml-env\\Lib\\site-packages\\joblib\\parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n\u001b[0;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[0;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[1;32m~\\Documents\\GitHub\\ML-Resume-Classifier\\resume-ml-env\\Lib\\site-packages\\joblib\\parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[1;32m~\\Documents\\GitHub\\ML-Resume-Classifier\\resume-ml-env\\Lib\\site-packages\\sklearn\\utils\\parallel.py:139\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    137\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 139\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\GitHub\\ML-Resume-Classifier\\resume-ml-env\\Lib\\site-packages\\sklearn\\decomposition\\_lda.py:140\u001b[0m, in \u001b[0;36m_update_doc_distribution\u001b[1;34m(X, exp_topic_word_distr, doc_topic_prior, max_doc_update_iter, mean_change_tol, cal_sstats, random_state)\u001b[0m\n\u001b[0;32m    137\u001b[0m exp_topic_word_d \u001b[38;5;241m=\u001b[39m exp_topic_word_distr[:, ids]\n\u001b[0;32m    139\u001b[0m \u001b[38;5;66;03m# Iterate between `doc_topic_d` and `norm_phi` until convergence\u001b[39;00m\n\u001b[1;32m--> 140\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, max_doc_update_iter):\n\u001b[0;32m    141\u001b[0m     last_d \u001b[38;5;241m=\u001b[39m doc_topic_d\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;66;03m# The optimal phi_{dwk} is proportional to\u001b[39;00m\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;66;03m# exp(E[log(theta_{dk})]) * exp(E[log(beta_{dw})]).\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Convert cleaned text into word frequency matrix\n",
    "vectorizer = CountVectorizer(stop_words=\"english\", max_features=1000)\n",
    "X = vectorizer.fit_transform(df[\"clean_description\"])\n",
    "\n",
    "# Train an LDA model to detect main topics\n",
    "lda = LatentDirichletAllocation(n_components=5, random_state=42)\n",
    "lda.fit(X)\n",
    "\n",
    "# Print most common words for each topic\n",
    "words = vectorizer.get_feature_names_out()\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    print(f\"\\nTopic {topic_idx + 1}:\")\n",
    "    print(\" \".join([words[i] for i in topic.argsort()[:-15 - 1:-1]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c66182-9c26-461c-b9a3-92966fd75b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    stop_words=\"english\",\n",
    "    max_features=500,  # Limit vocabulary to strongest words\n",
    "    min_df=100,  # Ignore words appearing in fewer than 100 job descriptions\n",
    "    max_df=0.5,  # Ignore words appearing in more than 50% of job descriptions\n",
    "    ngram_range=(1,2)  # Use bigrams to improve word context\n",
    ")\n",
    "\n",
    "# Apply TF-IDF again\n",
    "X = vectorizer.fit_transform(df[\"clean_description\"])\n",
    "\n",
    "# Print new top words\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "print(\"Top 20 Words in TF-IDF Feature List:\\n\", feature_names[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "fc562fda-2935-4d5a-ad03-87475ce209f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 Words in TF-IDF Feature List:\n",
      " ['algorithms' 'analytical' 'analyze' 'android' 'angular' 'angularjs'\n",
      " 'ansible' 'apis' 'apps' 'architecture' 'architectures' 'area' 'areas'\n",
      " 'assist' 'attention' 'attitude' 'automated' 'automation' 'availability'\n",
      " 'available']\n"
     ]
    }
   ],
   "source": [
    "print(\"Top 20 Words in TF-IDF Feature List:\\n\", feature_names[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "f9728be6-a05b-4d87-8026-72f509cab7d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\gopko\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words in first cleaned job description: ['experts', 'flutter', 'post', 'time', 'time', 'month', 'flexible', 'food', 'allowance', 'shift', 'supplemental', 'joining', 'bonus', 'overtime', 'year', 'housing', 'rent', 'subsidy', 'temporarily', 'covid']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "custom_stop_words = set(stopwords.words(\"english\")).union(set([\n",
    "    \"hire\", \"developer\", \"eligible\", \"resume\", \"types\", \"full\", \"part\", \n",
    "    \"salary\", \"benefits\", \"schedule\", \"total\", \"industry\", \"software\", \"remotely\",\n",
    "    \"work\", \"high\", \"tasks\", \"experience\", \"good\", \"preferred\", \"proven\",\n",
    "    \"job\", \"preferred\", \"company\", \"managers\", \"handle\", \"accuracy\", \"verbal\",\n",
    "    \"written\", \"handle\", \"opportunity\", \"responsibilities\", \"schedule\", \"frameworks\",\n",
    "    \"skills\", \"python\", \"java\", \"developer\", \"data\", \"computer\", \"science\"\n",
    "]))\n",
    "\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"Unknown\"  # Handle missing values\n",
    "    \n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r\"\\d+\", \"\", text)  # Remove numbers\n",
    "    text = re.sub(r\"[^\\w\\s]\", \" \", text)  # Remove punctuation\n",
    "    text = re.sub(r\"\\b[a-zA-Z]{1,2}\\b\", \"\", text)  # Remove very short words (1-2 letters)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()  # Remove extra spaces\n",
    "    words = [word for word in text.split() if word not in custom_stop_words and len(word) > 3]  # Remove stopwords & short words\n",
    "    return \" \".join(words)\n",
    "\n",
    "# Apply the improved cleaning function\n",
    "df[\"clean_description\"] = df[\"clean_description\"].apply(clean_text)\n",
    "\n",
    "# Show cleaned descriptions again\n",
    "print(\"Words in first cleaned job description:\", df[\"clean_description\"].iloc[0].split()[:30])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "4031fa80-cf89-4a9b-8689-c9813a0cf066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  address                                   career_objective  \\\n",
      "0     NaN  Big data analytics working and database wareho...   \n",
      "1     NaN  Fresher looking to join as a data analyst and ...   \n",
      "2     NaN                                                NaN   \n",
      "3     NaN  To obtain a position in a fast-paced business ...   \n",
      "4     NaN  Professional accountant with an outstanding wo...   \n",
      "\n",
      "                                              skills  \\\n",
      "0  ['Big Data', 'Hadoop', 'Hive', 'Python', 'Mapr...   \n",
      "1  ['Data Analysis', 'Data Analytics', 'Business ...   \n",
      "2  ['Software Development', 'Machine Learning', '...   \n",
      "3  ['accounts payables', 'accounts receivables', ...   \n",
      "4  ['Analytical reasoning', 'Compliance testing k...   \n",
      "\n",
      "                        educational_institution_name  \\\n",
      "0  ['The Amity School of Engineering & Technology...   \n",
      "1  ['Delhi University - Hansraj College', 'Delhi ...   \n",
      "2    ['Birla Institute of Technology (BIT), Ranchi']   \n",
      "3  ['Martinez Adult Education, Business Training ...   \n",
      "4                          ['Kent State University']   \n",
      "\n",
      "                                        degree_names     passing_years  \\\n",
      "0                                         ['B.Tech']          ['2019']   \n",
      "1    ['B.Sc (Maths)', 'M.Sc (Science) (Statistics)']  ['2015', '2018']   \n",
      "2                                         ['B.Tech']          ['2018']   \n",
      "3  ['Computer Applications Specialist Certificate...          ['2008']   \n",
      "4            ['Bachelor of Business Administration']            [None]   \n",
      "\n",
      "  educational_results    result_types             major_field_of_studies  \\\n",
      "0             ['N/A']          [None]                    ['Electronics']   \n",
      "1      ['N/A', 'N/A']  ['N/A', 'N/A']      ['Mathematics', 'Statistics']   \n",
      "2             ['N/A']         ['N/A']  ['Electronics/Telecommunication']   \n",
      "3              [None]          [None]          ['Computer Applications']   \n",
      "4            ['3.84']          [None]                     ['Accounting']   \n",
      "\n",
      "                          professional_company_names  ... online_links  \\\n",
      "0                                      ['Coca-COla']  ...          NaN   \n",
      "1                                ['BIB Consultancy']  ...          NaN   \n",
      "2                              ['Axis Bank Limited']  ...          NaN   \n",
      "3  ['Company Name ï¼ City , State', 'Company Name...  ...          NaN   \n",
      "4  ['Company Name', 'Company Name', 'Company Name...  ...       [None]   \n",
      "\n",
      "  issue_dates           expiry_dates  \\\n",
      "0         NaN                    NaN   \n",
      "1         NaN                    NaN   \n",
      "2         NaN                    NaN   \n",
      "3         NaN                    NaN   \n",
      "4      [None]  ['February 15, 2021']   \n",
      "\n",
      "                                  ﻿job_position_name  \\\n",
      "0                           Senior Software Engineer   \n",
      "1                     Machine Learning (ML) Engineer   \n",
      "2  Executive/ Senior Executive- Trade Marketing, ...   \n",
      "3                     Business Development Executive   \n",
      "4                                Senior iOS Engineer   \n",
      "\n",
      "                            educationaL_requirements experiencere_requirement  \\\n",
      "0  B.Sc in Computer Science & Engineering from a ...          At least 1 year   \n",
      "1  M.Sc in Computer Science & Engineering or in a...       At least 5 year(s)   \n",
      "2            Master of Business Administration (MBA)         At least 3 years   \n",
      "3                                    Bachelor/Honors             1 to 3 years   \n",
      "4      Bachelor of Science (BSc) in Computer Science         At least 4 years   \n",
      "\n",
      "      age_requirement                                 responsibilities.1  \\\n",
      "0                 NaN  Technical Support\\nTroubleshooting\\nCollaborat...   \n",
      "1                 NaN  Machine Learning Leadership\\nCross-Functional ...   \n",
      "2                 NaN  Trade Marketing Executive\\nBrand Visibility, S...   \n",
      "3  Age 22 to 30 years  Apparel Sourcing\\nQuality Garment Sourcing\\nRe...   \n",
      "4                 NaN  iOS Lifecycle\\nRequirement Analysis\\nNative Fr...   \n",
      "\n",
      "                                     skills_required matched_score  \n",
      "0                                                NaN      0.850000  \n",
      "1                                                NaN      0.750000  \n",
      "2  Brand Promotion\\nCampaign Management\\nField Su...      0.416667  \n",
      "3  Fast typing skill\\nIELTSInternet browsing & on...      0.760000  \n",
      "4  iOS\\niOS App Developer\\niOS Application Develo...      0.650000  \n",
      "\n",
      "[5 rows x 35 columns]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9544 entries, 0 to 9543\n",
      "Data columns (total 35 columns):\n",
      " #   Column                               Non-Null Count  Dtype  \n",
      "---  ------                               --------------  -----  \n",
      " 0   address                              784 non-null    object \n",
      " 1   career_objective                     4740 non-null   object \n",
      " 2   skills                               9488 non-null   object \n",
      " 3   educational_institution_name         9460 non-null   object \n",
      " 4   degree_names                         9460 non-null   object \n",
      " 5   passing_years                        9460 non-null   object \n",
      " 6   educational_results                  9460 non-null   object \n",
      " 7   result_types                         9460 non-null   object \n",
      " 8   major_field_of_studies               9460 non-null   object \n",
      " 9   professional_company_names           9460 non-null   object \n",
      " 10  company_urls                         9460 non-null   object \n",
      " 11  start_dates                          9460 non-null   object \n",
      " 12  end_dates                            9460 non-null   object \n",
      " 13  related_skils_in_job                 9460 non-null   object \n",
      " 14  positions                            9460 non-null   object \n",
      " 15  locations                            9460 non-null   object \n",
      " 16  responsibilities                     9544 non-null   object \n",
      " 17  extra_curricular_activity_types      3426 non-null   object \n",
      " 18  extra_curricular_organization_names  3426 non-null   object \n",
      " 19  extra_curricular_organization_links  3426 non-null   object \n",
      " 20  role_positions                       3426 non-null   object \n",
      " 21  languages                            700 non-null    object \n",
      " 22  proficiency_levels                   700 non-null    object \n",
      " 23  certification_providers              2008 non-null   object \n",
      " 24  certification_skills                 2008 non-null   object \n",
      " 25  online_links                         2008 non-null   object \n",
      " 26  issue_dates                          2008 non-null   object \n",
      " 27  expiry_dates                         2008 non-null   object \n",
      " 28  ﻿job_position_name                   9544 non-null   object \n",
      " 29  educationaL_requirements             9544 non-null   object \n",
      " 30  experiencere_requirement             8180 non-null   object \n",
      " 31  age_requirement                      5457 non-null   object \n",
      " 32  responsibilities.1                   9544 non-null   object \n",
      " 33  skills_required                      7843 non-null   object \n",
      " 34  matched_score                        9544 non-null   float64\n",
      "dtypes: float64(1), object(34)\n",
      "memory usage: 2.5+ MB\n",
      "None\n",
      "Columns in dataset: Index(['address', 'career_objective', 'skills', 'educational_institution_name',\n",
      "       'degree_names', 'passing_years', 'educational_results', 'result_types',\n",
      "       'major_field_of_studies', 'professional_company_names', 'company_urls',\n",
      "       'start_dates', 'end_dates', 'related_skils_in_job', 'positions',\n",
      "       'locations', 'responsibilities', 'extra_curricular_activity_types',\n",
      "       'extra_curricular_organization_names',\n",
      "       'extra_curricular_organization_links', 'role_positions', 'languages',\n",
      "       'proficiency_levels', 'certification_providers', 'certification_skills',\n",
      "       'online_links', 'issue_dates', 'expiry_dates', '﻿job_position_name',\n",
      "       'educationaL_requirements', 'experiencere_requirement',\n",
      "       'age_requirement', 'responsibilities.1', 'skills_required',\n",
      "       'matched_score'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the new dataset\n",
    "file_path = r\"C:\\Users\\gopko\\Documents\\GitHub\\ML-Resume-Classifier\\data\\resumeV4.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Show first few rows\n",
    "print(df.head())\n",
    "\n",
    "# Show dataset structure\n",
    "print(df.info())\n",
    "\n",
    "# Show column names\n",
    "print(\"Columns in dataset:\", df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "1ffea690-1728-4fe8-a242-686b35c1407f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['address', 'career_objective', 'skills', 'educational_institution_name',\n",
      "       'degree_names', 'passing_years', 'educational_results', 'result_types',\n",
      "       'major_field_of_studies', 'professional_company_names', 'company_urls',\n",
      "       'start_dates', 'end_dates', 'related_skils_in_job', 'positions',\n",
      "       'locations', 'responsibilities', 'extra_curricular_activity_types',\n",
      "       'extra_curricular_organization_names',\n",
      "       'extra_curricular_organization_links', 'role_positions', 'languages',\n",
      "       'proficiency_levels', 'certification_providers', 'certification_skills',\n",
      "       'online_links', 'issue_dates', 'expiry_dates', '﻿job_position_name',\n",
      "       'educationaL_requirements', 'experiencere_requirement',\n",
      "       'age_requirement', 'responsibilities.1', 'skills_required',\n",
      "       'matched_score'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "37f92943-0732-4c96-a322-7a0c4a8232ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Columns: Index(['address', 'career_objective', 'skills', 'educational_institution_name',\n",
      "       'degree_names', 'passing_years', 'educational_results', 'result_types',\n",
      "       'major_field_of_studies', 'professional_company_names', 'company_urls',\n",
      "       'start_dates', 'end_dates', 'related_skils_in_job', 'positions',\n",
      "       'locations', 'responsibilities', 'extra_curricular_activity_types',\n",
      "       'extra_curricular_organization_names',\n",
      "       'extra_curricular_organization_links', 'role_positions', 'languages',\n",
      "       'proficiency_levels', 'certification_providers', 'certification_skills',\n",
      "       'online_links', 'issue_dates', 'expiry_dates', '﻿job_position_name',\n",
      "       'educationaL_requirements', 'experiencere_requirement',\n",
      "       'age_requirement', 'responsibilities.1', 'skills_required',\n",
      "       'matched_score'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "df.rename(columns={\"actual_resume_column\": \"clean_description\", \"job_category_column\": \"category\"}, inplace=True)\n",
    "print(\"Updated Columns:\", df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "4d203af3-459d-4daa-a3cf-564e3a415f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\gopko\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'clean_description'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\Documents\\GitHub\\ML-Resume-Classifier\\resume-ml-env\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'clean_description'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[142], line 25\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(words)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Apply the improved cleaning function\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclean_description\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mclean_description\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mapply(clean_text)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Show cleaned text\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWords in first cleaned resume:\u001b[39m\u001b[38;5;124m\"\u001b[39m, df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclean_description\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39msplit()[:\u001b[38;5;241m30\u001b[39m])\n",
      "File \u001b[1;32m~\\Documents\\GitHub\\ML-Resume-Classifier\\resume-ml-env\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\Documents\\GitHub\\ML-Resume-Classifier\\resume-ml-env\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'clean_description'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "custom_stop_words = set(stopwords.words(\"english\")).union(set([\n",
    "    \"experience\", \"skills\", \"knowledge\", \"worked\", \"expertise\", \"responsible\",\n",
    "    \"ability\", \"working\", \"proficient\", \"development\", \"team\", \"projects\",\n",
    "    \"hiring\", \"looking\", \"job\", \"description\"\n",
    "]))\n",
    "\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"Unknown\"  # Handle missing values\n",
    "    \n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r\"\\d+\", \"\", text)  # Remove numbers\n",
    "    text = re.sub(r\"[^\\w\\s]\", \" \", text)  # Remove punctuation\n",
    "    text = re.sub(r\"\\b[a-zA-Z]{1,2}\\b\", \"\", text)  # Remove very short words (1-2 letters)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()  # Remove extra spaces\n",
    "    words = [word for word in text.split() if word not in custom_stop_words and len(word) > 3]  # Remove stopwords & short words\n",
    "    return \" \".join(words)\n",
    "\n",
    "# Apply the improved cleaning function\n",
    "df[\"clean_description\"] = df[\"clean_description\"].apply(clean_text)\n",
    "\n",
    "# Show cleaned text\n",
    "print(\"Words in first cleaned resume:\", df[\"clean_description\"].iloc[0].split()[:30])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "94ac68d6-a05b-47bd-a69d-879a76b55ad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Columns: ['address', 'career_objective', 'skills', 'educational_institution_name', 'degree_names', 'passing_years', 'educational_results', 'result_types', 'major_field_of_studies', 'professional_company_names', 'company_urls', 'start_dates', 'end_dates', 'related_skils_in_job', 'positions', 'locations', 'responsibilities', 'extra_curricular_activity_types', 'extra_curricular_organization_names', 'extra_curricular_organization_links', 'role_positions', 'languages', 'proficiency_levels', 'certification_providers', 'certification_skills', 'online_links', 'issue_dates', 'expiry_dates', '\\ufeffjob_position_name', 'educationaL_requirements', 'experiencere_requirement', 'age_requirement', 'responsibilities.1', 'skills_required', 'matched_score']\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataset Columns:\", df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "081619f6-bcc5-4707-9575-278a33879114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    career_objective  \\\n",
      "0  Big data analytics working and database wareho...   \n",
      "1  Fresher looking to join as a data analyst and ...   \n",
      "2                                                NaN   \n",
      "3  To obtain a position in a fast-paced business ...   \n",
      "4  Professional accountant with an outstanding wo...   \n",
      "\n",
      "                                    responsibilities  \\\n",
      "0  Technical Support\\nTroubleshooting\\nCollaborat...   \n",
      "1  Machine Learning Leadership\\nCross-Functional ...   \n",
      "2  Trade Marketing Executive\\nBrand Visibility, S...   \n",
      "3  Apparel Sourcing\\nQuality Garment Sourcing\\nRe...   \n",
      "4  iOS Lifecycle\\nRequirement Analysis\\nNative Fr...   \n",
      "\n",
      "                                              skills  \n",
      "0  ['Big Data', 'Hadoop', 'Hive', 'Python', 'Mapr...  \n",
      "1  ['Data Analysis', 'Data Analytics', 'Business ...  \n",
      "2  ['Software Development', 'Machine Learning', '...  \n",
      "3  ['accounts payables', 'accounts receivables', ...  \n",
      "4  ['Analytical reasoning', 'Compliance testing k...  \n"
     ]
    }
   ],
   "source": [
    "print(df[[\"career_objective\", \"responsibilities\", \"skills\"]].head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "1303fbb2-e439-4d00-919f-9c1ba20f08f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={\"career_objective\": \"clean_description\"}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "b1808d85-708a-4333-a686-7270f131ff32",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={\"responsibilities\": \"clean_description\"}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "ac55967f-f19a-4d77-8e89-9c02ae4b51ef",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'career_objective'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\Documents\\GitHub\\ML-Resume-Classifier\\resume-ml-env\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:191\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:234\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine._get_loc_duplicates\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:242\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine._maybe_get_bool_indexer\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:134\u001b[0m, in \u001b[0;36mpandas._libs.index._unpack_bool_indexer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'career_objective'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[147], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclean_description\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcareer_objective\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponsibilities\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\Documents\\GitHub\\ML-Resume-Classifier\\resume-ml-env\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\Documents\\GitHub\\ML-Resume-Classifier\\resume-ml-env\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'career_objective'"
     ]
    }
   ],
   "source": [
    "df[\"clean_description\"] = df[\"career_objective\"].fillna(\"\") + \" \" + df[\"responsibilities\"].fillna(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "ffc02b68-ea4c-4a4b-89ab-7c4f91ece63a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Columns: ['address', 'clean_description', 'skills', 'educational_institution_name', 'degree_names', 'passing_years', 'educational_results', 'result_types', 'major_field_of_studies', 'professional_company_names', 'company_urls', 'start_dates', 'end_dates', 'related_skils_in_job', 'positions', 'locations', 'clean_description', 'extra_curricular_activity_types', 'extra_curricular_organization_names', 'extra_curricular_organization_links', 'role_positions', 'languages', 'proficiency_levels', 'certification_providers', 'certification_skills', 'online_links', 'issue_dates', 'expiry_dates', '\\ufeffjob_position_name', 'educationaL_requirements', 'experiencere_requirement', 'age_requirement', 'responsibilities.1', 'skills_required', 'matched_score']\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataset Columns:\", df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "801f582e-8920-4f27-85a8-2d25ba676967",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['responsibilities'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[149], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponsibilities\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mskills\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpositions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m5\u001b[39m))\n",
      "File \u001b[1;32m~\\Documents\\GitHub\\ML-Resume-Classifier\\resume-ml-env\\Lib\\site-packages\\pandas\\core\\frame.py:4108\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   4107\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 4108\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   4110\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[1;32m~\\Documents\\GitHub\\ML-Resume-Classifier\\resume-ml-env\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32m~\\Documents\\GitHub\\ML-Resume-Classifier\\resume-ml-env\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6252\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6249\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6251\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m-> 6252\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['responsibilities'] not in index\""
     ]
    }
   ],
   "source": [
    "print(df[[\"responsibilities\", \"skills\", \"positions\"]].head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "c75255d7-2053-40ab-925d-7d5c44d5e3e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                   clean_description  \\\n",
      "0  Big data analytics working and database wareho...   \n",
      "1  Fresher looking to join as a data analyst and ...   \n",
      "2                                                NaN   \n",
      "3  To obtain a position in a fast-paced business ...   \n",
      "4  Professional accountant with an outstanding wo...   \n",
      "\n",
      "                                   clean_description  \n",
      "0  Technical Support\\nTroubleshooting\\nCollaborat...  \n",
      "1  Machine Learning Leadership\\nCross-Functional ...  \n",
      "2  Trade Marketing Executive\\nBrand Visibility, S...  \n",
      "3  Apparel Sourcing\\nQuality Garment Sourcing\\nRe...  \n",
      "4  iOS Lifecycle\\nRequirement Analysis\\nNative Fr...  \n"
     ]
    }
   ],
   "source": [
    "print(df[[\"clean_description\"]].head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "531cea6c-25d4-4c32-8dfc-fb97cf736c58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Columns: ['address', 'clean_description', 'skills', 'educational_institution_name', 'degree_names', 'passing_years', 'educational_results', 'result_types', 'major_field_of_studies', 'professional_company_names', 'company_urls', 'start_dates', 'end_dates', 'related_skils_in_job', 'positions', 'locations', 'extra_curricular_activity_types', 'extra_curricular_organization_names', 'extra_curricular_organization_links', 'role_positions', 'languages', 'proficiency_levels', 'certification_providers', 'certification_skills', 'online_links', 'issue_dates', 'expiry_dates', '\\ufeffjob_position_name', 'educationaL_requirements', 'experiencere_requirement', 'age_requirement', 'responsibilities.1', 'skills_required', 'matched_score']\n"
     ]
    }
   ],
   "source": [
    "df = df.loc[:, ~df.columns.duplicated()]\n",
    "print(\"Updated Columns:\", df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "75c84884-4e80-4704-8101-b1dd5bb11af3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Dataset Columns: ['address', 'clean_description', 'skills', 'educational_institution_name', 'degree_names', 'passing_years', 'educational_results', 'result_types', 'major_field_of_studies', 'professional_company_names', 'company_urls', 'start_dates', 'end_dates', 'related_skils_in_job', 'positions', 'locations', 'extra_curricular_activity_types', 'extra_curricular_organization_names', 'extra_curricular_organization_links', 'role_positions', 'languages', 'proficiency_levels', 'certification_providers', 'certification_skills', 'online_links', 'issue_dates', 'expiry_dates', '\\ufeffjob_position_name', 'educationaL_requirements', 'experiencere_requirement', 'age_requirement', 'responsibilities.1', 'skills_required', 'matched_score']\n"
     ]
    }
   ],
   "source": [
    "print(\"Final Dataset Columns:\", df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "11d81e64-6980-43b2-82d8-59799f33499e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gopko\\AppData\\Local\\Temp\\ipykernel_14132\\662830997.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.rename(columns={\"﻿job_position_name\": \"category\"}, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "df.rename(columns={\"﻿job_position_name\": \"category\"}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "209b5344-c1b3-447c-ac24-390ee2c3f54b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Columns: ['address', 'clean_description', 'skills', 'educational_institution_name', 'degree_names', 'passing_years', 'educational_results', 'result_types', 'major_field_of_studies', 'professional_company_names', 'company_urls', 'start_dates', 'end_dates', 'related_skils_in_job', 'positions', 'locations', 'extra_curricular_activity_types', 'extra_curricular_organization_names', 'extra_curricular_organization_links', 'role_positions', 'languages', 'proficiency_levels', 'certification_providers', 'certification_skills', 'online_links', 'issue_dates', 'expiry_dates', 'category', 'educationaL_requirements', 'experiencere_requirement', 'age_requirement', 'responsibilities.1', 'skills_required', 'matched_score']\n"
     ]
    }
   ],
   "source": [
    "df = df.copy()  # Ensures that df is an independent copy before modifying it\n",
    "\n",
    "# Rename the column\n",
    "df.rename(columns={\"﻿job_position_name\": \"category\"}, inplace=True)\n",
    "\n",
    "print(\"Updated Columns:\", df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "4bc16a0b-4440-4689-b15e-dd6ac422a49c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Dataset Columns: ['address', 'clean_description', 'skills', 'educational_institution_name', 'degree_names', 'passing_years', 'educational_results', 'result_types', 'major_field_of_studies', 'professional_company_names', 'company_urls', 'start_dates', 'end_dates', 'related_skils_in_job', 'positions', 'locations', 'extra_curricular_activity_types', 'extra_curricular_organization_names', 'extra_curricular_organization_links', 'role_positions', 'languages', 'proficiency_levels', 'certification_providers', 'certification_skills', 'online_links', 'issue_dates', 'expiry_dates', 'category', 'educationaL_requirements', 'experiencere_requirement', 'age_requirement', 'responsibilities.1', 'skills_required', 'matched_score']\n"
     ]
    }
   ],
   "source": [
    "print(\"Final Dataset Columns:\", df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "9fb38df3-ad6d-437c-b5e3-f46f908d0538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Dataset Columns: ['clean_description', 'category']\n",
      "                                   clean_description  \\\n",
      "0  Big data analytics working and database wareho...   \n",
      "1  Fresher looking to join as a data analyst and ...   \n",
      "2                                                NaN   \n",
      "3  To obtain a position in a fast-paced business ...   \n",
      "4  Professional accountant with an outstanding wo...   \n",
      "\n",
      "                                            category  \n",
      "0                           Senior Software Engineer  \n",
      "1                     Machine Learning (ML) Engineer  \n",
      "2  Executive/ Senior Executive- Trade Marketing, ...  \n",
      "3                     Business Development Executive  \n",
      "4                                Senior iOS Engineer  \n"
     ]
    }
   ],
   "source": [
    "df = df[[\"clean_description\", \"category\"]]\n",
    "\n",
    "print(\"Updated Dataset Columns:\", df.columns.tolist())\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "1e458ced-af8c-495d-b65e-69bf00ba0f54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\gopko\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words in first cleaned resume: ['data', 'analytics', 'database', 'warehouse', 'manager', 'robust', 'handling', 'kinds', 'data', 'also', 'used', 'multiple', 'cloud', 'infrastructure', 'services', 'well', 'acquainted', 'currently', 'search', 'role', 'offers']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "custom_stop_words = set(stopwords.words(\"english\")).union(set([\n",
    "    \"experience\", \"skills\", \"knowledge\", \"worked\", \"expertise\", \"responsible\",\n",
    "    \"ability\", \"working\", \"proficient\", \"development\", \"team\", \"projects\",\n",
    "    \"hiring\", \"looking\", \"job\", \"description\"\n",
    "]))\n",
    "\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"Unknown\"  # Handle missing values\n",
    "    \n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r\"\\d+\", \"\", text)  # Remove numbers\n",
    "    text = re.sub(r\"[^\\w\\s]\", \" \", text)  # Remove punctuation\n",
    "    text = re.sub(r\"\\b[a-zA-Z]{1,2}\\b\", \"\", text)  # Remove very short words (1-2 letters)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()  # Remove extra spaces\n",
    "    words = [word for word in text.split() if word not in custom_stop_words and len(word) > 3]  # Remove stopwords & short words\n",
    "    return \" \".join(words)\n",
    "\n",
    "# Apply text cleaning\n",
    "df[\"clean_description\"] = df[\"clean_description\"].apply(clean_text)\n",
    "\n",
    "# Show cleaned text\n",
    "print(\"Words in first cleaned resume:\", df[\"clean_description\"].iloc[0].split()[:30])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "85145cb1-0712-4f27-91a8-e2ddb11de17f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words in first cleaned resume: ['offers', 'acquainted', 'handling', 'search', 'also', 'manager', 'data', 'kinds', 'robust', 'warehouse', 'services', 'analytics', 'role', 'cloud', 'infrastructure', 'used', 'database', 'currently', 'well', 'multiple']\n"
     ]
    }
   ],
   "source": [
    "df[\"clean_description\"] = df[\"clean_description\"].apply(lambda x: \" \".join(set(x.split())))\n",
    "print(\"Words in first cleaned resume:\", df[\"clean_description\"].iloc[0].split()[:30])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "bf435b77-c89c-472a-966c-67f7605f1a72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF matrix shape: (9544, 308)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    stop_words=\"english\",\n",
    "    max_features=2000,  # Reduce vocabulary size\n",
    "    min_df=75,  # Ignore words appearing in fewer than 75 resumes\n",
    "    max_df=0.55,  # Ignore words appearing in more than 55% of resumes\n",
    "    ngram_range=(1,2)  # Use bigrams for better context\n",
    ")\n",
    "\n",
    "# Convert cleaned resumes into numerical vectors\n",
    "X = vectorizer.fit_transform(df[\"clean_description\"])\n",
    "\n",
    "# Print the shape of transformed data\n",
    "print(\"TF-IDF matrix shape:\", X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "b790086c-1319-41fe-9c77-d5981c4d3c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 Words in TF-IDF Feature List:\n",
      " ['abilities' 'accountant' 'accounting' 'achieve' 'acquainted'\n",
      " 'administration' 'advancement' 'algorithms' 'analysis' 'analyst'\n",
      " 'analyst like' 'analytical' 'analytics' 'analytics machine'\n",
      " 'analytics role' 'analytics seeking' 'analyzing' 'application'\n",
      " 'applications' 'apply']\n"
     ]
    }
   ],
   "source": [
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Print the first 20 important words\n",
    "print(\"Top 20 Words in TF-IDF Feature List:\\n\", feature_names[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "92162c84-139a-41c2-a7a3-f505edf393c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Resume Text Before TF-IDF:\n",
      " 0    offers acquainted handling search also manager...\n",
      "1    dashboards scientist creating experienced anal...\n",
      "2                                              Unknown\n",
      "3    technical office position organizational busin...\n",
      "4    communication ethic analytical computer organi...\n",
      "Name: clean_description, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(\"Sample Resume Text Before TF-IDF:\\n\", df[\"clean_description\"].iloc[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "b4897133-bc12-4ada-8c39-d321437900cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words in first cleaned resume: ['offers', 'acquainted', 'handling', 'search', 'also', 'manager', 'data', 'kinds', 'robust', 'warehouse', 'services', 'analytics', 'role', 'cloud', 'infrastructure', 'used', 'database', 'currently', 'well', 'multiple']\n"
     ]
    }
   ],
   "source": [
    "custom_stop_words = set(stopwords.words(\"english\")).union(set([\n",
    "    \"experience\", \"skills\", \"knowledge\", \"worked\", \"expertise\", \"responsible\",\n",
    "    \"ability\", \"working\", \"proficient\", \"development\", \"team\", \"projects\",\n",
    "    \"hiring\", \"looking\", \"job\", \"description\", \"like\", \"seeking\", \"apply\",\n",
    "    \"applications\", \"achieve\", \"administration\", \"advancement\", \"machine\"\n",
    "]))\n",
    "\n",
    "df[\"clean_description\"] = df[\"clean_description\"].apply(lambda text: \" \".join(\n",
    "    [word for word in text.split() if word not in custom_stop_words]\n",
    "))\n",
    "\n",
    "print(\"Words in first cleaned resume:\", df[\"clean_description\"].iloc[0].split()[:30])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "208085eb-0840-4f72-b97d-c78838402f37",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidParameterError",
     "evalue": "The 'stop_words' parameter of TfidfVectorizer must be a str among {'english'}, an instance of 'list' or None. Got {\"he'll\", 'what', 'didn', 'have', 'which', 'its', 'hasn', \"we'd\", 'a', 'shouldn', \"it's\", 'this', 'doing', 'he', 'couldn', 'more', \"didn't\", 'below', 'few', 'was', 'i', 'them', 'after', \"she's\", \"shouldn't\", 'an', 'theirs', \"he'd\", \"it'll\", \"they've\", 'itself', 'now', 'any', 't', \"couldn't\", 'should', 'themselves', 'projects', \"it'd\", 'same', \"she'd\", \"that'll\", \"should've\", \"we've\", 'don', 'apply', 'during', 'himself', 'can', 'herself', 'her', 'me', \"weren't\", 'll', 'most', 'such', 'there', 'through', 'd', \"i'd\", 'who', 'ain', 'not', 'yours', \"i've\", 'ours', 'hiring', 'like', 'if', 'with', 'in', 'these', 'she', 'they', 'that', 'about', 'working', \"don't\", \"aren't\", 'when', 'hadn', 'how', 'responsible', \"wasn't\", \"we'll\", 'mustn', 'shan', 'up', 'does', 'been', 'were', 'o', 'seeking', 'by', 'whom', 'for', \"i'm\", 'into', 'why', 'to', \"doesn't\", 'further', 'it', 'while', \"he's\", \"they're\", 'but', \"hadn't\", 'nor', 'proficient', 'my', 'achieve', 'on', 'experience', \"she'll\", 'no', \"they'll\", 'doesn', 'won', 'yourselves', 'expertise', 'skills', 'hers', 'out', 'their', 'yourself', 'once', 'as', \"wouldn't\", \"you've\", 'administration', 'your', 'very', \"you'll\", 'until', 'against', \"hasn't\", 'or', \"won't\", 'wasn', 'at', \"mustn't\", 'having', \"i'll\", 'between', 'myself', 'team', 'knowledge', 'machine', 'are', 'because', 'looking', 'job', 'ourselves', 'wouldn', 'here', 'being', \"haven't\", \"we're\", 'ma', 'needn', 'so', 'each', 'the', 'm', 'above', 'haven', 'his', 'is', 'all', 'will', 'do', 'isn', 'over', \"shan't\", 'before', 'be', \"mightn't\", 'aren', 'applications', 'of', 'from', 'just', 'mightn', 'under', 'ability', 'description', \"you'd\", 'those', 'our', \"isn't\", 'him', 'y', 'then', \"you're\", 'worked', \"needn't\", 'some', 'weren', 'development', 'where', 'both', 's', 'than', 'down', 'am', 'and', \"they'd\", 've', 'has', 'had', 'you', 're', 'only', 'too', 'we', 'advancement', 'did', 'again', 'other', 'off', 'own'} instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidParameterError\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[163], line 10\u001b[0m\n\u001b[0;32m      1\u001b[0m vectorizer \u001b[38;5;241m=\u001b[39m TfidfVectorizer(\n\u001b[0;32m      2\u001b[0m     stop_words\u001b[38;5;241m=\u001b[39mcustom_stop_words,\n\u001b[0;32m      3\u001b[0m     max_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m,  \u001b[38;5;66;03m# Reduce vocabulary size even further\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      6\u001b[0m     ngram_range\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# Use bigrams for better context\u001b[39;00m\n\u001b[0;32m      7\u001b[0m )\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Convert cleaned resumes into numerical vectors\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mvectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mclean_description\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Print the shape of transformed data\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTF-IDF matrix shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, X\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[1;32m~\\Documents\\GitHub\\ML-Resume-Classifier\\resume-ml-env\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:2104\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   2097\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params()\n\u001b[0;32m   2098\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf \u001b[38;5;241m=\u001b[39m TfidfTransformer(\n\u001b[0;32m   2099\u001b[0m     norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm,\n\u001b[0;32m   2100\u001b[0m     use_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_idf,\n\u001b[0;32m   2101\u001b[0m     smooth_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msmooth_idf,\n\u001b[0;32m   2102\u001b[0m     sublinear_tf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msublinear_tf,\n\u001b[0;32m   2103\u001b[0m )\n\u001b[1;32m-> 2104\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2105\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[0;32m   2106\u001b[0m \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[0;32m   2107\u001b[0m \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n",
      "File \u001b[1;32m~\\Documents\\GitHub\\ML-Resume-Classifier\\resume-ml-env\\Lib\\site-packages\\sklearn\\base.py:1382\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1377\u001b[0m partial_fit_and_fitted \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1378\u001b[0m     fit_method\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpartial_fit\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m _is_fitted(estimator)\n\u001b[0;32m   1379\u001b[0m )\n\u001b[0;32m   1381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m global_skip_validation \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m partial_fit_and_fitted:\n\u001b[1;32m-> 1382\u001b[0m     \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[0;32m   1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\Documents\\GitHub\\ML-Resume-Classifier\\resume-ml-env\\Lib\\site-packages\\sklearn\\base.py:436\u001b[0m, in \u001b[0;36mBaseEstimator._validate_params\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    428\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_validate_params\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    429\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Validate types and values of constructor parameters\u001b[39;00m\n\u001b[0;32m    430\u001b[0m \n\u001b[0;32m    431\u001b[0m \u001b[38;5;124;03m    The expected type and values must be defined in the `_parameter_constraints`\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    434\u001b[0m \u001b[38;5;124;03m    accepted constraints.\u001b[39;00m\n\u001b[0;32m    435\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 436\u001b[0m     \u001b[43mvalidate_parameter_constraints\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    437\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parameter_constraints\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    438\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    439\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcaller_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    440\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\GitHub\\ML-Resume-Classifier\\resume-ml-env\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:98\u001b[0m, in \u001b[0;36mvalidate_parameter_constraints\u001b[1;34m(parameter_constraints, params, caller_name)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     93\u001b[0m     constraints_str \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     94\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;28mstr\u001b[39m(c)\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mconstraints[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m or\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     95\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     96\u001b[0m     )\n\u001b[1;32m---> 98\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m InvalidParameterError(\n\u001b[0;32m     99\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_name\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m parameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcaller_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_val\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    101\u001b[0m )\n",
      "\u001b[1;31mInvalidParameterError\u001b[0m: The 'stop_words' parameter of TfidfVectorizer must be a str among {'english'}, an instance of 'list' or None. Got {\"he'll\", 'what', 'didn', 'have', 'which', 'its', 'hasn', \"we'd\", 'a', 'shouldn', \"it's\", 'this', 'doing', 'he', 'couldn', 'more', \"didn't\", 'below', 'few', 'was', 'i', 'them', 'after', \"she's\", \"shouldn't\", 'an', 'theirs', \"he'd\", \"it'll\", \"they've\", 'itself', 'now', 'any', 't', \"couldn't\", 'should', 'themselves', 'projects', \"it'd\", 'same', \"she'd\", \"that'll\", \"should've\", \"we've\", 'don', 'apply', 'during', 'himself', 'can', 'herself', 'her', 'me', \"weren't\", 'll', 'most', 'such', 'there', 'through', 'd', \"i'd\", 'who', 'ain', 'not', 'yours', \"i've\", 'ours', 'hiring', 'like', 'if', 'with', 'in', 'these', 'she', 'they', 'that', 'about', 'working', \"don't\", \"aren't\", 'when', 'hadn', 'how', 'responsible', \"wasn't\", \"we'll\", 'mustn', 'shan', 'up', 'does', 'been', 'were', 'o', 'seeking', 'by', 'whom', 'for', \"i'm\", 'into', 'why', 'to', \"doesn't\", 'further', 'it', 'while', \"he's\", \"they're\", 'but', \"hadn't\", 'nor', 'proficient', 'my', 'achieve', 'on', 'experience', \"she'll\", 'no', \"they'll\", 'doesn', 'won', 'yourselves', 'expertise', 'skills', 'hers', 'out', 'their', 'yourself', 'once', 'as', \"wouldn't\", \"you've\", 'administration', 'your', 'very', \"you'll\", 'until', 'against', \"hasn't\", 'or', \"won't\", 'wasn', 'at', \"mustn't\", 'having', \"i'll\", 'between', 'myself', 'team', 'knowledge', 'machine', 'are', 'because', 'looking', 'job', 'ourselves', 'wouldn', 'here', 'being', \"haven't\", \"we're\", 'ma', 'needn', 'so', 'each', 'the', 'm', 'above', 'haven', 'his', 'is', 'all', 'will', 'do', 'isn', 'over', \"shan't\", 'before', 'be', \"mightn't\", 'aren', 'applications', 'of', 'from', 'just', 'mightn', 'under', 'ability', 'description', \"you'd\", 'those', 'our', \"isn't\", 'him', 'y', 'then', \"you're\", 'worked', \"needn't\", 'some', 'weren', 'development', 'where', 'both', 's', 'than', 'down', 'am', 'and', \"they'd\", 've', 'has', 'had', 'you', 're', 'only', 'too', 'we', 'advancement', 'did', 'again', 'other', 'off', 'own'} instead."
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "    stop_words=custom_stop_words,\n",
    "    max_features=1000,  # Reduce vocabulary size even further\n",
    "    min_df=100,  # Ignore words appearing in fewer than 100 resumes\n",
    "    max_df=0.4,  # Ignore words appearing in more than 40% of resumes\n",
    "    ngram_range=(1,2)  # Use bigrams for better context\n",
    ")\n",
    "\n",
    "# Convert cleaned resumes into numerical vectors\n",
    "X = vectorizer.fit_transform(df[\"clean_description\"])\n",
    "\n",
    "# Print the shape of transformed data\n",
    "print(\"TF-IDF matrix shape:\", X.shape)\n",
    "\n",
    "# Print new top words\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "print(\"Top 20 Words in TF-IDF Feature List:\\n\", feature_names[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "07ba7b27-50d8-4464-8029-4a9df7a5f8be",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "Vocabulary not fitted or provided",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[164], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m feature_names \u001b[38;5;241m=\u001b[39m \u001b[43mvectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_feature_names_out\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNew Top 20 Words in TF-IDF Feature List:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, feature_names[:\u001b[38;5;241m20\u001b[39m])\n",
      "File \u001b[1;32m~\\Documents\\GitHub\\ML-Resume-Classifier\\resume-ml-env\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1472\u001b[0m, in \u001b[0;36mCountVectorizer.get_feature_names_out\u001b[1;34m(self, input_features)\u001b[0m\n\u001b[0;32m   1459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_feature_names_out\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1460\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get output feature names for transformation.\u001b[39;00m\n\u001b[0;32m   1461\u001b[0m \n\u001b[0;32m   1462\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1470\u001b[0m \u001b[38;5;124;03m        Transformed feature names.\u001b[39;00m\n\u001b[0;32m   1471\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1472\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_vocabulary\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39masarray(\n\u001b[0;32m   1474\u001b[0m         [t \u001b[38;5;28;01mfor\u001b[39;00m t, i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocabulary_\u001b[38;5;241m.\u001b[39mitems(), key\u001b[38;5;241m=\u001b[39mitemgetter(\u001b[38;5;241m1\u001b[39m))],\n\u001b[0;32m   1475\u001b[0m         dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mobject\u001b[39m,\n\u001b[0;32m   1476\u001b[0m     )\n",
      "File \u001b[1;32m~\\Documents\\GitHub\\ML-Resume-Classifier\\resume-ml-env\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:501\u001b[0m, in \u001b[0;36m_VectorizerMixin._check_vocabulary\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    499\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_vocabulary()\n\u001b[0;32m    500\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfixed_vocabulary_:\n\u001b[1;32m--> 501\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m NotFittedError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVocabulary not fitted or provided\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    503\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocabulary_) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    504\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVocabulary is empty\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNotFittedError\u001b[0m: Vocabulary not fitted or provided"
     ]
    }
   ],
   "source": [
    "feature_names = vectorizer.get_feature_names_out()\n",
    "print(\"New Top 20 Words in TF-IDF Feature List:\\n\", feature_names[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "7115c78c-7118-4396-b239-0a85e94452f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF matrix shape: (9544, 190)\n",
      "Top 20 Words in TF-IDF Feature List:\n",
      " ['accountant' 'accounting' 'across' 'also' 'always' 'analysis' 'analyst'\n",
      " 'analytical' 'analytics' 'analytics role' 'analyzing' 'application'\n",
      " 'artificial' 'background' 'based' 'best' 'better' 'build' 'building'\n",
      " 'business']\n"
     ]
    }
   ],
   "source": [
    "# Convert set to list\n",
    "custom_stop_words_list = list(custom_stop_words)\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    stop_words=custom_stop_words_list,  # Fix: Convert to list\n",
    "    max_features=1000,  # Reduce vocabulary size even further\n",
    "    min_df=100,  # Ignore words appearing in fewer than 100 resumes\n",
    "    max_df=0.4,  # Ignore words appearing in more than 40% of resumes\n",
    "    ngram_range=(1,2)  # Use bigrams for better context\n",
    ")\n",
    "\n",
    "# Convert cleaned resumes into numerical vectors\n",
    "X = vectorizer.fit_transform(df[\"clean_description\"])\n",
    "\n",
    "# Print the shape of transformed data\n",
    "print(\"TF-IDF matrix shape:\", X.shape)\n",
    "\n",
    "# Print new top words\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "print(\"Top 20 Words in TF-IDF Feature List:\\n\", feature_names[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "3bd4b999-7de6-46b2-9a07-5f3c2f507251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New TF-IDF matrix shape: (9544, 577)\n",
      "Top 20 Words in TF-IDF Feature List:\n",
      " ['abilities' 'able' 'accountant' 'accounting' 'accounting proven'\n",
      " 'achieving' 'acquainted' 'across' 'actively' 'advance' 'advanced'\n",
      " 'algorithms' 'algorithms data' 'allow' 'along' 'also' 'also desire'\n",
      " 'also problems' 'always' 'amazon']\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "    stop_words=custom_stop_words_list,  \n",
    "    max_features=3000,  # Increase vocabulary size\n",
    "    min_df=50,  # Lower threshold to capture more important words\n",
    "    max_df=0.5,  # Avoid very common words appearing in more than 50% of resumes\n",
    "    ngram_range=(1,2)  # Keep bigrams for better meaning\n",
    ")\n",
    "\n",
    "# Transform resumes into numerical vectors\n",
    "X = vectorizer.fit_transform(df[\"clean_description\"])\n",
    "\n",
    "# Print new shape\n",
    "print(\"New TF-IDF matrix shape:\", X.shape)\n",
    "\n",
    "# Show new top words\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "print(\"Top 20 Words in TF-IDF Feature List:\\n\", feature_names[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "4937372e-ff51-46e9-adc2-54829f4352d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 cleaned resumes:\n",
      " 0    offers acquainted handling search also manager...\n",
      "1    dashboards scientist creating experienced anal...\n",
      "2                                              Unknown\n",
      "3    technical office position organizational busin...\n",
      "4    communication ethic analytical computer organi...\n",
      "Name: clean_description, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(\"First 5 cleaned resumes:\\n\", df[\"clean_description\"].head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "9971e46e-991b-44bc-ac10-a2fd50d76369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF vectorizer saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the trained TF-IDF model\n",
    "joblib.dump(vectorizer, \"tfidf_vectorizer_v2.pkl\")\n",
    "\n",
    "print(\"TF-IDF vectorizer saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f8da71-1837-4ce9-9f1d-461ec16c02d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928f51a6-5bb3-4438-8cd6-9cb21b1a83c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (resume-ml-env)",
   "language": "python",
   "name": "resume-ml-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
